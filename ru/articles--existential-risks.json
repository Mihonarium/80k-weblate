{
    "articles--existential-risks:000": "# The case for reducing existential risks\n## !empty\n<p class=\"entry-meta small\">By <span class=\"byline author vcard\"><a href=\"https://80000hours.org/author/benjamin-todd/\" rel=\"author\" class=\"fn\">Benjamin Todd</a></span> · Published <time class=\"published\" datetime=\"2017-10-27T23:41:47+00:00\">October 2017</time> · Last updated <time class=\"update\" datetime=\"2022-06-08T00:00:00+00:00\">June 8th, 2022</time></p>",
    "articles--existential-risks:004": "!podcast-player",
    "articles--existential-risks:006": "В 1939 году Эйнштейн пишет Рузвельту:[^:``]",
    "articles--existential-risks:008": "> Есть причины считать возможным запуск цепной ядерной реакции в большой массе урана ... и можно предположить, хоть и с меньшей степенью уверенности, что это позволит создавать чрезвычайно мощные бомбы нового типа.",
    "articles--existential-risks:010": "Всего через несколько лет такие бомбы были созданы. Спустя десять с небольшим лет их число достигло такого уровня, что впервые в истории маленькая группа людей, принимающих решения, могла полностью уничтожить цивилизацию как таковую.",
    "articles--existential-risks:012": "Человечество вступило в новую эру. Теперь мы встретились, помимо природных экзистенциальных рисков[^:`Ник Бостром определяет понятие [экзистенциального риска](https://www.nickbostrom.com/existential/risks.html) как событие, которое \"может привести к вымиранию человечества или необратимо ограничить его потенциал\". Экзистенциальный риск отличается от [риска глобальной катастрофы (РГК)](https://en.wikipedia.org/wiki/Global_catastrophic_risk) по масштабу — РГК является катастрофичным на масштабе планеты, но включает возможность последующего восстановления. Термин [\"экзистенциальная угроза\"](https://www.theatlantic.com/ideas/archive/2019/06/2020-candidates-say-everything-existential-threat/591967/) обычно используется лишь как усиление, с целью представить некую проблему как более ужасную.`], ещё и с шансом самоуничтожения.",
    "articles--existential-risks:014": "<div class=\"well bg-gray-lighter margin-bottom margin-top padding-top-small padding-bottom-small\">",
    "articles--existential-risks:016": "### Предпочитаете подкаст?",
    "articles--existential-risks:018": "С момента публикации этой статьи мы записали два **подкаста** на тему экзистенциальных рисков с доктором Тоби Ордом, философом из Оксфорда и доверенным лицом проекта \"80 000 часов\". Мы считаем, что они являются столь же хорошим введением в эту тему, как и данная статья, если не лучше. Их можно послушать тут:",
    "articles--existential-risks:020": "- [Тоби Орд: о \"пропасти\" и различных сценариях будущего, с которыми мы можем столкнуться](https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/)",
    "articles--existential-risks:022": "- [Почему долгосрочное будущее человечества важнее всего, и что с этим делать](https://80000hours.org/podcast/episodes/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/)\n",
    "articles--existential-risks:025": "</div>",
    "articles--existential-risks:027": "<div class=\"panel clearfix \">",
    "articles--existential-risks:029": "[!img~ style=\"width: 30%; margin-left: 10%; margin-top:-10px; float: right;\" ~[book cover](https://80000hours.org/wp-content/uploads/2020/03/the-precipice-3d.jpg)](/the-precipice/)",
    "articles--existential-risks:031": "<div style=\"margin-top: -10px;\">",
    "articles--existential-risks:033": "### Предпочитаете книгу?",
    "articles--existential-risks:035": "У доктора Тоби Орда, философа из Оксфорда и доверенного лица проекта \"80 000 часов\", недавно вышла книга _[\"Пропасть: Экзистенциальный риск и будущее человечества\"](/the-precipice/)_, в которой даётся обзор моральной значимости будущих поколений и того, что мы можем сделать сегодня, чтобы помочь им.",
    "articles--existential-risks:037": "#### Мы вышлем вам книгу по почте (бесплатно)",
    "articles--existential-risks:039": "Присоединяйтесь к рассылке \"80 000 часов\", и мы пришлем вам бесплатный экземпляр книги.",
    "articles--existential-risks:041": "Мы также будем присылать вам свежую информацию о наших исследованиях, вакансии, связанные с работой над экзистенциальными рисками, а также новости от автора.",
    "articles--existential-risks:043": "!newsletter-subscribe",
    "articles--existential-risks:045": "Если вы уже подписаны на нашу рассылку, напишите нам на [book.giveaway@80000hours.org](/cdn-cgi/l/email-protection#89ebe6e6e2a7eee0ffece8fee8f0afeae6e4e4e8fdb2b1b9b9b9b9e1e6fcfbfaa7e6fbee), чтобы получить экземпляр.",
    "articles--existential-risks:047": "</div>",
    "articles--existential-risks:049": "</div>",
    "articles--existential-risks:051": "В эту новую эпоху, что должно быть нашим самым главным приоритетом как цивилизации? Совершенствование технологий? Помощь бедным? Изменение политической системы?",
    "articles--existential-risks:053": "Вот предложение, которое не так часто обсуждается: в первую очередь мы должны _выжить_.",
    "articles--existential-risks:055": "Пока цивилизация продолжает существовать, у нас есть шанс решить все остальные проблемы и прийти к гораздо лучшему будущему. Но если мы вымрем, это будет конец.",
    "articles--existential-risks:057": "Почему об этом приоритете мало говорят? Вот одна из причин: многие люди ещё не успели заметить, что общая ситуация значительно изменилась, и поэтому не думают, что наше будущее находится под угрозой.",
    "articles--existential-risks:059": "Исследователь в области социальных наук [Спенсер Гринберг](http://www.spencergreenberg.com/) провел опрос среди американцев, чтобы узнать, как они оценивают шансы вымирания человечества в течение 50 лет. Результаты показали, что многие считают шансы крайне низкими: более 30% полагают, что они меньше 1 к 10 миллионам. [^:`Гринберг опрашивал пользователей Mechanical Turk, чей средний возраст — 20-40 лет, а уровень образования — как правило, выше среднего, поэтому опрос не отражает мнения всех американцев. Подробнее смотрите в этом видео:",
    "articles--existential-risks:061": "[Социальные науки как оптика для эффективной благотворительности: результаты четырех новых исследований - Спенсер Гринберг](https://www.youtube.com/watch?v=tOSpj19eows), 12:15.",
    "articles--existential-risks:063": "Первоначальный опрос показал, что медианная оценка шансов вымирания в течение 50 лет составляет 1 к 10 миллионам. Гринберг провел три повторных исследования, которые дали более высокие оценки шансов. Самая высокая из них показала медиану 1 к 100 в течение 50 лет. Однако даже в этом случае 39% респондентов все равно предположили, что шансы были ниже 1 к 10 000 (что примерно равно шансам столкновения с астероидом диаметром в 1 км). Во всех случаях более 30% считали, что шансы меньше 1 к 10 миллионам. Сводку всех опросов можно посмотреть [здесь](http://spencergreenberg.com/documents/Comparison%20of%20study%20results%20about%20probability%20of%20human%20extinction.xlsx).",
    "articles--existential-risks:065": "Обратите внимание, что когда мы спрашивали людей о шансах вымирания без временных рамок, оценки были гораздо выше. В одном из опросов медиана составила 75%. Их можно понять: человечество _ рано или поздно_ вымрет. Это помогает объяснить расхождение с некоторыми другими опросами. Например, исследование \"Climate Change in the American Mind\" (май 2017 года, [архивная ссылка](https://web.archive.org/web/20171031033116/http://climatecommunication.yale.edu/wp-content/uploads/2017/07/Climate-Change-American-Mind-May-2017.pdf)) показало, что средний американец считает, что шансы вымирания в результате изменения климата составляют примерно 1 к 3. Однако в этом исследовании не задавался вопрос о конкретных сроках. Когда Гринберг попытался воспроизвести результат с тем же вопросом, он получил аналогичную цифру. Но когда Гринберг спрашивал о шансах вымирания в результате изменения климата в ближайшие 50 лет, медиана упала всего до 1%. Многие другие исследования также некорректно обращаются с низкими оценками вероятности — люди обычно не дают оценку в 0,00001%, если им явно не предложить такой вариант.",
    "articles--existential-risks:067": "Однако, как можно заметить, подобные опросы обычно дают очень ненадёжные результаты. Ответы могут зависеть от конкретной фомулировки вопроса и от контекста. Отчасти это так, потому что люди очень плохо умеют оценивать очень маленькие вероятности. Из-за этого трудно дать точную оценку того, что думает население в целом, но ничто из этого не опровергает вывод о том, что значительное число людей (скажем, более 25%) считает, что шансы вымирания в краткосрочной перспективе очень-очень малы, и скорее всего ниже, чем вероятность столкновения с астероидом. Более того, ненадёжность этих оценок не добавляет уверенности в том, что человечество рационально учитывает эти риски.`]",
    "articles--existential-risks:069": "Сначала мы тоже считали, что подобные риски крайне малы, но, изучив этот вопрос, мы стали думать иначе. Далее мы увидим, что исследователи, изучающие эти вопросы, считают, что реальный риск вымирания — в 1000+ раз выше оценки из опроса, и скорее всего лишь продолжает расти.",
    "articles--existential-risks:071": "Эти опасения положили начало новому движению по защите цивилизации, к которому присоединились Стивен Хокинг, Макс Тегмарк и новые институты, основанные исследователями из [Кембриджа](https://www.cser.ac.uk/about-us/), [Массачусетского технологического института](https://futureoflife.org/), [Оксфорда](https://www.fhi.ox.ac.uk/) и других мест.",
    "articles--existential-risks:073": "В оставшейся части этой статьи мы расскажем о самых больших рисках для цивилизации, в том числе о потенциально более серьёзных рисках, чем ядерная война и изменение климата. Затем мы приведём доводы в пользу того, что снижение этих рисков может быть самым важным делом вашей жизни, и объясняем, что именно вы можете сделать, чтобы помочь. Если вы хотели бы использовать свою карьеру для работы над этими проблемами, мы также можем предоставить [индивидуальную поддержку](https://80000hours.org/speak-with-us/?int_campaign=existential-risks).",
    "articles--existential-risks:075": "_Время чтения: 25 минут_",
    "articles--existential-risks:077": "## Какова вероятность того, что вас убьет астероид? Обзор естественных экзистенциальных угроз",
    "articles--existential-risks:079": "Шанс вымирания 1 к 10 миллионам в ближайшие 50 лет, — согласно оценкам многих людей, — не соответствует реальности. Естественные экзистенциальные угрозы можно довольно точно оценить по истории, и их шансы гораздо выше.",
    "articles--existential-risks:081": "Столкновение Земли с километровым астероидом может привести к уничтожению цивилизации. Изучая исторические данные и отслеживая объекты в небе, астрономы могут оценить риск столкновения астероида такого размера с Землей как 1 к 5 000 в столетие.[^:`> Чтобы привести к вымиранию человечества, столкнувшийся объект скорее всего должен быть больше 1 км в диаметре (а если точнее, то где-то в районе 3-10 км). На Земле произошло по крайней мере пять, а может и более десятка массовых вымираний, и по крайней мере некоторые из них, по всей видимости, были вызваны столкновениями (\\[9\\], стр. 81 и далее). В частности, К-Т вымирание, случившееся 65 миллионов лет назад, в результате которого вымерли динозавры, связывают с ударом астероида диаметром от 10 до 15 км на полуострове Юкатан. По оценкам, тело диаметром 1 км или больше сталкивается с Землей примерно раз в 0,5 миллиона лет. Мы знаем лишь о небольшой части потенциально опасных тел.",
    "articles--existential-risks:083": "Бостром, Ник. \"Экзистенциальные риски: Анализ сценариев вымирания человечества и связанных с ним опасностей.\" (2002). [Архивная ссылка](https://web.archive.org/web/20171022043143/https://nickbostrom.com/existential/risks.html), доступ проверен 21.10.2017.`] Это выше, чем шансы большинства людей попасть в авиакатастрофу (примерно 1 к 5 миллионам за рейс), и уже примерно в 1000 раз выше, чем шанс один к десяти миллионам, который оценили некоторые люди.[^:` Указаны шансы упасть в Атлантический океан на самолете A330, управляемом компанией Virgin, летевшем из Хитроу в Кеннеди (1 к 5,4 миллиона). Таким образом, вам нужно сделать 1000 полётов, чтобы ваша вероятность попадания в авиакатастрофу сравнялась с вероятностью попасть в катастрофу из-за удара астероида.",
    "articles--existential-risks:085": "_Краткий курс по вероятностям_, The Economist, 2015.",
    "articles--existential-risks:087": "[Web](https://www.economist.com/blogs/gulliver/2015/01/air-safety), доступ проверен 14.10.2017`]",
    "articles--existential-risks:089": "Некоторые утверждают, что хотя километровый объект был бы катастрофой, его недостаточно, чтобы привести к вымиранию человечества, поэтому данная оценка риска скорее из высоких. Но с другой стороны, существуют и другие природные риски, такие как супервулканы.[^:` Достаточно большой супервулкан может вызвать долгую зиму, которая тоже положит конец жизни. К другим природным рискам можно отнести особо смертоносную пандемию, вспышка близкой к нам сверхновой или гамма-всплеск, или же вызванное естественными причинами резкое изменение климата.`].",
    "articles--existential-risks:091": "При этом всём естественные риски всё ещё довольно малы в абсолютных числах. В статье доктора Тоби Орда, которая выйдет в ближайшее время, подсчитано, что если сложить все естественные риски вместе, то вероятность вымирания за столетие вряд ли превысит 1 к 300.[^:`С кратким изложением работы можно ознакомиться в лекции \"Доктор Тоби Орд: Будем ли мы причиной собственного вымирания? Естественные и антропогенные риски вымирания\", прочитанной в CSER в Кембридже в 2015 году. [Ссылка](https://www.youtube.com/watch?v=DCfLheUxHEI).`].",
    "articles--existential-risks:093": "К сожалению, как мы сейчас покажем, естественные риски ничтожны по сравнению с антропогенными. Именно поэтому риск вымирания стал особенно острой проблемой.",
    "articles--existential-risks:095": "## История прогресса, вплоть до начала самой опасной эпохи в истории человечества",
    "articles--existential-risks:097": "Если посмотреть на историю в масштабе тысячелетий, то суть можно выразить так: долгое время почти все были бедными, а затем, в 18 веке, все изменилось.[^:` График взят из Maddison, Angus (2007): \"Contours of the World Economy, 1-2030 AD. Essays in Macro-Economic History,\" Oxford University Press, ISBN 978-0-19-922721-1, стр. 379, таблица A.4.`]",
    "articles--existential-risks:099": "![Стремительный экономический рост создал условия, из-за которых мы сейчас сталкиваемся с антропогенными экзистенциальными угрозами](https://80000hours.org/wp-content/uploads/2017/10/something-weird-happened.jpg)[` `].",
    "articles--existential-risks:101": "Это было вызвано промышленной революцией — возможно, самым важным событием в истории.",
    "articles--existential-risks:103": "Росло не только богатство. Следующая диаграмма показывает, что в долгосрочной перспективе продолжительность жизни, энергопотребление и демократия быстро росли, в то время как процент живущих в бедности резко сократился.[^:`_How big a deal was the Industrial Revolution?_, by Luke Muehlhauser, 2017, [Архивная ссылка](https://web.archive.org/web/20171022033906/http://lukemuehlhauser.com/industrial-revolution/), доступ проверен 21.10.2017.`]",
    "articles--existential-risks:105": "![](https://80000hours.org/wp-content/uploads/2017/10/luke1.jpg)[`График подготовлен Люком Мелхаузером в 2017 году.`]",
    "articles--existential-risks:107": "Уровень грамотности и образования также значительно вырос:",
    "articles--existential-risks:109": "![](https://80000hours.org/wp-content/uploads/2017/10/Literate-and-illiterate-world-population.jpg)[` Image source.`]",
    "articles--existential-risks:111": "Наблюдения показывают, что с повышением уровня богатства люди также [становятся счастливее](https://80000hours.org/articles/money-and-happiness/).",
    "articles--existential-risks:113": "В книге _Лучшее в нас_ Стивен Пинкер утверждает, что уровень насилия снижается.[^:`Pinker, S., 2011. The better angels of our nature: The decline of violence in history and its causes. Penguin uk. [Web](https://www.amazon.com/Better-Angels-Our-Nature-Violence/dp/0143122010)`].",
    "articles--existential-risks:115": "Личных свобод стало больше, а уровни расизма, сексизма и гомофобии снизились.",
    "articles--existential-risks:117": "Многие люди считают, что мир становится хуже,[^:`Результаты разных опросов могут значительно отличаться в оценках того, насколько люди пессимистичны в отношении будущего, но многие из них находят, что большинство настроено пессимистично. Например, недавний государственный опрос в Великобритании показал, что 71% респондентов считает мир ухудшающимся.",
    "articles--existential-risks:119": "_Declinism: is the world actually getting worse? _, Pete Etchells, The Guardian, 2015, [Архивная ссылка](https://web.archive.org/web/20171017102431/https://www.theguardian.com/science/head-quarters/2015/jan/16/declinism-is-the-world-actually-getting-worse), доступ проверен 17.10.2017`] и в этом есть своя правда — в современной цивилизации существуют ужасные вещи, вроде промышленного животноводства. Но, как видно из приведенных данных, многие важные показатели прогресса значительно улучшились.",
    "articles--existential-risks:121": "Так или иначе, что бы вы не думали про прошлое, если смотреть в будущее, то улучшение технологий, политической организации общества и личных свобод сможет дать нашим потомкам возможность решить наши текущие проблемы, а также жить намного лучше.[^:`Становится ли мир лучше?",
    "articles--existential-risks:123": "Хотя есть причины считать, что большинство показателей прогресса увеличивается (как показано в статье), есть некоторые аспекты, в которых жизнь могла стать хуже. Например, в книге [_Sapiens_](https://www.amazon.com/Sapiens-Humankind-Yuval-Noah-Harari/dp/0062316095) Юваль Харари утверждает, что в современную эпоху усилились проблемы одиночества и психического здоровья, в то время как ощущения значимости и смысла могли снизиться. Мы скептически относимся к тому, что эти минусы перевешивают плюсы, но сложно сказать наверняка, какова ситуация на самом деле.",
    "articles--existential-risks:125": "Более весомые аргументы в пользу того, что мир становится хуже, возникают в контексте нашего воздействия на животных. В частности, с 1960-х годов резко выросло промышленное животноводство, и сейчас [где-то более 30 миллиардов животных](https://80000hours.org/problem-profiles/factory-farming/) ежегодно живут в ужасных условиях на фабриках. Если нас волнуют страдания этих животных, то это может перевесить наши успехи в сфере человеческого благополучия.",
    "articles--existential-risks:127": "Учитывая все эти аргументы, мы не можем однозначно утверждать, что суммарное благополучие выросло. Однако более важным является вопрос того, что нас ждет в будущем.",
    "articles--existential-risks:129": "Будет ли мир лучше?",
    "articles--existential-risks:131": "Мы считаем, что до тех пор, пока человечество существует, развитие технологий и моральный прогресс дают нам возможности справиться с самыми серьёзными социальными проблемами, а также жить гораздо лучше в будущем. Если отодвинуть в сторону экзистенциальные угрозы, то многие конкретные глобальные проблемы могут быть решены, если уровни богатства, технологического развития, а также морального и политического прогресса будут и дальше повышаться.",
    "articles--existential-risks:133": "Например, в случае с промышленным животноводством мы ожидаем, что по мере того, как люди будут становиться богаче, проблема будет уменьшаться. Во-первых, богатые люди более склонны к \"этичному\" потреблению, потому что могут себе это позволить. Во-вторых, технологии способны положить конец промышленному животноводству при помощи заменителей мяса, искуственно выращенного мяса или более гуманных методов ведения сельского хозяйства. В-третьих, забота о других живых существах, как мы видим, увеличилась с течением времени (\"расширяющийся круг заботы\"), поэтому мы ожидаем, что в будущем люди будут ещё больше заботиться о благополучии животных.",
    "articles--existential-risks:135": "Если посмотреть ещё шире, то в целом мы ожидаем, что будущее будет лучше, потому что люди сами этого хотят. Чем больше технологической мощи и личных свобод мы имеем, тем проще людям реализовывать свои ценности. Поскольку люди _хотят_ жить хорошо, улучшение будущего — более вероятный сценарий, чем его ухудшение.",
    "articles--existential-risks:137": "При этом остаётся много вопросов. Например, многие наши ценности в какой-то степени противоречат другим, и это может приводить к конфликтам. Вопросы о том, что ждет нас в будущем, также мало изучены. Поэтому хотя мы и ожидаем, что в будущем будет лучше, мы также признаём, что в нашем суждении присутствует значительная степень неопределённости.`] Можно будет покончить с бедностью, предотвратить изменение климата, облегчить многие страдания — список лишь продолжается.",
    "articles--existential-risks:139": "Но также обратите внимание на фиолетовую линию на втором графике: _военный потенциал_. Данные о военном потенциале были взяты из оценки глобальной военной мощи историка Иэна Морриса. Как видите, эта линия тоже резко стремится вверх.",
    "articles--existential-risks:141": "Проблема заключается в следующем: улучшение технологий может таить в себе как огромные выгоды, так и огромные риски.",
    "articles--existential-risks:143": "Каждый раз, когда мы получаем доступ к новым технологиям, в большинстве случаев они приносят огромную пользу. Но есть так же шанс, что эти технологии будут обладать настолько высоким разрушительным потенциалом, что мы не сможем обеспечить себе необходимую безопасность для их использования.",
    "articles--existential-risks:145": "И поэтому, хотя нынешнее поколение живет в самый благополучный период в истории человечества, он, возможно, также является самым опасным.",
    "articles--existential-risks:147": "Первой разрушительной технологией такого рода стало ядерное оружие.",
    "articles--existential-risks:149": "## История ядерного оружия: череда крайне опасных ситуаций",
    "articles--existential-risks:151": "Сегодня мы все думаем о ядерной программе Северной Кореи, но нынешние события — лишь одна глава в длинной истории крайне опасных ситуаций.",
    "articles--existential-risks:153": "Только во время Кубинского ракетного кризиса мы несколько раз были близки к ядерной войне.[^:` Больше об истории подобных ситуаций можно узнать в [нашем подкасте с доктором Тоби Ордом.](https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/)`] В одном из случаев американцы решили, что если один из их самолетов-шпионов будет сбит, то они немедленно начнут вторжение на Кубу без дополнительного заседания Военного совета. На следующий день самолет-шпион был сбит. Кеннеди всё равно созвал совет и принял решение не вводить войска.",
    "articles--existential-risks:155": "Вторжение на Кубу вполне могло привести к ядерной войне; позже выяснилось, что Кастро выступал за ядерное возмездие, даже если \"это привело бы к полному уничтожению Кубы\". Некоторые из командиров пусковых установок на Кубе также имели независимые полномочия наносить удары по американским силам тактическим ядерным оружием в случае вторжения.",
    "articles--existential-risks:157": "В другом инциденте российская атомная подводная лодка пыталась контрабандой переправить материалы на Кубу, когда ее обнаружил американский флот. Флот начал сбрасывать фиктивные глубинные бомбы, чтобы заставить подводную лодку всплыть. Русский капитан подумал, что это настоящие глубинные бомбы и что, пока не было радиосвязи, началась третья мировая война. Он приказал нанести ядерный удар по американскому флоту одной из ядерных торпед.",
    "articles--existential-risks:159": "К счастью, для такого приказа было необходимо разрешение от других старших офицеров. Один их них, Василий Архипов, был против, и таким образом предотвратил войну.",
    "articles--existential-risks:161": "![Благодаря Василию Архипову мы чудом предотвратили глобальный риск катастрофы от ядерного оружия](https://80000hours.org/wp-content/uploads/2017/10/VA.jpg)[`Спасибо вам, Василий Архипов.`]",
    "articles--existential-risks:163": "Сопоставив все эти события вместе, Кеннеди позже говорил, что вероятность ядерной войны была \"между 25 и 50 процентами\".[^:`> Пятьдесят лет назад Кубинский ракетный кризис поставил мир на грань ядерной катастрофы. Во время конфликта президент Джон Ф. Кеннеди считал, что вероятность эскалации войны была \"между 25 и 50 процентами\", и то, что мы узнали в последующие десятилетия, не даёт причин сомневаться в этой оценке. Такой конфликт мог привести к гибели 100 миллионов американцев и более 100 миллионов русских.",
    "articles--existential-risks:165": "_At 50, the Cuban Missile Crisis as Guide_, Graham Allison, The New York Times, 2012,",
    "articles--existential-risks:167": "[Архивная ссылка](https://web.archive.org/web/20171017104052/http://www.nytimes.com/2012/06/16/opinion/at-50-the-cuban-missile-crisis-as-guide.html), доступ проверен 17.10.2017`]",
    "articles--existential-risks:169": "Подобных опасных столкновений с Россией было немало, даже после холодной войны: на Википедии [есть список](https://en.wikipedia.org/wiki/List_of_nuclear_close_calls). И это лишь те, о которых нам известно.",
    "articles--existential-risks:171": "Сегодня эксперты по ядерным вопросам обеспокоены напряжённостью между Индией и Пакистаном (обе страны обладают ядерным оружием) не меньше, чем Северной Кореей.[^:`Оценки шансов ядерного удара по гражданской цели указаны по ссылке ниже, на рисунке в разделе \"What is the probability that a nuclear bomb will be dropped on a civilian target in the next decade?” Обратите внимание, что один эксперт оценил шанс ядерного удара по гражданской цели в ближайшие 10 лет менее чем в 1%.",
    "articles--existential-risks:173": "Правда ли, что экспертов больше беспокоят индийско-пакистанские отношения, чем Северная Корея?",
    "articles--existential-risks:175": "> Первое место в списке конфликтов, вызывающих опасения экспертов, занимает Индия-Пакистан. Оба государства разработали ядерное оружие вне юрисдикции Договора о нераспространении ядерного оружия, оба государства имеют ограниченные военные возможности, что может стать причиной раннего применения, и оба государства, как известно, имеют планы на случай непредвиденных обстоятельств, которые предполагают первые ядерные удары по военным целям (хоть их публичные заявления и намеренно расплывчаты).",
    "articles--existential-risks:177": "_We’re Edging Closer To Nuclear War_, Milo Beckman, FiveThirtyEight, 2017,",
    "articles--existential-risks:179": "[Архивная ссылка](https://web.archive.org/web/20171017104351/https://fivethirtyeight.com/features/were-edging-closer-to-nuclear-war/), доступ проверен 17.10.2017`]",
    "articles--existential-risks:181": "Главная проблема заключается в том, что несколько стран располагают крупными ядерными арсеналами и могут применить их в считанные минуты. Это означает, что ложная тревога или иная ошибка может быстро перерасти в полномасштабную ядерную войну, особенно в период напряженных международных отношений.",
    "articles--existential-risks:183": "Сможет ли ядерная война привести к гибели цивилизации? Первоначально считалось, что ядерный взрыв может быть настолько горячим, что воспламенит атмосферу и сделает Землю непригодной для жизни. Но учёные решили, что такой исход достаточно маловероятен. Это позволило нам провести \"безопасные\" испытания ядерного оружия, и теперь мы знаем, что такого не случится.",
    "articles--existential-risks:185": "В 1980-х годах люди опасались, что пепел от горящих зданий погрузит Землю в очень долгую зиму, которая сделает невозможным выращивание сельскохозяйственных культур в течение десятилетий.[^:`Когда \"ядерная зима\" стала предметом беспокойства?",
    "articles--existential-risks:187": "> \"Ядерная зима\", и предшествующая ей концепция, \"ядерные сумерки\", относятся к ядерным событиям. Ядерная зима стала интересна науке в 1980-х годах, после того как стало ясно, что более ранняя гипотеза про разрушение озонового слоя из-за выбросов NOx от огненных шаров начинает терять свою достоверность. Именно в этом контексте климатическое воздействие сажи от пожаров было \"случайно обнаружено\" и вскоре стало основной теорией в области климатических последствий ядерной войны. В этих модельных сценариях предполагалось, что различные облака, содержащие сажу в каком-то количестве, образуются над городами, нефтеперерабатывающими заводами и сельскими ракетными шахтами. После того, как исследователи задают количество сажи, моделируются климатические эффекты этих облаков. Термин «ядерная зима» был придуман в 1983 году Ричардом П. Турко в связи с одномерной (1-D) компьютерной моделью, созданной для изучения идеи «ядерных сумерек». На основе этой 1-D модели был сделан вывод, что огромное количество сажи и дыма будет годами держаться в воздухе и станет причиной сильного падения температуры на всей планете. Турко позже дистанцировался от этих радикальных 1-D выводов.\"",
    "articles--existential-risks:189": "Статья \"Nuclear Winter\" на Википедии, [архивная ссылка](https://web.archive.org/web/20171030214107/https://en.wikipedia.org/wiki/Nuclear_winter), доступ проверен 30.10.2017.`] Современные климатические модели предполагают, что сильная ядерная зима, в результате которой все умрут, очень маловероятна (хотя нельзя сказать наверняка, из-за [неопределённости моделей](https://concepts.effectivealtruism.org/concepts/uncertainty-about-models/).[^:`Модели климата содержат значительную неопределённость, а это значит, что реальные риски запросто могут быть выше. Более того, из-за самого факта наличия неопределённости в моделях сложно присуждать очень низкие вероятности большинству рисков. Об этом можно почитать в следующей работе:",
    "articles--existential-risks:191": "Ord, T., Hillerbrand, R., & Sandberg, A. (2010). Probing the improbable: methodological challenges for risks with low probabilities and high stakes. Journal of Risk Research, 13(2), стр. 191-205. arXiv:0810.5515v1, [ссылка](http://amirrorclear.net/files/probing-the-improbable.pdf).`])",
    "articles--existential-risks:193": "Однако даже \"умеренная\" ядерная зима всё равно может привести к массовому голоду.[^:`Ожидаемая тяжесть ядерной зимы всё ещё обсуждается, и Open Philantropy [недавно выделила средства](https://www.openphilanthropy.org/focus/global-catastrophic-risks/miscellaneous/rutgers-university-nuclear-conflict-climate-modeling) на дальнейшее исследование этой темы.`] По этой и другим причинам ядерная война была бы чрезвычайно дестабилизирующей для мира, и неизвестно, сможет ли цивилизация восстановиться после неё.",
    "articles--existential-risks:195": "Какова вероятность того, что ядерная война навсегда уничтожит цивилизацию? Это очень трудно оценить, но похоже, что вероятность такого события в следующем столетии превышает 0,3%. Если это так, то риски от ядерного оружия будут выше, чем все природные риски вместе взятые. ([Подробнее о ядерных рисках](https://80000hours.org/problem-profiles/nuclear-security/).)",
    "articles--existential-risks:197": "Вот почему 1950-е годы стали началом новой эры для человечества. Впервые в истории у небольшого числа людей, принимающих решения, появилась возможность разрушить весь мир. Теперь самой большой угрозой для собственного выживания являемся мы сами, и это делает сегодняшний день самым опасным в истории человечества.",
    "articles--existential-risks:199": "И ядерное оружие — не единственный способ, которым мы можем положить конец цивилизации.",
    "articles--existential-risks:201": "## Насколько велики риски, связанные с изменением климата?",
    "articles--existential-risks:203": "В 2015 году президент Обама [сказал в своем обращении \"О положении дел в стране\"](https://web.archive.org/web/20171017105713/http://edition.cnn.com/2015/01/21/us/climate-change-us-obama/index.html), что \"ни одна проблема не представляет столь большой угрозы для будущих поколений, как изменение климата\".",
    "articles--existential-risks:205": "Безусловно, изменение климата представляет собой серьёзный риск для цивилизации.",
    "articles--existential-risks:207": "Наиболее вероятный результат — 2-4 градуса потепления.[^:` См. Box SPM.1.1 в разделе B, [Summary for Policymakers](https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_SPM_final.pdf) of the Working Group I Contribution to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change.`] Это плохо, но такие изменения не ставят под угрозу выживание нашего вида.",
    "articles--existential-risks:209": "Однако [некоторые оценки](https://www.huffingtonpost.com/michael-e-mann/the-fat-tail-of-climate-change-risk_b_8116264.html) дают 10% вероятность потепления свыше 6 градусов, и где-то 1% шанс потепления на 9 градусов.",
    "articles--existential-risks:211": "Таким образом, похоже, что вероятность масштабной климатической катастрофы (в результате эмиссий CO2) примерно равна вероятности ядерной войны.",
    "articles--existential-risks:213": "Но, как мы утверждаем в нашем [обзоре проблемы изменения климата](https://80000hours.org/problem-profiles/climate-change/), даже потепление в 13 градусов вряд ли само по себе приведёт к вымиранию человечества. Поэтому исследователи, изучающие эти вопросы, считают риск ядерной войны более серьёзным (из-за вероятности непосредственного вымирания в результате ядерной зимы), чем риск от изменения климата, и тут мы с ними согласны.",
    "articles--existential-risks:215": "Тем не менее, изменение климата является серьёзной проблемой, и его дестабилизирующие последствия могут усугубить другие риски, в том числе риск ядерного конфликта. Это нужно учитывать в нашей оценке рисков данной проблемы.",
    "articles--existential-risks:217": "## Какие новые технологии могут быть столь же опасными, как ядерное оружие?",
    "articles--existential-risks:219": "Изобретение ядерного оружия привело к возникновению антиядерного движения всего десятилетие спустя, а движение защитников окружающей среды вскоре приняло на вооружение борьбу с изменением климата.",
    "articles--existential-risks:221": "Меньше внимания уделяется тому факту, что новые технологии будут создавать новые катастрофические риски. Вот почему нам необходимо движение, которое занимается защитой цивилизации в целом.",
    "articles--existential-risks:223": "Предсказывать будущее технологий сложно, но поскольку у нас всего одна цивилизация, нам нужно стараться изо всех сил. Вот несколько кандидатов на роль следующей технологии, потенциально столь же опасной, как ядерное оружие.",
    "articles--existential-risks:225": "В 1918-1919 годах от испанского гриппа умерло более 3% населения Земли.[^:`> По разным оценкам, от него погибло от 3% до 6% мирового населения.",
    "articles--existential-risks:227": "_World War One’s role in the worst ever flu pandemic_, John Mathews, The Conversation, 2014, [архивная ссылка](https://web.archive.org/web/20171027035359/https://theconversation.com/world-war-ones-role-in-the-worst-ever-flu-pandemic-29849), доступ проверен 27.10.2017.",
    "articles--existential-risks:229": "> При населении мира в 1811 миллионов человек, 30 миллионов смертей привели бы к уровню смертности в 16,6 на тысячу человек. Это в три раза выше, чем в богатых странах, но вполне в пределах нормы для бедных стран. Если оценивать количество смертей от гриппа в 50-100 миллионов, то числа были бы в районе 27,6-55,2 на тысячу.",
    "articles--existential-risks:231": "Patterson, K.D. and Pyle, G.F., 1991. The geography and mortality of the 1918 influenza pandemic. Bulletin of the History of Medicine, 65(1), p.4.",
    "articles--existential-risks:233": "[Архивная ссылка](https://web.archive.org/web/20171022101640/https://pida.nihlibrary.com/sites/pida.nihlibrary.com/files/pdf_files/1991_K.David%20Patterson_The%20geography%20and%20mortality%20of%20the%201918%20influenza%20pandemic..pdf), доступ проверен 22.10.2017.",
    "articles--existential-risks:235": "> Дальнейшие исследования на тему пандемии испанского гриппа приводили к регулярному пересмотру значений предполагаемой глобальной смертности в сторону увеличения: согласно изначальным расчетам 1920-х годов, она составляла примерно 21,5 миллиона человек. В работе 1991 года смертность была пересчитана, с новым значением в диапазоне 24,7-39,3 миллиона человек. В данной работе предполагается, что она была порядка 50 миллионов. Однако следует признать, что даже эта огромная цифра может быть значительно ниже реальной — вплоть до разницы в 100%.",
    "articles--existential-risks:237": "Johnson, N.P. and Mueller, J., 2002. Updating the accounts: global mortality of the 1918-1920\" Spanish\" influenza pandemic. Bulletin of the History of Medicine, 76(1), стр. 105-115.",
    "articles--existential-risks:239": "[Ссылка](https://muse.jhu.edu/article/4826/summary)`] Если бы такая пандемия вспыхнула сегодня, её было бы ещё труднее сдерживать из-за быстрого глобального транспорта.",
    "articles--existential-risks:241": "Однако больше тревоги вызывает то, что вскоре может появиться возможность генетически сконструировать вирус, который будет таким же заразным, как испанский грипп, но при этом более смертоносным, и который сможет годами распространяться незамеченным.",
    "articles--existential-risks:243": "Это было бы оружие с разрушительной мощью ядерного, но чьё применение гораздо труднее предотвратить. Ядерное оружие требует огромных заводов и редких материалов для производства, что делает его относительно легким для контроля. Искусственные вирусы можно потенциально создать в лаборатории с парой докторов биологических наук. Более того, в 2006 году The Guardian смогли заказать по почте сегменты вымершего вируса оспы.[^:`_Revealed: the lax laws that could allow assembly of deadly virus DNA: Urgent calls for regulation after Guardian buys part of smallpox genome through mail order_, The Guardian, 2006,",
    "articles--existential-risks:245": "[Архивная ссылка](https://web.archive.org/web/20171022042133/https://www.theguardian.com/world/2006/jun/14/terrorism.topstories3), доступ проверен 21.10.2017.`] Некоторые террористические группировки уже заявляли о своём интересе к подобному оружию неизбирательного действия. ([Подробнее о рисках пандемий](https://80000hours.org/problem-profiles/biosecurity/).)",
    "articles--existential-risks:247": "![В 2006 году The Guardian смогли заказать по почте сегменты вымершего вируса оспы. Эксперты предполагают, что синтетические патогены потенциально могут представлять глобальный катастрофический риск.](https://80000hours.org/wp-content/uploads/2017/10/smallpox.png)[`Кто заказывал оспу? Источник: The Guardian`]",
    "articles--existential-risks:249": "Ещё одна новая технология с огромной потенциальной мощью — искусственный интеллект.",
    "articles--existential-risks:251": "Причина, по которой люди стали главными на планете, а не шимпанзе, полностью заключается в интеллекте. Размеры и мощь наших мозгов позволили нам установить поразительный контроль над окружающим миром, хоть и физически мы намного слабее шимпанзе.",
    "articles--existential-risks:253": "Что же произойдёт, если однажды мы создадим нечто гораздо более разумное, чем мы сами?",
    "articles--existential-risks:255": "В 2017 году 350 исследователей, публиковавших рецензируемые исследования в области искусственного интеллекта на ведущих конференциях, были опрошены о том, когда, по их мнению, будет разработан компьютер с человеческим уровнем интеллекта: то есть машина, способная выполнять все рабочие задачи лучше, чем человек.",
    "articles--existential-risks:257": "Медианной оценкой была 50% вероятность того, что мы разработаем высокоуровневый машинный интеллект (ВМИ) через 45 лет, и 75% к концу века.[^:`> Исследователи считают, что существует 50% вероятность того, что ИИ превзойдет человека во всех задачах через 45 лет.\n>\n> Респондентов спросили, какое влияние ВМИ окажет на человечество в долгосрочной перспективе: положительное или отрицальное. Они оценивали вероятности различных исходов по пятибальной шкале. Медианными оценками были 25% для \"хорошего исхода\" и 20% для \"чрезвычайно хорошего исхода\". В свою очередь, медианная оценка для \"плохого\" исхода была 10%, и 5% для исхода, сформулированного как \"чрезвычайно плохой (например, вымирание человечества)\".",
    "articles--existential-risks:261": "Grace, K., Salvatier, J., Dafoe, A., Zhang, B. and Evans, O., 2017. When Will AI Exceed Human Performance? Evidence from AI Experts. arXiv preprint arXiv:1705.08807.",
    "articles--existential-risks:263": "[Ссылка](https://arxiv.org/abs/1705.08807)`]",
    "articles--existential-risks:265": "!img~ class=\"alignnone size-full wp-image-40209\" width=\"602\" height=\"389\" ~[График прогнозов экспертов, Grace et al: медианной оценкой был 50% шанс того, что мы разработаем высокоуровневый машинный интеллект в течение 45 лет](https://80000hours.org/wp-content/uploads/2017/10/probability-of-HLMI.png)",
    "articles--existential-risks:267": "Такие вероятности сложно оценивать, и исследователи говорили очень разные цифры, в зависимости от конкретной формулировки вопроса.[^:`Если вам интересно почитать обсуждение этой непоследовательности в оценках, есть пост от AI Impacts, \"Some Survey Results,\" [архивная ссылка](https://web.archive.org/web/20171030220008/https://aiimpacts.org/some-survey-results/), доступ проверен 30.10.2017. К примеру:",
    "articles--existential-risks:269": "> Вопросы, связанные с конкретными видами работы, существенно влияют на прогнозы про ВМИ. Когда мы задавали некоторым людям вопросы про то, когда ИИ начнёт успешно выполнять несколько конкретных видов деятельности, а потом про все виды работы, которую делает человек (что считается подвидом всех возможных задач вообще), мы получали сильно более поздние сроки, чем когда мы просто спрашивали про ВМИ. Если мы при этом просили назвать вероятности для конкретных лет, то для \"через 20 лет\" цифры отличались в 1000 раз! (10% для общего вопроса, против 0,01% для конкретных вопросов) Если же мы просили назвать годы для некоторых вероятностей, общий вопрос получал оценку \"через 40 лет\" для 50% вероятности, а формулировки про конкретные виды работы получали ответ \"через 90 лет\" для тех же 50%.\n>\n> Люди регулярно дают более поздние прогнозы, если спрашивать их про \"вероятность через N лет\", чем если спрашивать про \"год, в который вероятность будет M\". Мы наблюдали это в контексте общего вопроса про ВМИ и конкретных вопросов про виды работы, а также в большинстве случаев, когда мы тестировали эти вопросы на респондентах с MTurk ранее (Amazon Mechanical Turk, сервис аутсорсинга задач, которые ещё не могут выполняться компьютерами). Например, для общего вопроса про ВМИ, если спрашивать, когда вероятность изобретения ВМИ достигнет 50%, медианным ответом будет \"через 40 лет\", но если спрашивать, какова вероятность изобретения ВМИ через 40 лет, медианный ответ будет \"30%\".`] Тем не менее, есть причины полагать, что существует разумный шанс изобретения некого революционного машинного интеллекта в следующем столетии.",
    "articles--existential-risks:273": "Какие риски может представлять это развитие? Первопроходцы в области вычислительной техники, такие как Алан Тьюринг и Марвин Мински, высказывали опасения по поводу рисков, связанных с мощными компьютерными системами,[^:`> Некоторые заявления таких ученых, как Алан Тьюринг, И. Дж. Гуд и Марвин Мински, указывали на опасения (по философским соображениям) насчёт того, что сверхинтеллект может захватить контроль.",
    "articles--existential-risks:275": "См. сноски 15-18 в статье _Existential risk from artificial intelligence_ на Википедии, [архивная ссылка](https://web.archive.org/web/20171022041500/https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence), доступ проверен 21.10.2018.`] и эти риски до сих пор актуальны. Мы говорим не о \"злых\" компьютерах, а скорее о том, что мощная ИИ-система может быть использована какой-то группой людей для установления контроля над миром, или как-то иначе использована в неправильных целях. Если бы СССР разработал ядерное оружие на 10 лет раньше, чем США, то СССР мог бы стать доминирующей мировой державой. Мощные компьютерные технологии могут представлять аналогичные риски.",
    "articles--existential-risks:277": "Другая проблема заключается в том, что запуск такой системы может привести к непредвиденным последствиям, поскольку сложно предсказать действия чего-то более умного, чем мы сами. Достаточно мощной системой также может быть сложно управлять, и поэтому её запуск может оказаться труднообратимым. Эти опасения были описаны оксфордским профессором Ником Бостромом в работе [_Superintelligence_](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742) и пионером ИИ [Стюартом Расселом](https://www.ted.com/talks/stuart_russell_how_ai_might_make_us_better_people).",
    "articles--existential-risks:279": "Большинство экспертов считают, что совершенствование ИИ приведёт к чрезвычайно положительным результатам, но при этом также не забывают о рисках. В вышеупомянутом опросе, эксперты в области ИИ оценили, что разработка высокоуровневого машинного интеллекта с 10% шансом может привести к \"плохому исходу\", и с 5% шансом к \"чрезвычайно плохому исходу\", такому, как вымирание человечества.[^:`> Исследователи считают, что существует 50% вероятность того, что ИИ превзойдет человека во всех задачах через 45 лет.\n>\n> Респондентов спросили, какое влияние ВМИ окажет на человечество в долгосрочной перспективе: положительное или отрицальное. Они оценивали вероятности различных исходов по пятибальной шкале. Медианными оценками были 25% для \"хорошего исхода\" и 20% для \"чрезвычайно хорошего исхода\". В свою очередь, медианная оценка для \"плохого\" исхода была 10%, и 5% для исхода, сформулированного как \"чрезвычайно плохой (например, вымирание человечества)\".",
    "articles--existential-risks:283": "Grace, K., Salvatier, J., Dafoe, A., Zhang, B. and Evans, O., 2017. When Will AI Exceed Human Performance? Evidence from AI Experts. arXiv preprint arXiv:1705.08807.",
    "articles--existential-risks:285": "[Ссылка](https://arxiv.org/abs/1705.08807)`] И нам скорее всего следует ожидать излишней оптимистичности от этой группы людей, поскольку они зарабатывают на жизнь благодаря этой технологии.",
    "articles--existential-risks:287": "Если сложить эти оценки вместе, то при 75% шансе создания высокоуровневого машинного интеллекта в следующем веке, вероятность крупной катастрофы, связанной с ИИ, составляет 5% от 75%, то есть около 4%. ([Подробнее о рисках, связанных с искусственным интеллектом](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/).)",
    "articles--existential-risks:289": "Люди выражали беспокойство по поводу других новых технологий, таких как некоторые формы геоинженерии и атомного производства, но они кажутся значительно менее неизбежными и, следовательно, менее опасными, чем другие технологии, о которых мы говорили. Более длинный список экзистенциальных рисков можно посмотреть [здесь](https://nickbostrom.com/existential/risks.html).",
    "articles--existential-risks:291": "Больше беспокойств вызывают риски, о которых мы могли ещё не подумать. Если бы вы спросили людей в 1900-м году, что представляет наибольшую угрозу для цивилизации, они вряд ли бы упомянули атомное оружие, генную инженерию или искусственный интеллект, потому что ничто из этого ещё не было изобретено. Вполне возможно, что мы находимся в такой же ситуации относительно следующего века. Грядущие \"неизвестные неизвестные\" могут представлять большую опасность, чем уже известные нам риски.",
    "articles--existential-risks:293": "Каждый раз, когда мы открываем новую технологию, это немного похоже на ставку _против_ одного числа в рулетке. В большинстве случаев мы выигрываем, и технология в целом имеет хорошие последствия. Но каждый раз есть небольшой шанс, что технология будет обладать настолько большим разрушительным потенциалом, что мы не сможем с ней совладать, и тогда мы потеряем всё.",
    "articles--existential-risks:295": "![](https://80000hours.org/wp-content/uploads/2017/10/roulette.jpg)[`У каждой новой технологии беспрецедентными являются как и потенциал, так и опасности. Image source.`]",
    "articles--existential-risks:297": "### Каким будет суммарный риск вымирания человечества, если учесть все эти факторы?",
    "articles--existential-risks:299": "По оценкам многих экспертов, изучающих эти вопросы, суммарная вероятность вымирания человечества в следующем столетии составляет от 1 до 20%.",
    "articles--existential-risks:301": "Например, согласно неофициальному опросу, проведенному в 2008 году на конференции по катастрофическим рискам, эксперты считают вполне вероятной катастрофу, в результате которой погибнет более миллиарда человек, и оценивают вероятность вымирания до 2100 года в 19%.[^:`Sandberg, A. & Bostrom, N. (2008): “Global Catastrophic Risks Survey”, Technical",
    "articles--existential-risks:303": "Report #2008-1, Future of Humanity Institute, Oxford University: стр. 1-5.",
    "articles--existential-risks:305": "[Ссылка](https://www.fhi.ox.ac.uk/reports/2008-1.pdf)`]",
    "articles--existential-risks:307": "<div class=\"tablepress-scroll-wrapper\">",
    "articles--existential-risks:309": "| Риск | Минимум миллиард смертей | Вымирание человечества |\n| --- | --- | --- |\n| Количество смертей из-за молекулярного нанооружия. | 10% | 5% |\n| Суммарное количество смертей из-за сильного ИИ. | 5% | 5% |\n| Суммарное количество смертей из-за всех войн (включая гражданские). | 30% | 4% |\n| Количество смертей из-за самой большой искусственной пандемии. | 10% | 2% |\n| Суммарное количество смертей из-за всех атомных конфликтов. | 10% | 1% |\n| Количество смертей из-за самой большой нанотехнологической аварии. | 1% | 0.5% |\n| Количество смертей из-за самой большой естественной пандемии. | 5% | 0.05% |\n| Суммарное количество смертей из-за всех актов терроризма с применением атомного оружия. | 1% | 0.03% |\n| Общий риск вымирания до 2100 года | n/a | 19% |",
    "articles--existential-risks:321": "</div>",
    "articles--existential-risks:323": "Эти цифры примерно в миллион раз превышают ожидания большинства людей.",
    "articles--existential-risks:325": "В [эпизоде нашего подкаста с Уиллом МакАскиллом](https://80000hours.org/podcast/episodes/will-macaskill-paralysis-and-hinge-of-history/) мы обсуждаем то, почему он оценивает риск вымирания в этом веке примерно в 1%.",
    "articles--existential-risks:327": "В своей книге [_Пропасть: Экзистенциальный риск и будущее человечества_](https://80000hours.org/the-precipice/) Тоби Орд предполагает, что общий экзистенциальный риск в этом веке составляет 1/6, что равноценно броску игральной кости. [Послушать наш подкаст с Тоби можно тут](https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/).",
    "articles--existential-risks:329": "Какие выводы стоит делать из этих оценок? Предположительно, исследователи работают над этими вопросами лишь потому, что считают их очень важными, поэтому мы должны ожидать, что их оценки будут высокими (см. [\"систематическая ошибка отбора\"](https://en.wikipedia.org/wiki/Selection_bias)). Но значит ли это, что мы можем полностью игнорировать их опасения?",
    "articles--existential-risks:331": "С учётом всего этого, какова наша личная оценка? Это очень сложный вопрос, но мы не можем с уверенностью игнорировать эти риски. В целом, мы полагаем, что общий риск скорее всего превышает 3%.",
    "articles--existential-risks:333": "## Почему участие в защите будущего может быть самым важным делом вашей жизни",
    "articles--existential-risks:335": "Насколько приоритетной должна быть работа по снижению этих рисков по сравнению с другими вопросами, такими как глобальная бедность, борьба с раком или улучшение политической системы?",
    "articles--existential-risks:337": "В рамках проекта \"80 000 часов\" мы проводим исследования, чтобы помогать людям находить работу, приносящую пользу обществу. Для этого мы стараемся найти наиболее актуальные проблемы в мире, над которыми необходимо работать. Мы оцениваем различные глобальные проблемы, используя нашу [модель](/articles/problem-framework/), которая сравнивает их по следующим критериям:",
    "articles--existential-risks:339": "- Масштаб — сколько людей затронуто этой проблемой\n- Недооценённость — сколько людей уже работают над ней\n- Разрешимость — насколько легко продвинуться в её решении",
    "articles--existential-risks:343": "Из этой модели следует, что защита будущего является самым главным приоритетом общества. Поэтому если вы хотите принести много пользы миру при помощи своей деятельности, в первую очередь стоит сфокусироваться на этой области.",
    "articles--existential-risks:345": "В следующих нескольких разделах мы рассмотрим эту проблему с точки зрения масштаба, недооценённости и разрешимости, в значительной степени опираясь на _Existential Risk Prevention as a Global Priority_ Ника Бострома и [неопубликованную работу Тоби Орда](https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/), а также на наши исследования.",
    "articles--existential-risks:347": "Во-первых, давайте начнем с масштаба проблемы. Мы утверждали, что вероятность вымирания в следующем столетии составляет более 3%. Насколько это серьёзно?",
    "articles--existential-risks:349": "Одна из цифр, на которую мы можем взглянуть, — это количество людей, которое может погибнуть в результате такой катастрофы. Население планеты в середине этого века будет составлять примерно 10 миллиардов человек. Значит, 3% шанс того, что все умрут, даёт математическое ожидание в 300 миллионов смертей. Это скорее всего больше смертей, чем можно суммарно ожидать за весь следующий век от болезней, связанных с бедностью, вроде малярии.[^:`Каждый месяц от легко предотвратимых болезней, таких, как малярия и диарея, умирают миллионы людей. Но поскольку эти числа стремительно падают, мы не ожидаем, что они привысят 300 миллионов в следующем веке.",
    "articles--existential-risks:351": "> Ежегодная смертность от малярии сократилась с 3,8 миллиона до 0,7 миллиона человек\n>\n> Ежегодная смертность от диареи сократилась с 4,6 миллиона до 1,6 миллиона человек",
    "articles--existential-risks:355": "_Aid Works (On Average)_, Dr Toby Ord, Giving What We Can, [Ссылка](http://studylib.net/doc/13259236/aid-works--on-average--toby-ord-president--giving-what-we)`]",
    "articles--existential-risks:357": "Многие из рассмотренных нами рисков также могут привести к катастрофе \"средней\" тяжести (вместо гибели цивилизации), что может быть намного более вероятным. Опрос, о котором мы рассказывали ранее, показал 10%+ шанс катастрофы, из-за которой в следующем веке может погибнуть более 1 миллиарда людей. Это даёт нам математическое ожидание как минимум ещё в 100 миллионов смертей, и ещё больше страданий среди тех, кто выживет.",
    "articles--existential-risks:359": "Таким образом, даже если мы сосредоточимся только на последствиях для ныне живущего поколения, эти катастрофические риски являются одной из самых серьёзных проблем, стоящих перед человечеством.",
    "articles--existential-risks:361": "Но в таком случае мы сильно недооценим масштабы проблемы, потому что если цивилизации придёт конец, то мы полностью отказываемся и от нашего будущего тоже.",
    "articles--existential-risks:363": "Большинство людей хотят оставить лучший мир для своих внуков, и большинство из нас также считает, что нам нужно в какой-то мере заботиться и о [будущих поколениях](/future-generations/) в целом. В будущем может быть намного больше людей, довольных своей жизнью, чем сейчас, и [нам стоит сколько-то учитывать их интересы](https://80000hours.org/articles/future-generations/). Есть шанс того, что человеческая цивилизация просуществует миллионы лет, поэтому если учитывать последствия рисков для всех будущих поколений, ставки увеличиваются в миллионы раз — в равной степени и для хороших исходов, и для плохих. Как [писал Карл Саган](http://www.jstor.org/stable/20041818) о цене ядерной войны в журнале _Foreign Affairs_:",
    "articles--existential-risks:365": "> Ядерная война ставит под угрозу всех наших потомков, которые будут существовать за всю историю человечества. Даже если наше суммарное население не будет расти, средняя продолжительность жизни будет порядка 100 лет, в условиях типичного периода времени, необходимого для цикла биологической эволюции успешного вида (примерно 10 миллионов лет) — даже в этом случае речь идёт о 500 триллионах человек, которых ещё не было. С учётом этого, ставки для полного вымирания в миллион раз больше, чем для более скромных ядерных войн, которые убивают \"всего лишь\" сотни миллионов людей. Есть много других возможных способов измерять потенциальные потери, которые включают культуру и науку, эволюционную историю планеты и значимость жизней всех наших предков, сделавших вклад в будущее своих потомков. Вымирание сотрёт всё, что построило человечество.",
    "articles--existential-risks:367": "Мы рады, что римляне не позволили человечеству исчезнуть, ведь это позволило существовать всей современной цивилизации. Мы считаем, что на нас лежит такая же ответственность за людей, которые будут после нас, в случае если их жизни будут скорее хорошими (а мы предполагаем, что будут). Подвергать их жизни опасности лишь для того, чтобы сделать себе получше в краткосрочной перспективе, было бы необдуманно и несправедливо."
}
