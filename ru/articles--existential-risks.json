{
    "articles--existential-risks:000": "# Почему нужно снижать экзистенциальные риски?\n## !empty\n<p class=\"entry-meta small\">Автор: <span class=\"byline author vcard\"><a href=\"https://80000hours.org/author/benjamin-todd/\" rel=\"author\" class=\"fn\">Бенджамин Тодд</a></span> · Опубликовано в <time class=\"published\" datetime=\"2017-10-27T23:41:47+00:00\">октябре 2017</time> · Обновлено <time class=\"update\" datetime=\"2022-06-08T00:00:00+00:00\">8 июля, 2022</time></p>",
    "articles--existential-risks:004": "!podcast-player",
    "articles--existential-risks:006": "В 1939 году Эйнштейн пишет Рузвельту:[^:``]",
    "articles--existential-risks:008": "> Есть причины считать возможным запуск цепной ядерной реакции в большой массе урана ... и можно предположить, хоть и с меньшей степенью уверенности, что это позволит создавать чрезвычайно мощные бомбы нового типа.",
    "articles--existential-risks:010": "Всего через несколько лет такие бомбы были созданы. Спустя десять с небольшим лет их число достигло такого уровня, что впервые в истории маленькая группа людей, принимающих решения, могла полностью уничтожить цивилизацию как таковую.",
    "articles--existential-risks:012": "Человечество вступило в новую эру. Теперь мы встретились, помимо природных экзистенциальных рисков,[^:`Ник Бостром определяет понятие [экзистенциального риска](https://www.nickbostrom.com/existential/risks.html) как событие, которое \"может привести к вымиранию человечества или необратимо ограничить его потенциал\". Экзистенциальный риск отличается от [риска глобальной катастрофы (РГК)](https://en.wikipedia.org/wiki/Global_catastrophic_risk) по масштабу — РГК является катастрофичным на масштабе планеты, но включает возможность последующего восстановления. Термин [\"экзистенциальная угроза\"](https://www.theatlantic.com/ideas/archive/2019/06/2020-candidates-say-everything-existential-threat/591967/) обычно используется лишь как усиление, с целью представить некую проблему как более ужасную.`] ещё и с шансом самоуничтожения.",
    "articles--existential-risks:014": "<div class=\"well bg-gray-lighter margin-bottom margin-top padding-top-small padding-bottom-small\">",
    "articles--existential-risks:016": "### Предпочитаете подкаст?",
    "articles--existential-risks:018": "После публикации этой статьи мы записали два **подкаста** на тему экзистенциальных рисков с доктором Тоби Ордом, философом из Оксфорда и доверенным лицом проекта \"80 000 часов\". Мы считаем, что они являются столь же хорошим введением в эту тему, как и данная статья, если не лучше. Их можно послушать тут:",
    "articles--existential-risks:020": "- [Тоби Орд: о \"пропасти\" и различных сценариях будущего, с которыми мы можем столкнуться](https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/)",
    "articles--existential-risks:022": "- [Почему долгосрочное будущее человечества важнее всего, и что с этим делать](https://80000hours.org/podcast/episodes/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/)\n",
    "articles--existential-risks:025": "</div>",
    "articles--existential-risks:027": "<div class=\"panel clearfix \">",
    "articles--existential-risks:029": "[!img~ style=\"width: 30%; margin-left: 10%; margin-top:-10px; float: right;\" ~[book cover](https://80000hours.org/wp-content/uploads/2020/03/the-precipice-3d.jpg)](/the-precipice/)",
    "articles--existential-risks:031": "<div style=\"margin-top: -10px;\">",
    "articles--existential-risks:033": "### Предпочитаете книгу?",
    "articles--existential-risks:035": "У доктора Тоби Орда, философа из Оксфорда и доверенного лица проекта \"80 000 часов\", недавно вышла книга _[\"Пропасть: Экзистенциальный риск и будущее человечества\"](/the-precipice/)_, в которой даётся обзор моральной значимости будущих поколений и того, что мы можем сделать сегодня, чтобы помочь им.",
    "articles--existential-risks:037": "#### Мы вышлем вам книгу по почте (бесплатно)",
    "articles--existential-risks:039": "Присоединяйтесь к рассылке \"80 000 часов\", и мы пришлем вам бесплатный экземпляр книги.",
    "articles--existential-risks:041": "Мы также будем присылать вам свежую информацию о наших исследованиях, вакансии, связанные с работой над экзистенциальными рисками, а также новости от автора.",
    "articles--existential-risks:043": "!newsletter-subscribe",
    "articles--existential-risks:045": "Если вы уже подписаны на нашу рассылку, напишите нам на [book.giveaway@80000hours.org](/cdn-cgi/l/email-protection#89ebe6e6e2a7eee0ffece8fee8f0afeae6e4e4e8fdb2b1b9b9b9b9e1e6fcfbfaa7e6fbee), чтобы получить экземпляр.",
    "articles--existential-risks:047": "</div>",
    "articles--existential-risks:049": "</div>",
    "articles--existential-risks:051": "В эпоху современности, что должно быть нашим самым главным приоритетом как цивилизации? Совершенствование технологий? Помощь бедным? Изменение политической системы?",
    "articles--existential-risks:053": "Вот предложение, которое не так часто обсуждается: в первую очередь мы должны _выжить_.",
    "articles--existential-risks:055": "Пока цивилизация продолжает существовать, у нас есть шанс решить все остальные проблемы и прийти к гораздо лучшему будущему. Но если мы вымрем, это будет конец.",
    "articles--existential-risks:057": "Почему об этом приоритете мало говорят? Вот одна из причин: многие люди ещё не успели заметить, что общая ситуация в мире значительно изменилась, и поэтому не думают, что наше будущее находится под угрозой.",
    "articles--existential-risks:059": "Исследователь в области социальных наук [Спенсер Гринберг](http://www.spencergreenberg.com/) провел опрос среди американцев, чтобы узнать, как они оценивают шансы вымирания человечества в течение 50 лет. Результаты показали, что многие считают шансы крайне низкими: более 30% полагают, что они меньше 1 к 10 миллионам. [^:`Гринберг опрашивал пользователей Mechanical Turk, чей средний возраст — 20-40 лет, а уровень образования, как правило, выше среднего, поэтому опрос не отражает мнения всех американцев. Подробнее смотрите в этом видео:",
    "articles--existential-risks:061": "[Social Science as Lens on Effective Charity: results from four new studies - Spencer Greenberg](https://www.youtube.com/watch?v=tOSpj19eows), таймкод 12:15.",
    "articles--existential-risks:063": "Первоначальный опрос показал, что медианная оценка шансов вымирания в течение 50 лет составляет 1 к 10 миллионам. Гринберг провел три повторных исследования, которые дали более высокие оценки тех же шансов. Самая высокая из них показала медиану 1 к 100 в течение 50 лет. Однако даже в этом случае 39% респондентов все равно предположили, что шансы были ниже 1 к 10 000 (что примерно равно шансам столкновения с астероидом диаметром в 1 км). Во всех случаях более 30% считали, что шансы меньше 1 к 10 миллионам. Сводку всех опросов можно посмотреть [здесь](http://spencergreenberg.com/documents/Comparison%20of%20study%20results%20about%20probability%20of%20human%20extinction.xlsx).",
    "articles--existential-risks:065": "Обратите внимание: когда мы спрашивали людей о шансах вымирания без временных рамок, оценки были гораздо выше. В одном из опросов медиана составила 75%. Их можно понять: человечество _рано или поздно_ вымрет. Это помогает объяснить расхождение с некоторыми другими опросами. Например, исследование \"Climate Change in the American Mind\" (май 2017 года, [архивная ссылка](https://web.archive.org/web/20171031033116/http://climatecommunication.yale.edu/wp-content/uploads/2017/07/Climate-Change-American-Mind-May-2017.pdf)) показало, что средний американец считает, что шансы вымирания в результате изменения климата составляют примерно 1 к 3. Однако в этом исследовании не задавался вопрос о конкретных сроках. Когда Гринберг попытался воспроизвести результат с тем же вопросом, он получил аналогичную цифру. Но когда Гринберг спрашивал о шансах вымирания в результате изменения климата в ближайшие 50 лет, медиана упала всего до 1%. Многие другие исследования также некорректно обращаются с низкими оценками вероятности — люди обычно не дают оценку в 0,00001%, если им явно не предложить такой вариант.",
    "articles--existential-risks:067": "Однако, как можно заметить, подобные опросы обычно дают очень ненадёжные результаты. Ответы могут зависеть от конкретной фомулировки вопроса и от контекста. Отчасти это так, потому что люди очень плохо умеют оценивать очень маленькие вероятности. Из-за этого трудно дать точную оценку того, что думает население в целом, но ничто из этого не опровергает вывод о том, что значительное число людей (скажем, более 25%) считает, что шансы вымирания в краткосрочной перспективе очень-очень малы, и скорее всего ниже, чем вероятность столкновения с астероидом. Более того, ненадёжность этих оценок не добавляет уверенности в том, что человечество рационально учитывает эти риски.`]",
    "articles--existential-risks:069": "Сначала мы тоже считали, что подобные риски крайне малы, но, изучив этот вопрос, мы стали думать иначе. Далее мы увидим, что исследователи, изучающие эти вопросы, считают, что реальный риск вымирания в 1000+ раз выше оценки из опроса, и скорее всего лишь продолжает расти.",
    "articles--existential-risks:071": "Эти опасения положили начало новому движению по защите цивилизации, к которому присоединились Стивен Хокинг, Макс Тегмарк и новые институты, основанные исследователями из [Кембриджа](https://www.cser.ac.uk/about-us/), [Массачусетского технологического института](https://futureoflife.org/), [Оксфорда](https://www.fhi.ox.ac.uk/) и других мест.",
    "articles--existential-risks:073": "В оставшейся части этой статьи мы расскажем о самых больших рисках для цивилизации, в том числе о потенциально более серьёзных рисках, чем ядерная война и изменение климата. Затем мы приведём доводы в пользу того, что снижение этих рисков может быть самым важным делом вашей жизни, и объясняем, что именно вы можете сделать, чтобы помочь. Если вы хотели бы использовать свою карьеру для работы над этими проблемами, мы также можем предоставить вам [индивидуальную поддержку](https://80000hours.org/speak-with-us/?int_campaign=existential-risks).",
    "articles--existential-risks:075": "_Время чтения: 25 минут_",
    "articles--existential-risks:077": "## Какова вероятность того, что вас убьет астероид? Обзор природных экзистенциальных угроз",
    "articles--existential-risks:079": "Шанс вымирания 1 к 10 миллионам в ближайшие 50 лет, — оценка, которую дают многие люди, — не соответствует реальности. Природные экзистенциальные угрозы можно довольно точно оценить по истории, и их шансы гораздо выше.",
    "articles--existential-risks:081": "Столкновение Земли с километровым астероидом может привести к уничтожению цивилизации. Изучая исторические данные и отслеживая объекты в небе, астрономы оценивают риск столкновения астероида такого размера с Землей как 1 к 5000 в столетие.[^:`> Чтобы привести к вымиранию человечества, столкнувшийся объект скорее всего должен быть больше 1 км в диаметре (а если точнее, то где-то в районе 3-10 км). На Земле произошло по крайней мере пять, а может и более десятка массовых вымираний, и по крайней мере некоторые из них, по всей видимости, были вызваны столкновениями (\\[9\\], стр. 81 и далее). В частности, К-Т вымирание, случившееся 65 миллионов лет назад, в результате которого вымерли динозавры, связывают с ударом астероида диаметром от 10 до 15 км на полуострове Юкатан. По оценкам, тело диаметром 1 км или больше сталкивается с Землей примерно раз в 0,5 миллиона лет. Мы знаем лишь о небольшой части потенциально опасных тел.",
    "articles--existential-risks:083": "Bostrom, Nick. \"Existential risks: Analyzing human extinction scenarios and related hazards.\" (2002). [Архивная ссылка](https://web.archive.org/web/20171022043143/https://nickbostrom.com/existential/risks.html), доступ проверен 21.10.2017.`] Это выше, чем шансы большинства людей попасть в авиакатастрофу (примерно 1 к 5 миллионам за рейс), и уже примерно в 1000 раз выше, чем шанс один к десяти миллионам, который дали некоторые люди.[^:` Указаны шансы упасть в Атлантический океан на самолете A330, управляемом компанией Virgin, летевшем из Хитроу в Кеннеди (1 к 5,4 миллиона). Таким образом, вам нужно сделать 1000 полётов, чтобы ваша вероятность попадания в авиакатастрофу сравнялась с вероятностью попасть в катастрофу из-за удара астероида.",
    "articles--existential-risks:085": "_A crash course in probability_, The Economist, 2015.",
    "articles--existential-risks:087": "[Web](https://www.economist.com/blogs/gulliver/2015/01/air-safety), доступ проверен 14.10.2017`]",
    "articles--existential-risks:089": "Некоторые утверждают, что хотя километровый объект был бы катастрофой, его недостаточно, чтобы привести к вымиранию человечества, поэтому данная оценка риска скорее из высоких. Но с другой стороны, существуют и другие природные риски, такие как супервулканы.[^:` Достаточно большой супервулкан может вызвать долгую зиму, которая тоже положит конец жизни. К другим природным рискам можно отнести особо смертоносную пандемию, вспышку близкой к нам сверхновой или гамма-всплеск, или же вызванное естественными причинами резкое изменение климата.`]",
    "articles--existential-risks:091": "При всём этом естественные риски всё ещё довольно малы в абсолютных числах. В статье доктора Тоби Орда, которая выйдет в ближайшее время, подсчитано, что если сложить все естественные риски вместе, то вероятность вымирания за столетие вряд ли превысит 1 к 300.[^:`С кратким изложением работы можно ознакомиться в лекции \"Dr Toby Ord - Will We Cause Our Own Extinction? Natural versus Anthropogenic Extinction Risks\", прочитанной в CSER в Кембридже в 2015 году. [Ссылка](https://www.youtube.com/watch?v=DCfLheUxHEI).`]",
    "articles--existential-risks:093": "К сожалению, как мы сейчас покажем, естественные риски ничтожны по сравнению с антропогенными. Именно поэтому риск вымирания стал особенно острой проблемой.",
    "articles--existential-risks:095": "## История прогресса, вплоть до начала самой опасной эпохи в истории человечества",
    "articles--existential-risks:097": "Если посмотреть на историю в масштабе тысячелетий, то суть можно выразить так: долгое время почти все были бедными, а затем, в 18 веке, все изменилось.[^:` График взят из Maddison, Angus (2007): \"Contours of the World Economy, 1-2030 AD. Essays in Macro-Economic History,\" Oxford University Press, ISBN 978-0-19-922721-1, стр. 379, таблица A.4.`]",
    "articles--existential-risks:099": "![Стремительный экономический рост создал условия, из-за которых мы сейчас сталкиваемся с антропогенными экзистенциальными угрозами](https://80000hours.org/wp-content/uploads/2017/10/something-weird-happened.jpg)[` `].",
    "articles--existential-risks:101": "Это было вызвано промышленной революцией — возможно, самым важным событием в истории.",
    "articles--existential-risks:103": "Росло не только богатство. Следующая диаграмма показывает, что в долгосрочной перспективе продолжительность жизни, энергопотребление и демократия быстро выросли, в то время как процент живущих в бедности резко сократился.[^:`_How big a deal was the Industrial Revolution?_, by Luke Muehlhauser, 2017, [Архивная ссылка](https://web.archive.org/web/20171022033906/http://lukemuehlhauser.com/industrial-revolution/), доступ проверен 21.10.2017.`]",
    "articles--existential-risks:105": "![](https://80000hours.org/wp-content/uploads/2017/10/luke1.jpg)[`График подготовлен Люком Мелхаузером в 2017 году.`]",
    "articles--existential-risks:107": "Уровни грамотности и образования также значительно выросли:",
    "articles--existential-risks:109": "![](https://80000hours.org/wp-content/uploads/2017/10/Literate-and-illiterate-world-population.jpg)[` Image source.`]",
    "articles--existential-risks:111": "С повышением уровня богатства люди также [становятся счастливее](https://80000hours.org/articles/money-and-happiness/).",
    "articles--existential-risks:113": "В книге _\"Лучшее в нас\"_ Стивен Пинкер утверждает, что уровень насилия снижается.[^:`Pinker, S., 2011. The better angels of our nature: The decline of violence in history and its causes. Penguin uk. [Web](https://www.amazon.com/Better-Angels-Our-Nature-Violence/dp/0143122010)`]",
    "articles--existential-risks:115": "Личных свобод стало больше, а уровни расизма, сексизма и гомофобии снизились.",
    "articles--existential-risks:117": "Многие люди считают, что мир становится хуже,[^:`Результаты разных опросов могут значительно отличаться в оценках того, насколько люди пессимистичны в отношении будущего, но многие из них находят, что большинство настроено пессимистично. Например, недавний государственный опрос в Великобритании показал, что 71% респондентов считает мир ухудшающимся.",
    "articles--existential-risks:119": "_Declinism: is the world actually getting worse?_, Pete Etchells, The Guardian, 2015, [Архивная ссылка](https://web.archive.org/web/20171017102431/https://www.theguardian.com/science/head-quarters/2015/jan/16/declinism-is-the-world-actually-getting-worse), доступ проверен 17.10.2017`] и в этом есть своя правда — в современной цивилизации существуют ужасные вещи, вроде промышленного животноводства. Но, как видно из приведённых данных, многие важные показатели прогресса значительно улучшились.",
    "articles--existential-risks:121": "Так или иначе, что бы вы не думали про прошлое, если смотреть в будущее, то улучшение технологий, политической организации и свободы общества сможет дать нашим потомкам возможность решить наши текущие проблемы, а также жить намного лучше.[^:`Становится ли мир лучше?",
    "articles--existential-risks:123": "Хотя есть причины считать, что большинство показателей прогресса увеличивается (как показано в статье), есть некоторые аспекты, в которых жизнь могла стать хуже. Например, в книге [_Sapiens_](https://www.amazon.com/Sapiens-Humankind-Yuval-Noah-Harari/dp/0062316095) Юваль Харари утверждает, что в современную эпоху усилились проблемы одиночества и психического здоровья, в то время как ощущения значимости и смысла могли снизиться. Мы скептически относимся к тому, что эти минусы перевешивают плюсы, но сложно сказать наверняка, какова ситуация на самом деле.",
    "articles--existential-risks:125": "Более весомые аргументы в пользу того, что мир становится хуже, возникают в контексте нашего воздействия на животных. В частности, с 1960-х годов резко выросло промышленное животноводство, и сейчас [где-то более 30 миллиардов животных](https://80000hours.org/problem-profiles/factory-farming/) ежегодно живут в ужасных условиях на фабриках. Если нас волнуют страдания этих животных, то это может перевесить наши успехи в сфере человеческого благополучия.",
    "articles--existential-risks:127": "Учитывая все эти аргументы, мы не можем однозначно утверждать, что суммарное благополучие выросло. Однако более важным является вопрос того, что нас ждет в будущем.",
    "articles--existential-risks:129": "Станет ли мир лучше?",
    "articles--existential-risks:131": "Мы считаем, что до тех пор, пока человечество существует, развитие технологий и моральный прогресс дают нам возможности справиться с самыми серьёзными социальными проблемами, а также жить гораздо лучше в будущем. Если отодвинуть в сторону экзистенциальные угрозы, то многие конкретные глобальные проблемы могут быть решены, если уровни богатства, технологического развития, а также морального и политического прогресса будут и дальше повышаться.",
    "articles--existential-risks:133": "Например, в случае с промышленным животноводством мы ожидаем, что по мере того, как люди будут становиться богаче, проблема будет уменьшаться. Во-первых, богатые люди более склонны к \"этичному\" потреблению, потому что могут себе это позволить. Во-вторых, технологии способны положить конец промышленному животноводству при помощи заменителей мяса, искуственно выращенного мяса или более гуманных методов ведения сельского хозяйства. В-третьих, как мы видим, забота о других живых существах увеличилась с течением времени (\"расширяющийся круг заботы\"), поэтому мы ожидаем, что в будущем люди будут ещё больше заботиться о благополучии животных.",
    "articles--existential-risks:135": "Если посмотреть ещё шире, то в целом мы ожидаем, что будущее будет лучше, потому что люди сами этого хотят. Чем больше технологической мощи и личных свобод мы имеем, тем проще людям реализовывать свои ценности. Поскольку люди _хотят_ жить хорошо, улучшение будущего — более вероятный сценарий, чем его ухудшение.",
    "articles--existential-risks:137": "При этом остаётся много вопросов. Например, многие наши ценности в какой-то степени противоречат другим, и это может привести к конфликтам. Вопросы о том, что ждет нас в будущем, также мало изучены. Поэтому хотя мы и ожидаем, что в будущем будет лучше, мы также признаём, что в нашем суждении присутствует значительная степень неопределённости.`] Можно будет покончить с бедностью, предотвратить изменение климата, облегчить многие страдания — список лишь продолжается.",
    "articles--existential-risks:139": "Но также обратите внимание на фиолетовую линию на втором графике: _военный потенциал_. Данные о военном потенциале были взяты из оценки глобальной военной мощи историка Иэна Морриса. Как видите, эта линия тоже резко стремится вверх.",
    "articles--existential-risks:141": "Проблема заключается в следующем: развитие технологий может таить в себе как огромные выгоды, так и огромные риски.",
    "articles--existential-risks:143": "Каждый раз, когда мы получаем доступ к новым технологиям, в большинстве случаев они приносят огромную пользу. Но есть так же шанс, что эти технологии будут обладать настолько высоким разрушительным потенциалом, что мы не сможем обеспечить себе необходимую безопасность для их использования.",
    "articles--existential-risks:145": "И поэтому, хотя нынешнее поколение живет в самый благополучный период в истории человечества, он, возможно, также является самым опасным.",
    "articles--existential-risks:147": "Первой разрушительной технологией такого рода стало ядерное оружие.",
    "articles--existential-risks:149": "## История ядерного оружия: череда крайне опасных ситуаций",
    "articles--existential-risks:151": "Сегодня мы все думаем о ядерной программе Северной Кореи, но нынешние события — лишь одна глава в длинной истории крайне опасных ситуаций.",
    "articles--existential-risks:153": "Только во время Кубинского ракетного кризиса мы несколько раз были близки к ядерной войне.[^:` Больше об истории подобных ситуаций можно узнать в [нашем подкасте с Тоби Ордом.](https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/)`] В одном из случаев американцы решили, что если один из их самолетов-шпионов будет сбит, то они немедленно начнут вторжение на Кубу без дополнительного заседания Военного совета. На следующий день самолет-шпион был сбит. Кеннеди всё равно созвал совет и принял решение не вводить войска.",
    "articles--existential-risks:155": "Вторжение на Кубу вполне могло привести к ядерной войне; позже выяснилось, что Кастро выступал за ядерное возмездие, даже если \"это привело бы к полному уничтожению Кубы\". Некоторые из командиров пусковых установок на Кубе также имели независимые полномочия наносить удары по американским силам тактическим ядерным оружием в случае вторжения.",
    "articles--existential-risks:157": "В другом инциденте российская атомная подводная лодка пыталась контрабандой переправить материалы на Кубу, когда ее обнаружил американский флот. Флот начал сбрасывать фиктивные глубинные бомбы, чтобы заставить подводную лодку всплыть. Русский капитан подумал, что это настоящие глубинные бомбы и что, пока не было радиосвязи, началась третья мировая война. Он приказал нанести ядерный удар по американскому флоту одной из ядерных торпед.",
    "articles--existential-risks:159": "К счастью, для такого приказа было необходимо разрешение от других старших офицеров. Один их них, Василий Архипов, был против, и таким образом предотвратил войну.",
    "articles--existential-risks:161": "![Благодаря Василию Архипову мы чудом предотвратили глобальный риск катастрофы от ядерного оружия](https://80000hours.org/wp-content/uploads/2017/10/VA.jpg)[`Спасибо вам, Василий Архипов.`]",
    "articles--existential-risks:163": "Сопоставив все эти события вместе, Кеннеди позже говорил, что вероятность ядерной войны была \"между 25 и 50 процентами\".[^:`> Пятьдесят лет назад Кубинский ракетный кризис поставил мир на грань ядерной катастрофы. Во время конфликта президент Джон Ф. Кеннеди считал, что вероятность эскалации войны была \"между 25 и 50 процентами\", и то, что мы узнали в последующие десятилетия, не даёт причин сомневаться в этой оценке. Такой конфликт мог привести к гибели 100 миллионов американцев и более 100 миллионов русских.",
    "articles--existential-risks:165": "_At 50, the Cuban Missile Crisis as Guide_, Graham Allison, The New York Times, 2012,",
    "articles--existential-risks:167": "[Архивная ссылка](https://web.archive.org/web/20171017104052/http://www.nytimes.com/2012/06/16/opinion/at-50-the-cuban-missile-crisis-as-guide.html), доступ проверен 17.10.2017`]",
    "articles--existential-risks:169": "Подобных опасных столкновений с Россией было немало, даже после холодной войны: на Википедии [есть список](https://en.wikipedia.org/wiki/List_of_nuclear_close_calls). И это лишь те, о которых нам известно.",
    "articles--existential-risks:171": "Сегодня эксперты по ядерным вопросам обеспокоены напряжённостью между Индией и Пакистаном (обе страны обладают ядерным оружием) не меньше, чем Северной Кореей.[^:`Оценки шансов ядерного удара по гражданской цели указаны по ссылке ниже, на рисунке в разделе \"What is the probability that a nuclear bomb will be dropped on a civilian target in the next decade?” Обратите внимание, что один эксперт оценил шанс ядерного удара по гражданской цели в ближайшие 10 лет менее чем в 1%.",
    "articles--existential-risks:173": "Правда ли, что экспертов больше беспокоят индийско-пакистанские отношения, чем Северная Корея?",
    "articles--existential-risks:175": "> Первое место в списке конфликтов, вызывающих опасения экспертов, занимает Индия-Пакистан. Оба государства разработали ядерное оружие вне юрисдикции Договора о нераспространении ядерного оружия, оба государства имеют ограниченные военные возможности, что может стать причиной раннего применения, и оба государства, как известно, имеют планы на случай непредвиденных обстоятельств, которые предполагают первые ядерные удары по военным целям (хоть их публичные заявления и намеренно расплывчаты).",
    "articles--existential-risks:177": "_We’re Edging Closer To Nuclear War_, Milo Beckman, FiveThirtyEight, 2017,",
    "articles--existential-risks:179": "[Архивная ссылка](https://web.archive.org/web/20171017104351/https://fivethirtyeight.com/features/were-edging-closer-to-nuclear-war/), доступ проверен 17.10.2017`]",
    "articles--existential-risks:181": "Главная проблема заключается в том, что несколько стран располагают крупными ядерными арсеналами и могут применить их в считанные минуты. Это означает, что ложная тревога или иная ошибка может быстро перерасти в полномасштабную ядерную войну, особенно в период напряженных международных отношений.",
    "articles--existential-risks:183": "Сможет ли ядерная война привести к гибели цивилизации? Первоначально считалось, что ядерный взрыв может быть настолько горячим, что воспламенит атмосферу и сделает Землю непригодной для жизни. Но учёные решили, что такой исход достаточно маловероятен. Это позволило нам провести \"безопасные\" испытания ядерного оружия, и теперь мы знаем, что такого не случится.",
    "articles--existential-risks:185": "В 1980-х годах люди опасались, что пепел от горящих зданий погрузит Землю в очень долгую зиму, которая сделает невозможным выращивание сельскохозяйственных культур в течение десятилетий.[^:`Когда \"ядерная зима\" стала предметом беспокойства?",
    "articles--existential-risks:187": "> \"Ядерная зима\", и предшествующая ей концепция, \"ядерные сумерки\", относятся к ядерным событиям. Ядерная зима стала интересна науке в 1980-х годах, после того как стало ясно, что более ранняя гипотеза про разрушение озонового слоя из-за выбросов NOx от огненных шаров начинает терять свою достоверность. Именно в этом контексте климатическое воздействие сажи от пожаров было \"случайно обнаружено\" и вскоре стало основной теорией в области климатических последствий ядерной войны. В этих модельных сценариях предполагалось, что различные облака, содержащие сажу в каком-то количестве, образуются над городами, нефтеперерабатывающими заводами и сельскими ракетными шахтами. После того, как исследователи задают количество сажи, моделируются климатические эффекты этих облаков. Термин «ядерная зима» был придуман в 1983 году Ричардом П. Турко в связи с одномерной (1-D) компьютерной моделью, созданной для изучения идеи «ядерных сумерек». На основе этой 1-D модели был сделан вывод, что огромное количество сажи и дыма будет годами держаться в воздухе и станет причиной сильного падения температуры на всей планете. Турко позже дистанцировался от этих радикальных 1-D выводов.\"",
    "articles--existential-risks:189": "Статья \"Nuclear Winter\" на Википедии, [архивная ссылка](https://web.archive.org/web/20171030214107/https://en.wikipedia.org/wiki/Nuclear_winter), доступ проверен 30.10.2017.`] Современные климатические модели предполагают, что сильная ядерная зима, в результате которой все умрут, очень маловероятна (хотя нельзя сказать наверняка, из-за [неопределённости моделей](https://concepts.effectivealtruism.org/concepts/uncertainty-about-models/).[^:`Модели климата содержат значительную неопределённость, а это значит, что реальные риски запросто могут быть выше. Более того, из-за самого факта наличия неопределённости в моделях сложно присуждать очень низкие вероятности большинству рисков. Об этом можно почитать в следующей работе:",
    "articles--existential-risks:191": "Ord, T., Hillerbrand, R., & Sandberg, A. (2010). Probing the improbable: methodological challenges for risks with low probabilities and high stakes. Journal of Risk Research, 13(2), стр. 191-205. arXiv:0810.5515v1, [ссылка](http://amirrorclear.net/files/probing-the-improbable.pdf).`])",
    "articles--existential-risks:193": "Однако даже \"умеренная\" ядерная зима всё равно может привести к массовому голоду.[^:`Ожидаемая тяжесть ядерной зимы всё ещё обсуждается, и Open Philantropy [недавно выделила средства](https://www.openphilanthropy.org/focus/global-catastrophic-risks/miscellaneous/rutgers-university-nuclear-conflict-climate-modeling) на дальнейшее исследование этой темы.`] По этой и другим причинам ядерная война была бы чрезвычайно дестабилизирующей для мира, и неизвестно, сможет ли цивилизация восстановиться после неё.",
    "articles--existential-risks:195": "Какова вероятность того, что ядерная война навсегда уничтожит цивилизацию? Это очень трудно оценить, но похоже, что вероятность такого события в следующем столетии превышает 0,3%. Если это так, то риски от ядерного оружия будут выше, чем все природные риски вместе взятые. ([Подробнее о ядерных рисках](https://80000hours.org/problem-profiles/nuclear-security/).)",
    "articles--existential-risks:197": "Вот почему 1950-е годы стали началом новой эры для человечества. Впервые в истории у небольшого числа людей, принимающих решения, появилась возможность разрушить весь мир. Теперь самой большой угрозой для собственного выживания являемся мы сами, и это делает сегодняшний день самым опасным в истории человечества.",
    "articles--existential-risks:199": "И ядерное оружие — не единственный способ, которым мы можем положить конец цивилизации.",
    "articles--existential-risks:201": "## Насколько велики риски, связанные с изменением климата?",
    "articles--existential-risks:203": "В 2015 году президент Обама [сказал в своем обращении \"О положении дел в стране\"](https://web.archive.org/web/20171017105713/http://edition.cnn.com/2015/01/21/us/climate-change-us-obama/index.html), что \"ни одна проблема не представляет столь большой угрозы для будущих поколений, как изменение климата\".",
    "articles--existential-risks:205": "Безусловно, изменение климата представляет собой серьёзный риск для цивилизации.",
    "articles--existential-risks:207": "Наиболее вероятный результат — 2-4 градуса потепления.[^:` См. Box SPM.1.1 в разделе B, [Summary for Policymakers](https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_SPM_final.pdf) of the Working Group I Contribution to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change.`] Это плохо, но такие изменения не ставят под угрозу выживание нашего вида.",
    "articles--existential-risks:209": "Однако [некоторые оценки](https://www.huffingtonpost.com/michael-e-mann/the-fat-tail-of-climate-change-risk_b_8116264.html) дают 10% вероятность потепления свыше 6 градусов, и где-то 1% шанс потепления на 9 градусов.",
    "articles--existential-risks:211": "Таким образом, похоже, что вероятность масштабной климатической катастрофы (в результате эмиссий CO2) примерно равна вероятности ядерной войны.",
    "articles--existential-risks:213": "Но, как мы утверждаем в нашем [обзоре проблемы изменения климата](https://80000hours.org/problem-profiles/climate-change/), даже потепление в 13 градусов вряд ли само по себе приведёт к вымиранию человечества. Поэтому исследователи, изучающие эти вопросы, считают риск ядерной войны более серьёзным (из-за вероятности непосредственного вымирания в результате ядерной зимы), чем риск от изменения климата, и тут мы с ними согласны.",
    "articles--existential-risks:215": "Тем не менее, изменение климата является серьёзной проблемой, и его дестабилизирующие последствия могут усугубить другие риски, в том числе риск ядерного конфликта. Это нужно учитывать в нашей оценке рисков данной проблемы.",
    "articles--existential-risks:217": "## Какие новые технологии могут быть столь же опасными, как ядерное оружие?",
    "articles--existential-risks:219": "Изобретение ядерного оружия привело к возникновению антиядерного движения всего десятилетие спустя, а движение защитников окружающей среды вскоре приняло на вооружение борьбу с изменением климата.",
    "articles--existential-risks:221": "Меньше внимания уделяется тому факту, что новые технологии будут создавать новые катастрофические риски. Вот почему нам необходимо движение, которое занимается защитой цивилизации в целом.",
    "articles--existential-risks:223": "Предсказывать будущее технологий сложно, но поскольку у нас всего одна цивилизация, нам нужно стараться изо всех сил. Вот несколько кандидатов на роль следующей технологии, потенциально столь же опасной, как ядерное оружие.",
    "articles--existential-risks:225": "В 1918-1919 годах от испанского гриппа умерло более 3% населения Земли.[^:`> По разным оценкам, от него погибло от 3% до 6% мирового населения.",
    "articles--existential-risks:227": "_World War One’s role in the worst ever flu pandemic_, John Mathews, The Conversation, 2014, [архивная ссылка](https://web.archive.org/web/20171027035359/https://theconversation.com/world-war-ones-role-in-the-worst-ever-flu-pandemic-29849), доступ проверен 27.10.2017.",
    "articles--existential-risks:229": "> При населении мира в 1811 миллионов человек, 30 миллионов смертей привели бы к уровню смертности в 16,6 на тысячу человек. Это в три раза выше, чем в богатых странах, но вполне в пределах нормы для бедных стран. Если оценивать количество смертей от гриппа в 50-100 миллионов, то числа были бы в районе 27,6-55,2 на тысячу.",
    "articles--existential-risks:231": "Patterson, K.D. and Pyle, G.F., 1991. The geography and mortality of the 1918 influenza pandemic. Bulletin of the History of Medicine, 65(1), p.4.",
    "articles--existential-risks:233": "[Архивная ссылка](https://web.archive.org/web/20171022101640/https://pida.nihlibrary.com/sites/pida.nihlibrary.com/files/pdf_files/1991_K.David%20Patterson_The%20geography%20and%20mortality%20of%20the%201918%20influenza%20pandemic..pdf), доступ проверен 22.10.2017.",
    "articles--existential-risks:235": "> Дальнейшие исследования на тему пандемии испанского гриппа приводили к регулярному пересмотру значений предполагаемой глобальной смертности в сторону увеличения: согласно изначальным расчетам 1920-х годов, она составляла примерно 21,5 миллиона человек. В работе 1991 года смертность была пересчитана, с новым значением в диапазоне 24,7-39,3 миллиона человек. В данной работе предполагается, что она была порядка 50 миллионов. Однако следует признать, что даже эта огромная цифра может быть значительно ниже реальной — вплоть до разницы в 100%.",
    "articles--existential-risks:237": "Johnson, N.P. and Mueller, J., 2002. Updating the accounts: global mortality of the 1918-1920\" Spanish\" influenza pandemic. Bulletin of the History of Medicine, 76(1), стр. 105-115.",
    "articles--existential-risks:239": "[Ссылка](https://muse.jhu.edu/article/4826/summary)`] Если бы такая пандемия вспыхнула сегодня, её было бы ещё труднее сдерживать из-за быстрого глобального транспорта.",
    "articles--existential-risks:241": "Однако больше тревоги вызывает то, что вскоре может появиться возможность генетически сконструировать вирус, который будет таким же заразным, как испанский грипп, но при этом более смертоносным, и который сможет годами распространяться незамеченным.",
    "articles--existential-risks:243": "Это было бы оружие с разрушительной мощью ядерного, но чьё применение гораздо труднее предотвратить. Ядерное оружие требует огромных заводов и редких материалов для производства, что делает его относительно легким для контроля. Искусственные вирусы можно потенциально создать в лаборатории с парой докторов биологических наук. Более того, в 2006 году The Guardian смогли заказать по почте сегменты вымершего вируса оспы.[^:`_Revealed: the lax laws that could allow assembly of deadly virus DNA: Urgent calls for regulation after Guardian buys part of smallpox genome through mail order_, The Guardian, 2006,",
    "articles--existential-risks:245": "[Архивная ссылка](https://web.archive.org/web/20171022042133/https://www.theguardian.com/world/2006/jun/14/terrorism.topstories3), доступ проверен 21.10.2017.`] Некоторые террористические группировки уже заявляли о своём интересе к подобному оружию неизбирательного действия. ([Подробнее о рисках пандемий](https://80000hours.org/problem-profiles/biosecurity/).)",
    "articles--existential-risks:247": "![В 2006 году The Guardian смогли заказать по почте сегменты вымершего вируса оспы. Эксперты предполагают, что синтетические патогены потенциально могут представлять глобальный катастрофический риск.](https://80000hours.org/wp-content/uploads/2017/10/smallpox.png)[`Кто заказывал оспу? Источник: The Guardian`]",
    "articles--existential-risks:249": "Ещё одна новая технология с огромной потенциальной мощью — искусственный интеллект.",
    "articles--existential-risks:251": "Причина, по которой люди стали главными на планете, а не шимпанзе, полностью заключается в интеллекте. Размеры и мощь наших мозгов позволили нам установить поразительный контроль над окружающим миром, хоть и физически мы намного слабее шимпанзе.",
    "articles--existential-risks:253": "Что же произойдёт, если однажды мы создадим нечто гораздо более разумное, чем мы сами?",
    "articles--existential-risks:255": "В 2017 году 350 исследователей, публиковавших рецензируемые исследования в области искусственного интеллекта на ведущих конференциях, были опрошены о том, когда, по их мнению, будет разработан компьютер с человеческим уровнем интеллекта: то есть машина, способная выполнять все рабочие задачи лучше, чем человек.",
    "articles--existential-risks:257": "Медианной оценкой была 50% вероятность того, что мы разработаем высокоуровневый машинный интеллект (ВМИ) через 45 лет, и 75% к концу века.[^:`> Исследователи считают, что существует 50% вероятность того, что ИИ превзойдет человека во всех задачах через 45 лет.\n>\n> Респондентов спросили, какое влияние ВМИ окажет на человечество в долгосрочной перспективе: положительное или отрицальное. Они оценивали вероятности различных исходов по пятибальной шкале. Медианными оценками были 25% для \"хорошего исхода\" и 20% для \"чрезвычайно хорошего исхода\". В свою очередь, медианная оценка для \"плохого\" исхода была 10%, и 5% для исхода, сформулированного как \"чрезвычайно плохой (например, вымирание человечества)\".",
    "articles--existential-risks:261": "Grace, K., Salvatier, J., Dafoe, A., Zhang, B. and Evans, O., 2017. When Will AI Exceed Human Performance? Evidence from AI Experts. arXiv preprint arXiv:1705.08807.",
    "articles--existential-risks:263": "[Ссылка](https://arxiv.org/abs/1705.08807)`]",
    "articles--existential-risks:265": "!img~ class=\"alignnone size-full wp-image-40209\" width=\"602\" height=\"389\" ~[График прогнозов экспертов, Grace et al: медианной оценкой был 50% шанс того, что мы разработаем высокоуровневый машинный интеллект в течение 45 лет](https://80000hours.org/wp-content/uploads/2017/10/probability-of-HLMI.png)",
    "articles--existential-risks:267": "Такие вероятности сложно оценивать, и исследователи говорили очень разные цифры, в зависимости от конкретной формулировки вопроса.[^:`Если вам интересно почитать обсуждение этой непоследовательности в оценках, есть пост от AI Impacts, \"Some Survey Results,\" [архивная ссылка](https://web.archive.org/web/20171030220008/https://aiimpacts.org/some-survey-results/), доступ проверен 30.10.2017. К примеру:",
    "articles--existential-risks:269": "> Вопросы, связанные с конкретными видами работы, существенно влияют на прогнозы про ВМИ. Когда мы задавали некоторым людям вопросы про то, когда ИИ начнёт успешно выполнять несколько конкретных видов деятельности, а потом про все виды работы, которую делает человек (что считается подвидом всех возможных задач вообще), мы получали сильно более поздние сроки, чем когда мы просто спрашивали про ВМИ. Если мы при этом просили назвать вероятности для конкретных лет, то для \"через 20 лет\" цифры отличались в 1000 раз! (10% для общего вопроса, против 0,01% для конкретных вопросов) Если же мы просили назвать годы для некоторых вероятностей, общий вопрос получал оценку \"через 40 лет\" для 50% вероятности, а формулировки про конкретные виды работы получали ответ \"через 90 лет\" для тех же 50%.\n>\n> Люди регулярно дают более поздние прогнозы, если спрашивать их про \"вероятность через N лет\", чем если спрашивать про \"год, в который вероятность будет M\". Мы наблюдали это в контексте общего вопроса про ВМИ и конкретных вопросов про виды работы, а также в большинстве случаев, когда мы тестировали эти вопросы на респондентах с MTurk ранее (Amazon Mechanical Turk, сервис аутсорсинга задач, которые ещё не могут выполняться компьютерами). Например, для общего вопроса про ВМИ, если спрашивать, когда вероятность изобретения ВМИ достигнет 50%, медианным ответом будет \"через 40 лет\", но если спрашивать, какова вероятность изобретения ВМИ через 40 лет, медианный ответ будет \"30%\".`] Тем не менее, есть причины полагать, что существует разумный шанс изобретения некого революционного машинного интеллекта в следующем столетии.",
    "articles--existential-risks:273": "Какие риски может представлять это развитие? Первопроходцы в области вычислительной техники, такие как Алан Тьюринг и Марвин Мински, высказывали опасения по поводу рисков, связанных с мощными компьютерными системами,[^:`> Некоторые заявления таких ученых, как Алан Тьюринг, И. Дж. Гуд и Марвин Мински, указывали на опасения (по философским соображениям) насчёт того, что сверхинтеллект может захватить контроль.",
    "articles--existential-risks:275": "См. сноски 15-18 в статье _Existential risk from artificial intelligence_ на Википедии, [архивная ссылка](https://web.archive.org/web/20171022041500/https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence), доступ проверен 21.10.2018.`] и эти риски до сих пор актуальны. Мы говорим не о \"злых\" компьютерах, а скорее о том, что мощная ИИ-система может быть использована какой-то группой людей для установления контроля над миром, или как-то иначе использована в неправильных целях. Если бы СССР разработал ядерное оружие на 10 лет раньше, чем США, то СССР мог бы стать доминирующей мировой державой. Мощные компьютерные технологии могут представлять аналогичные риски.",
    "articles--existential-risks:277": "Другая проблема заключается в том, что запуск такой системы может привести к непредвиденным последствиям, поскольку сложно предсказать действия чего-то более умного, чем мы сами. Достаточно мощной системой также может быть сложно управлять, и поэтому её запуск может оказаться труднообратимым. Эти опасения были описаны оксфордским профессором Ником Бостромом в работе [_Superintelligence_](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742) и пионером ИИ [Стюартом Расселом](https://www.ted.com/talks/stuart_russell_how_ai_might_make_us_better_people).",
    "articles--existential-risks:279": "Большинство экспертов считают, что совершенствование ИИ приведёт к чрезвычайно положительным результатам, но при этом также не забывают о рисках. В вышеупомянутом опросе, эксперты в области ИИ оценили, что разработка высокоуровневого машинного интеллекта с 10% шансом может привести к \"плохому исходу\", и с 5% шансом к \"чрезвычайно плохому исходу\", такому, как вымирание человечества.[^:`> Исследователи считают, что существует 50% вероятность того, что ИИ превзойдет человека во всех задачах через 45 лет.\n>\n> Респондентов спросили, какое влияние ВМИ окажет на человечество в долгосрочной перспективе: положительное или отрицальное. Они оценивали вероятности различных исходов по пятибальной шкале. Медианными оценками были 25% для \"хорошего исхода\" и 20% для \"чрезвычайно хорошего исхода\". В свою очередь, медианная оценка для \"плохого\" исхода была 10%, и 5% для исхода, сформулированного как \"чрезвычайно плохой (например, вымирание человечества)\".",
    "articles--existential-risks:283": "Grace, K., Salvatier, J., Dafoe, A., Zhang, B. and Evans, O., 2017. When Will AI Exceed Human Performance? Evidence from AI Experts. arXiv preprint arXiv:1705.08807.",
    "articles--existential-risks:285": "[Ссылка](https://arxiv.org/abs/1705.08807)`] И нам скорее всего следует ожидать излишней оптимистичности от этой группы людей, поскольку они зарабатывают на жизнь благодаря этой технологии.",
    "articles--existential-risks:287": "Если сложить эти оценки вместе, то при 75% шансе создания высокоуровневого машинного интеллекта в следующем веке, вероятность крупной катастрофы, связанной с ИИ, составляет 5% от 75%, то есть около 4%. ([Подробнее о рисках, связанных с искусственным интеллектом](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/).)",
    "articles--existential-risks:289": "Люди выражали беспокойство по поводу других новых технологий, таких как некоторые формы геоинженерии и атомного производства, но они кажутся значительно менее неизбежными и, следовательно, менее опасными, чем другие технологии, о которых мы говорили. Более длинный список экзистенциальных рисков можно посмотреть [здесь](https://nickbostrom.com/existential/risks.html).",
    "articles--existential-risks:291": "Больше беспокойств вызывают риски, о которых мы могли ещё не подумать. Если бы вы спросили людей в 1900-м году, что представляет наибольшую угрозу для цивилизации, они вряд ли бы упомянули атомное оружие, генную инженерию или искусственный интеллект, потому что ничто из этого ещё не было изобретено. Вполне возможно, что мы находимся в такой же ситуации относительно следующего века. Грядущие \"неизвестные неизвестные\" могут представлять большую опасность, чем уже известные нам риски.",
    "articles--existential-risks:293": "Каждый раз, когда мы открываем новую технологию, это немного похоже на ставку _против_ одного числа в рулетке. В большинстве случаев мы выигрываем, и технология в целом имеет хорошие последствия. Но каждый раз есть небольшой шанс, что технология будет обладать настолько большим разрушительным потенциалом, что мы не сможем с ней совладать, и тогда мы потеряем всё.",
    "articles--existential-risks:295": "![](https://80000hours.org/wp-content/uploads/2017/10/roulette.jpg)[`У каждой новой технологии беспрецедентными являются как и потенциал, так и опасности. Image source.`]",
    "articles--existential-risks:297": "### Каким будет суммарный риск вымирания человечества, если учесть все эти факторы?",
    "articles--existential-risks:299": "По оценкам многих экспертов, изучающих эти вопросы, суммарная вероятность вымирания человечества в следующем столетии составляет от 1 до 20%.",
    "articles--existential-risks:301": "Например, согласно неофициальному опросу, проведенному в 2008 году на конференции по катастрофическим рискам, эксперты считают вполне вероятной катастрофу, в результате которой погибнет более миллиарда человек, и оценивают вероятность вымирания до 2100 года в 19%.[^:`Sandberg, A. & Bostrom, N. (2008): “Global Catastrophic Risks Survey”, Technical",
    "articles--existential-risks:303": "Report #2008-1, Future of Humanity Institute, Oxford University: стр. 1-5.",
    "articles--existential-risks:305": "[Ссылка](https://www.fhi.ox.ac.uk/reports/2008-1.pdf)`]",
    "articles--existential-risks:307": "<div class=\"tablepress-scroll-wrapper\">",
    "articles--existential-risks:309": "| Риск | Минимум миллиард смертей | Вымирание человечества |\n| --- | --- | --- |\n| Количество смертей из-за молекулярного нанооружия. | 10% | 5% |\n| Суммарное количество смертей из-за сильного ИИ. | 5% | 5% |\n| Суммарное количество смертей из-за всех войн (включая гражданские). | 30% | 4% |\n| Количество смертей из-за самой большой искусственной пандемии. | 10% | 2% |\n| Суммарное количество смертей из-за всех атомных конфликтов. | 10% | 1% |\n| Количество смертей из-за самой большой нанотехнологической аварии. | 1% | 0.5% |\n| Количество смертей из-за самой большой естественной пандемии. | 5% | 0.05% |\n| Суммарное количество смертей из-за всех актов терроризма с применением атомного оружия. | 1% | 0.03% |\n| Общий риск вымирания до 2100 года | n/a | 19% |",
    "articles--existential-risks:321": "</div>",
    "articles--existential-risks:323": "Эти цифры примерно в миллион раз превышают ожидания большинства людей.",
    "articles--existential-risks:325": "В [эпизоде нашего подкаста с Уиллом МакАскиллом](https://80000hours.org/podcast/episodes/will-macaskill-paralysis-and-hinge-of-history/) мы обсуждаем то, почему он оценивает риск вымирания в этом веке примерно в 1%.",
    "articles--existential-risks:327": "В своей книге [_Пропасть: Экзистенциальный риск и будущее человечества_](https://80000hours.org/the-precipice/) Тоби Орд предполагает, что общий экзистенциальный риск в этом веке составляет 1/6, что равноценно броску игральной кости. [Послушать наш подкаст с Тоби можно тут](https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/).",
    "articles--existential-risks:329": "Какие выводы стоит делать из этих оценок? Предположительно, исследователи работают над этими вопросами лишь потому, что считают их очень важными, поэтому мы должны ожидать, что их оценки будут высокими (см. [\"систематическая ошибка отбора\"](https://en.wikipedia.org/wiki/Selection_bias)). Но значит ли это, что мы можем полностью игнорировать их опасения?",
    "articles--existential-risks:331": "С учётом всего этого, какова наша личная оценка? Это очень сложный вопрос, но мы не можем с уверенностью игнорировать эти риски. В целом, мы полагаем, что общий риск скорее всего превышает 3%.",
    "articles--existential-risks:333": "## Почему участие в защите будущего может быть самым важным делом вашей жизни",
    "articles--existential-risks:335": "Насколько приоритетной должна быть работа по снижению этих рисков по сравнению с другими вопросами, такими как глобальная бедность, борьба с раком или улучшение политической системы?",
    "articles--existential-risks:337": "В рамках проекта \"80 000 часов\" мы проводим исследования, чтобы помогать людям находить работу, приносящую пользу обществу. Для этого мы стараемся найти наиболее актуальные проблемы в мире, над которыми необходимо работать. Мы оцениваем различные глобальные проблемы, используя нашу [модель](/articles/problem-framework/), которая сравнивает их по следующим критериям:",
    "articles--existential-risks:339": "- Масштаб — сколько людей затронуто этой проблемой\n- Недооценённость — сколько людей уже работают над ней\n- Разрешимость — насколько легко продвинуться в её решении",
    "articles--existential-risks:343": "Из этой модели следует, что защита будущего является самым главным приоритетом общества. Поэтому если вы хотите принести много пользы миру при помощи своей деятельности, в первую очередь стоит сфокусироваться на этой области.",
    "articles--existential-risks:345": "В следующих нескольких разделах мы рассмотрим эту проблему с точки зрения масштаба, недооценённости и разрешимости, в значительной степени опираясь на _Existential Risk Prevention as a Global Priority_ Ника Бострома и [неопубликованную работу Тоби Орда](https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/), а также на наши исследования.",
    "articles--existential-risks:347": "Во-первых, давайте начнем с масштаба проблемы. Мы утверждали, что вероятность вымирания в следующем столетии составляет более 3%. Насколько это серьёзно?",
    "articles--existential-risks:349": "Одна из цифр, на которую мы можем взглянуть, — это количество людей, которое может погибнуть в результате такой катастрофы. Население планеты в середине этого века будет составлять примерно 10 миллиардов человек. Значит, 3% шанс того, что все умрут, даёт математическое ожидание в 300 миллионов смертей. Это скорее всего больше смертей, чем можно суммарно ожидать за весь следующий век от болезней, связанных с бедностью, вроде малярии.[^:`Каждый месяц от легко предотвратимых болезней, таких, как малярия и диарея, умирают миллионы людей. Но поскольку эти числа стремительно падают, мы не ожидаем, что они привысят 300 миллионов в следующем веке.",
    "articles--existential-risks:351": "> Ежегодная смертность от малярии сократилась с 3,8 миллиона до 0,7 миллиона человек\n>\n> Ежегодная смертность от диареи сократилась с 4,6 миллиона до 1,6 миллиона человек",
    "articles--existential-risks:355": "_Aid Works (On Average)_, Dr Toby Ord, Giving What We Can, [Ссылка](http://studylib.net/doc/13259236/aid-works--on-average--toby-ord-president--giving-what-we)`]",
    "articles--existential-risks:357": "Многие из рассмотренных нами рисков также могут привести к катастрофе \"средней\" тяжести (вместо гибели цивилизации), что может быть намного более вероятным. Опрос, о котором мы рассказывали ранее, показал 10%+ шанс катастрофы, из-за которой в следующем веке может погибнуть более 1 миллиарда людей. Это даёт нам математическое ожидание как минимум ещё в 100 миллионов смертей, и ещё больше страданий среди тех, кто выживет.",
    "articles--existential-risks:359": "Таким образом, даже если мы сосредоточимся только на последствиях для ныне живущего поколения, эти катастрофические риски являются одной из самых серьёзных проблем, стоящих перед человечеством.",
    "articles--existential-risks:361": "Но в таком случае масштабы проблемы будут сильно недооценены, потому что если цивилизации придёт конец, то мы полностью отказываемся и от нашего будущего тоже.",
    "articles--existential-risks:363": "Большинство людей хотят оставить лучший мир для своих внуков, и большинство из нас также считает, что нам нужно в какой-то мере заботиться и о [будущих поколениях](/future-generations/) в целом. В будущем может быть намного больше людей, довольных своей жизнью, чем сейчас, и [нам стоит сколько-то учитывать их интересы](https://80000hours.org/articles/future-generations/). Есть шанс того, что человеческая цивилизация просуществует миллионы лет, поэтому если учитывать последствия рисков для всех будущих поколений, ставки увеличиваются в миллионы раз — в равной степени и для хороших исходов, и для плохих. Как [писал Карл Саган](http://www.jstor.org/stable/20041818) о цене ядерной войны в журнале _Foreign Affairs_:",
    "articles--existential-risks:365": "> Ядерная война ставит под угрозу всех наших потомков, которые будут существовать за всю историю человечества. Даже если наше суммарное население не будет расти, средняя продолжительность жизни будет порядка 100 лет, в условиях типичного периода времени, необходимого для цикла биологической эволюции успешного вида (примерно 10 миллионов лет) — даже в этом случае речь идёт о 500 триллионах человек, которых ещё не было. С учётом этого, ставки для полного вымирания в миллион раз больше, чем для более скромных ядерных войн, которые убивают \"всего лишь\" сотни миллионов людей. Есть много других возможных способов измерять потенциальные потери, которые включают культуру и науку, эволюционную историю планеты и значимость жизней всех наших предков, сделавших вклад в будущее своих потомков. Вымирание сотрёт всё, что построило человечество.",
    "articles--existential-risks:367": "Мы рады, что римляне не позволили человечеству исчезнуть, ведь это позволило существовать всей современной цивилизации. Мы считаем, что на нас лежит такая же ответственность за людей, которые будут после нас, в случае если их жизни будут скорее хорошими (а мы предполагаем, что будут). Подвергать их жизни опасности лишь для того, чтобы сделать себе получше в краткосрочной перспективе, было бы необдуманно и несправедливо.",
    "articles--existential-risks:369": "Дело не только в том, что в будущем может быть больше людей. Как отмечал ещё Саган, вне зависимости от того, что вы цените, [в будущем этого может быть потенциально гораздо больше](https://80000hours.org/articles/future-generations). Цивилизация будущего может построить мир без нужды или добиться потрясающих интеллектуальных и художественных успехов. Мы можем построить намного более справедливое и нравственное общество. И нет принципиальных причин, по которым цивилизация не смогла бы достичь других планет, которых в нашей галактике около 100 миллиардов.[^:`_The Milky Way Contains at Least 100 Billion Planets According to Survey_, Hubblesite",
    "articles--existential-risks:371": "[Архивная ссылка]((https://web.archive.org/web/20140723213047/http://hubblesite.org/newscenter/archive/releases/2012/07/full/), доступ проверен 22.10.2017`] Если мы позволим цивилизации погибнуть, то все эти возможности будут навсегда перечёркнуты.",
    "articles--existential-risks:373": "Мы не уверены, что это замечательное будущее действительно наступит, но это лишь добавляет причин защищать цивилизацию — ведь тогда мы сможем узнать, как всё обернётся. Непередача эстафеты следующему поколению может быть самым худшим из всего, что мы можем сделать.",
    "articles--existential-risks:375": "Таким образом, несколько процентов риска уничтожения цивилизации скорее всего являются самой большой проблемой, стоящей перед миром сегодня. Поражает также то, насколько этими рисками пренебрегают.",
    "articles--existential-risks:377": "## Почему эти риски являются одними из самых недооценённых глобальных проблем",
    "articles--existential-risks:379": "Вот сколько денег ежегодно тратится на некоторые из важных целей:[^:`> Глобальные валовые расходы на исследования и разработки в 2013 году составили 1,48 триллиона долларов по ППС (паритету покупательной способности).",
    "articles--existential-risks:381": "_Facts and figures: R&D expenditure_, UNESCO,",
    "articles--existential-risks:383": "[Архивная ссылка](https://web.archive.org/web/20171020233546/https://en.unesco.org/node/252279), доступ проверен 21.10.2017",
    "articles--existential-risks:385": "> В целом индустрия демонстрирует устойчивый рост на 4%, и в 2016 году объем розничных продаж оценивается в 1,08 триллиона евро.",
    "articles--existential-risks:387": "_Luxury Goods Worldwide Market Study, Fall-Winter 2016_, Claudia D’Arpizio, Federica Levato, Daniele Zito, Marc-André Kamel and Joëlle de Montgolfier, Bain & Company,",
    "articles--existential-risks:389": "[Архивная ссылка](https://web.archive.org/web/20171020234247/http://www.bain.com/publications/articles/luxury-goods-worldwide-market-study-fall-winter-2016.aspx), доступ проверен 21.10.2017",
    "articles--existential-risks:391": "> По данным Heritage Foundation, наилучшая оценка стоимости 185 федеральных программ социального обеспечения (включающих проверку на нуждаемость) на 2010 год составляет почти 700 миллиардов долларов, что на треть больше, чем в 2008 году — и это только расходы федерального правительства. С учетом расходов штатов, общие расходы на социальное обеспечение в 2010 году достигли почти $900 млрд, увеличившись почти на четверть с 2008 года (24,3%).",
    "articles--existential-risks:393": "_America's Ever Expanding Welfare Empire_, Peter Ferrara, Forbes,",
    "articles--existential-risks:395": "[Архивная ссылка](https://web.archive.org/web/20160126160330/http://www.forbes.com/sites/peterferrara/2011/04/22/americas-ever-expanding-welfare-empire/#48b5521855a6), доступ проверен 02.03.2016",
    "articles--existential-risks:397": "> После стабилизации в 2012 году и снижения в 2013 году, в 2014-ом глобальный объем климатического финансирования увеличился на 18%, с $331 млрд до $391 млрд, согласно текущим оценкам.",
    "articles--existential-risks:399": "_Global Landscape of Climate Finance 2015_, Climate Policy Initiative, 2015,",
    "articles--existential-risks:401": "[Архивная ссылка](https://web.archive.org/web/20171027031731/http://climatepolicyinitiative.org/wp-content/uploads/2015/11/Global-Landscape-of-Climate-Finance-2015.pdf), доступ проверен 27.10.2017. В нашем [обзоре проблемы изменения климата](https://80000hours.org/problem-profiles/climate-change/) можно найти ещё цифры.",
    "articles--existential-risks:403": "Для глобальной бедности, даже лишь в рамках глобального здравоохранения:",
    "articles--existential-risks:405": "> Наименее развитые страны (плюс Индия) ежегодно тратят на здравоохранение около 300 миллиардов долларов (ППС).",
    "articles--existential-risks:407": "_Здоровье в бедных странах_, наш [обзор проблемы](https://80000hours.org/problem-profiles/health-in-poor-countries/)",
    "articles--existential-risks:409": "Если более широко рассматривать ресурсы, поступающие к бедным слоям населения мира, то нам стоит учитывать все расходы на помощь, денежные переводы, а также их собственные доходы. В мире насчитывается более 700 миллионов человек, живущих за чертой бедности (т.е. с доходом ниже $1,75 в день). Если допустить, что их средний доход равен $1 в день, то это составит $256 млрд. суммарного дохода в год.",
    "articles--existential-risks:411": "> Ресурсы, глобально выделяемые на предотвращение риска ядерной войны, с учётом всех государственных и негосударственных источников, примерно составляют $10 млрд в год или выше. Однако мы снижаем эту цифру до $1-10 млрд в год с поправкой на качество, потому что большая часть этих расходов направлена не на снижение риска применения ядерного оружия в целом, а на защиту одной страны или предоставление одной стране преимущества над другой. Много средств также тратится на меры, не связанные с наиболее опасными сценариями, в которых используются сотни боеголовок.",
    "articles--existential-risks:413": "_Ядерная безопасность_, наш [обзор проблемы](https://80000hours.org/problem-profiles/nuclear-security/)",
    "articles--existential-risks:415": "Что касается профилактики пандемий, то здесь сложно делать точные оценки, поскольку многие расходы имеют к этому косвенное отношение (например, больницы сами по себе снижают риски пандемий). Если оценивать достаточно широко, то $10+ млрд в год будет вполне разумной цифрой. Если оценивать более узко, фокусируясь на более прицельных мерах, то выйдет где-то $1 млрд в год. Если же говорить именно о целенаправленных усилиях по снижению экзистенциальных рисков от искусственных пандемий, то существует лишь несколько экспертов, специализирующихся на этой теме. Подробнее в нашей статье: _Биобезопасность_, [обзор проблемы](https://80000hours.org/problem-profiles/biosecurity/)",
    "articles--existential-risks:417": "> Глобальные расходы на исследования и другую деятельность, связанную с безопасностью разработки машинного интеллекта, составят всего 9 миллионов долларов в 2017 году.",
    "articles--existential-risks:419": "_Позитивное влияние на разработку искусственного интеллекта_, наш [обзор проблемы](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/).`]",
    "articles--existential-risks:421": "<div class=\"tablepress-scroll-wrapper\">",
    "articles--existential-risks:423": "| Цель | Ежегодные целевые расходы из всех источников (очень приблизительно) |\n| --- | --- |\n| Глобальные исследования и разработки | $1.5 триллиона |\n| Предметы роскоши | $1.3 триллиона |\n| Социальное обеспечение в США | $900 миллиардов |\n| Изменение климата | >$300 миллиардов |\n| Глобальная бедность | >$250 миллиардов |\n| Ядерная безопасность | $1-10 миллиардов |\n| Предотвращение сильнейших пандемий | $1 миллиард |\n| Исследования в области безопасности ИИ | $10 миллионов |",
    "articles--existential-risks:434": "</div>",
    "articles--existential-risks:436": "Как видите, мы тратим огромное количество ресурсов на разработку ещё более мощных технологий. Мы также тратим много средств на предметы роскоши, пытаясь таким образом улучшить свои жизни (что, возможно, не так уж и хорошо работает).",
    "articles--existential-risks:438": "Гораздо меньше средств уходит на снижение катастрофических рисков, связанных с изменением климата. Только лишь в США расходы на соц. обеспечение уже затмевают расходы на борьбу с изменением климата _во всём мире_.",
    "articles--existential-risks:440": "Но на проблему изменения климата всё равно выделяются огромные суммы денег по сравнению с некоторыми другими рисками, о которых мы говорили. По нашим приблизительным оценкам, на предотвращение экстремальных глобальных пандемий тратится в 300 раз меньше средств, хотя размер риска является примерно таким же.",
    "articles--existential-risks:442": "Исследованиям в области безопасности ИИ уделяется меньше всего внимания, и на них опять же выделяется ещё в 100 раз меньше средств: [около 10 млн долларов в год](https://www.centreforeffectivealtruism.org/blog/changes-in-funding-in-the-ai-safety-field/).",
    "articles--existential-risks:444": "Аналогичную картину можно увидеть, если посмотреть на _количество людей_, работающих над этими рисками, а не на потраченные деньги, но для денег проще получить цифры.",
    "articles--existential-risks:446": "Если мы посмотрим на внимание научного сообщества к различным вопросам, то опять увидим похожую картину (хотя некоторые отдельные риски получают значительное внимание — например, изменение климата):",
    "articles--existential-risks:448": "![Исследования экзистенциальных рисков получают меньше финансирования, чем исследования навозных жуков.](https://80000hours.org/wp-content/uploads/2017/10/Publications-by-topicArtboard-2-1.jpg)[`Источник: Ник Бостром`]",
    "articles--existential-risks:450": "Нам кажется, что если посмотреть на распределение внимания в политике, то всё будет примерно так же, как и с финансированием. Подавляющее количество внимания уделяется краткосрочным вопросам, связанным с ныне живущим поколением, потому что именно это приносит голоса. Катастрофические риски получают намного меньше внимания. Далее, среди катастрофических рисков больше всего говорят об изменении климата, а о проблемах вроде пандемий и искусственного интеллекта — меньше всего.",
    "articles--existential-risks:452": "Такое пренебрежение в отношении ресурсов, научного и политического внимания — именно то, чего следует ожидать из текущих экономических предпосылок, а также то, почему эта область является отличной возможностью для всех, кто хочет сделать мир лучше.",
    "articles--existential-risks:454": "Во-первых, ответственность за эти риски не лежит на каком-либо одном государстве. Предположим, США инвестируют значительные средства в предотвращение изменения климата. Это выгодно всем в мире, но в США проживает только около 5% населения планеты, поэтому граждане США получат только 5% выгоды от этих расходов. Это значит, что США будет значительно занижать свои инвестиции в эту область, по сравнению с её значимостью для всего мира. То же самое можно сказать о всех других странах.",
    "articles--existential-risks:456": "Эту проблему можно решить, если мы все _скоординируемся_: если каждая страна согласится внести свою справедливую долю в уменьшение изменения климата, то все страны выиграют и смогут избежать худших возможных последствий.",
    "articles--existential-risks:458": "К сожалению, с точки зрения каждой отдельной страны, будет лучше, если _все другие_ страны сократят свои выбросы, а её собственная экономика останется нетронутой. Поэтому у каждой страны есть стимул не соблюдать климатические соглашения, и именно поэтому достигается так мало прогресса (это называется [\"дилемма заключённого\"](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma)).",
    "articles--existential-risks:460": "Более того, в таком виде проблема всё равно будет сильно преуменьшена. Наибольшую пользу от усилий по снижению катастрофических рисков получают _будущие_ поколения. У них нет ни экономической, ни политической возможности отстаивать свои интересы.",
    "articles--existential-risks:462": "Если бы [будущие поколения](/future-generations/) могли голосовать в наших выборах, они бы отдали подавляющее большинство голосов за принятие законов, повышающих безопасность. Точно также, если бы будущие поколения могли присылать деньги в прошлое, они были бы готовы заплатить нам огромные суммы денег за снижение этих рисков. (Технически говоря, снижение этих рисков является глобальным общественным благом на уровне _поколений_, что вполне логично делает его одним из самых недооценённых способов приносить пользу миру.)",
    "articles--existential-risks:464": "Наша нынешняя система плохо справляется с защитой будущих поколений. Мы знаем людей, которые разговаривали с высокопоставленными чиновниками в правительстве Великобритании, и многие из них хотят что-то сделать с этими рисками, но, по их словам, давление новостей и избирательного цикла мешает уделять должное внимание этим вопросам. В большинстве стран нет государственного учереждения, в прямые обязанности которого входит снижение этих рисков.",
    "articles--existential-risks:466": "Это удручающая ситуация, но это также и возможность. Для людей, которые _действительно_ хотят сделать мир лучше, такой недостаток внимания означает, что есть много способов принести огромную пользу обществу.",
    "articles--existential-risks:468": "## Что можно сделать с этими рисками?",
    "articles--existential-risks:470": "Мы рассмотрели масштабы и недооценённость этих вопросов, но что насчёт третьего критерия нашей модели — разрешимости?",
    "articles--existential-risks:472": "Добиться прогресса по этим проблемам кажется более сложным, чем в более привычных областях вроде глобального здравоохранения. Измерять влияние действий на здоровье (по крайней мере в краткосрочной перспективе) намного проще, и за десятки лет было получено много данных о том, что лучше всего работает. Поэтому работа по снижению катастрофических рисков выглядит хуже в плане разрешимости.",
    "articles--existential-risks:474": "Однако мы всё равно можем многое сделать, и с учётом гигантских масштабов и недооценённости этих рисков, они по-прежнему выглядят самыми важными из проблем.",
    "articles--existential-risks:476": "Мы наметим некоторые способы снижения этих рисков, разделив их на три широкие категории:",
    "articles--existential-risks:478": "### 1\\. Прицельные усилия для снижения отдельных рисков",
    "articles--existential-risks:480": "Один из подходов заключается в непосредственной работе над каким-то из рисков. Существует множество конкретных предложений по борьбе с каждым из них, например:",
    "articles--existential-risks:482": "1. Многие эксперты считают, что повышение эффективности эпидемиологического надзора снизит риски пандемий. Сюда могут входить технические улучшения или более хорошие способы сбора и обработки уже существующих данных, чтобы мы могли быстрее замечать вспышки новых пандемий. А чем быстрее будет замечена новая пандемия, тем проще будет с ней справиться.",
    "articles--existential-risks:484": "2. Существует множество способов повлиять на изменение климата: например, помощь с разработкой более эффективных солнечных батарей или введение углеродного налога.",
    "articles--existential-risks:486": "3. Если говорить про ИИ, то можно заниматься исследованиями \"проблемы контроля\" в информатике, чтобы снизить вероятность непреднамеренного вреда от мощных ИИ-систем. В недавней статье, _[Concrete problems in AI safety](https://blog.openai.com/concrete-ai-safety-problems/)_, описано несколько конкретных тем для исследований, но на данный момент всего около 20 человек в мире всерьёз изучает подобные вопросы.",
    "articles--existential-risks:488": "4. В области ядерной безопасности многие эксперты считают, что преимущества ядерного оружия для сдерживания агрессии могут быть сохранены при гораздо меньших запасах боеголовок. Но уменьшение запасов также снизит риски аварий и потенциальную опасность ядерных войн для выживания цивилизации.\n",
    "articles--existential-risks:491": "В наших обзорах проблем мы подробнее говорим о том, что вы можете сделать для борьбы с каждым из рисков:",
    "articles--existential-risks:493": "1. [Безопасность в области ИИ](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/)\n2. [Профилактика пандемий](https://80000hours.org/problem-profiles/biosecurity/)\n3. [Ядерная безопасность](https://80000hours.org/problem-profiles/nuclear-security/)\n4. [Изменение климата](https://80000hours.org/problem-profiles/climate-change/)",
    "articles--existential-risks:498": "В этой категории мы не говорим о природных рисках, потому что они намного менее вероятны, и потому что человечество уже многое делает для борьбы с некоторыми из них. Повышение уровней богатства и технологического прогресса делает нас более устойчивыми к природным рискам, а на эти цели и так уже уходит огромное количество наших ресурсов.",
    "articles--existential-risks:500": "### 2\\. Общие усилия по снижению рисков",
    "articles--existential-risks:502": "Вместо того чтобы пробовать снизить каждый риск по отдельности, мы можем попытаться сделать так, чтобы цивилизация в целом лучше с ними справлялась. \"Общие\" усилия помогают снизить все риски сразу, даже те, о которых мы ещё не думали.",
    "articles--existential-risks:504": "Например, есть лица, принимающие ключевые решения, часто работающие в правительствах, которым придётся работать с этими рисками по мере их возникновения. Если мы могли бы улучшить процессы принятия решений этими людьми и учереждениями, то это бы сделало общество более устойчивым в целом, а также решило бы многие другие проблемы.",
    "articles--existential-risks:506": "Последние исследования выявили множество способов улучшить принятие решений, но большинство из них ещё не было внедрено. При это мало кто работает над этим вопросом. Мы более подробно разбираем эту тему в нашей статье об [улучшении институционального принятия решений](https://80000hours.org/problem-profiles/improving-institutional-decision-making/).",
    "articles--existential-risks:508": "Другой пример: мы можем попытаться сделать так, чтобы цивилизации было легче восстановиться после катастрофы. \"Всемирное хранилище семян\" — это замороженое хранилище в Арктике, в котором находятся семена многих важных сортов сельскохозяйственных культур. Его существование снижает шанс того, что мы навсегда потеряем какой-то из них. Талая вода [недавно проникла в тунель, ведущий к хранилищу](https://www.theguardian.com/environment/2017/may/19/arctic-stronghold-of-worlds-seeds-flooded-after-permafrost-melts), вследствие изменения климата (что иронично), поэтому проекту могло бы пригодиться дополнительное финансирование. Можно разработать много других проектов по сохранению знаний, подобных этому.",
    "articles--existential-risks:510": "Также мы могли бы [построить более хорошие убежища на случай чрезвычайных ситуаций](https://www.openphilanthropy.org/research/cause-reports/disaster-shelters), что снизило бы шансы вымирания человечества в результате пандемий, ядерной зимы и ударов астероидов (но не ИИ), а ещё бы повысило шансы успешного восстановления после катастрофы. На данный момент подобные меры выглядят менее эффективными, чем усилия по изначальному снижению рисков, но они всё равно полезны. Более недоценённым, а также возможно намного более дешёвым вариантом будет создание [альтернативных источников пищи](https://en.wikipedia.org/wiki/Feeding_Everyone_No_Matter_What): например, таких, которым не нужен свет, и чьё производство можно быстро нарастить в условиях длительной зимы.",
    "articles--existential-risks:512": "Поскольку общие усилия приносят пользу, даже если мы не уверены в деталях рисков, их привлекательность увеличивается параллельно вашей неопределенности. Чем более ясными и актуальными становятся риски, тем больше ресурсов нужно перенаправлять с общих усилий на прицельные ([подробнее об этом тут](https://www.fhi.ox.ac.uk/the-timing-of-labour-aimed-at-reducing-existential-risk/)).",
    "articles--existential-risks:514": "Мы ожидаем, что есть много других перспективных направлений для общих усилий, но эта область крайне мало изучена. Например, ещё один подход мог бы быть основан на улучшении международной координации. Поскольку эти риски вызваны человечеством, человечество же и может их предотвратить, но нам мешают [проблемы координации](https://conceptually.org/concepts/coordination-problems/). К примеру, Россия не хочет разоружаться, потому что это сделает её слабее по сравнению с США, и наоборот, хотя обе страны бы выиграли от отсутствия возможности ядерной войны.",
    "articles--existential-risks:516": "Однако может быть возможным улучшить нашу способность координироваться на уровне цивилизации: например, при помощи улучшения международных отношений или создания более эффективных международных учереждений. Мы хотели бы видеть больше исследований в области этих направлений.",
    "articles--existential-risks:518": "Популярные направления усилий по улучшению мира, вроде работы над образованием и международным развитием, тоже могут помочь обществу стать более устойчивым и разумным, поэтому также делают вклад в снижение катастрофических рисков. Например, более образованное население скорее всего голосовало бы за более разумных лидеров, а более развитые страны (при прочих равных) лучше справляются с пандемиями — неслучайно вспышки вируса Эбола сконцентрировались в некоторых из беднейших районов Западной Африки.",
    "articles--existential-risks:520": "Но мы не считаем образование и здравоохранение наилучшими областями для приложения усилий по двум причинам. Во-первых, этими областями пренебрегают намного меньше, чем более нестандартными подходами, о которых мы говорили. Более того, работа над образованием [является чуть ли не самым популярным направлением](https://80000hours.org/2017/01/5-reasons-not-to-go-into-education/) для тех, кто хочет приносить пользу миру. В одних лишь Штатах образование получает 800 миллиардов долларов от государства, а также ещё триллион долларов частного финансирования. Во-вторых, эти подходы оказывают намного более диффузное влияние на снижение этих рисков — чтобы иметь значимый эффект, нужно улучшить образование в очень больших масштабах. Поэтому мы предпочитаем работать над более прицельными и недооценёнными решениями.",
    "articles--existential-risks:522": "### 3\\. Расширение знаний и наращивание потенциала",
    "articles--existential-risks:524": "Мы крайне неуверены в том, какие риски являются наиболее значимыми, что с ними лучше всего делать, и какова вероятность того, что наши представления о глобальных приоритетах могут быть полностью ошибочны. Это означает, что ещё одна ключевая цель — получше разобраться во всех этих вопросах.",
    "articles--existential-risks:526": "Мы можем получить больше знаний, просто пробуя снизить эти риски и наблюдая за тем, какого прогресса получается достичь. Однако мы считаем, что наиболее недооценённым и важным способом расширить наши знания являются [исследования глобальных приоритетов](https://80000hours.org/problem-profiles/global-priorities-research/).",
    "articles--existential-risks:528": "Эта область находится на пересечении экономики и моральной философии, а её цель — найти ответы на общие вопросы о наиболее важных проблемах для человечества. Крайне мало исследователей целенаправленно занимаются этими вопросами.",
    "articles--existential-risks:530": "Другой способ работать с неопределённостью — накапливать ресурсы для использования в будущем, когда будет больше информации. Например, можно специально зарабатывать и откладывать деньги. Также можно инвестировать в свой [карьерный капитал](/articles/career-capital) (особенно в переносимые навыки и связи с влиятельными людьми), чтобы увеличить свой будущий потенциал.",
    "articles--existential-risks:532": "Однако мы считаем, что ещё более эффективным подходом может быть создание _сообщества_, ориентированного на снижение этих рисков — какими бы они не оказались в итоге. Причина заключается в том, что работать над развитием сообщества может быть проще, чем в одиночку наращивать свой финансовый или карьерный капитал. Например, если вы потратите год на нетворкинг с целью привлечь больше людей к работе над глобальными проблемами, вполне вероятно, что вы найдёте ещё одного человека с релевантной экспертизой, который будет готов работать с вами. Таким образом, за год работы вы получите выгоду примерно в 100% — если говорить об усилиях, которые благодаря вам идут на решение важнейших проблем.",
    "articles--existential-risks:534": "На данный момент мы [вкладываем наши усилия в сообщество эффективного альтруизма](https://80000hours.org/problem-profiles/promoting-effective-altruism/), в котором много людей, стремящихся снизить экзистенциальные риски. Более того, недавние темпы роста и исследования конкретных инициатив по развитию сообщества говорят о потенциально высокой эффективности этого подхода.",
    "articles--existential-risks:536": "Однако мы ожидаем, что другие усилия по созданию сообществ тоже будут ценными. Мы рады были бы увидеть сообщество ученых, стремящихся продвигать культуру безопасности в академических кругах. Было бы здорово увидеть сообщество политиков, которые хотят попытаться снизить эти риски и сделать так, чтобы правительство начало больше заботиться о будущих поколениях.",
    "articles--existential-risks:538": "С учётом того, как мало людей активно работает над снижением экзистенциальных рисков, мы ожидаем, что можно многое сделать для создания движения вокруг них.",
    "articles--existential-risks:540": "### В целом, насколько эффективным является снижение этих рисков?",
    "articles--existential-risks:542": "Учитывая все подходы к снижению этих рисков, а также то, как мало ресурсов выделяется на борьбу с некоторыми из них, кажется, что существенный прогресс возможен.",
    "articles--existential-risks:544": "Более того, даже если рассматривать влияние этих рисков только на _нынешнее_ поколение (игнорируя любые выгоды для будущих поколений), то они с большой вероятностью будут наиболее приоритетными.",
    "articles--existential-risks:546": "Чтобы это проиллюстрировать, мы приведём несколько грубых, упрощённых цифр. Вот утверждение, которое кажется нам реалистичным: если потратить $100 миллиардов на борьбу с экзистенциальными рисками, то за следующий век можно было бы снизить их шансы более чем на 1%. Снижение этих рисков на единицу процента спасёт, согласно математическому ожиданию, около 100 миллионов жизней в нынешнем поколении (1% от ~10 миллиардов людей, живущих сейчас). Это бы значило, что такое вложение спасло бы все эти жизни, отдав всего $1000 за каждую из них.[^:`В последнем обновлении этой статьи мы поменяли цифры на более умеренные и понятные. Мы считаем, что так лучше передаётся смысл — по сравнению с изначальными цифрами, которые мы использовали (особенно если учитывать всю неопределённость). Изначально было так: \"По нашим грубым оценкам, $10 миллиардов разумных вложений в борьбу с этими рисками могло бы снизить их на 1% за этот век. Другими словами, если сейчас риск составляет 4%, то он стал бы 3%. Снижение этих рисков на единицу процента спасло бы, согласно математическому ожиданию, 100 миллионов жизней (1% от 10 миллиардов). Это бы значило, что таким образом можно спасти жизни, отдав всего $100 за каждую из них.”`]",
    "articles--existential-risks:548": "[Грег Льюис](http://effective-altruism.com/ea/1n0/the_personaffecting_value_of_existential_risk/) сделал более подробные расчёты, согласно которым, средняя стоимость года жизни для нынешнего поколения (при таком подходе) составляла бы $9200, т.е. ~$300 000 за одну жизнь.[^:`[The Person-Affecting Value of Reducing Existential Risk](https://forum.effectivealtruism.org/posts/dfiKak8ZPa46N7Np6/the-person-affecting-value-of-existential-risk-reduction) by Greg Lewis, [архивная версия](https://web.archive.org/web/20180808133606/http://effective-altruism.com/ea/1n0/the_personaffecting_value_of_existential_risk/), доступ проверен в августе 2018.",
    "articles--existential-risks:550": "> Учитывая всё это, модель выдаёт среднюю \"стоимость года жизни\" в $1500-$26 000 (в среднем $9200).`] Там также можно найти и другие оценки. Мы считаем, что Грег скорее всего слишком консервативен, потому что он предполагает, что риск вымирания в течение следующего века равен 1%, когда наша оценка риска — в несколько раз выше. Мы также считаем, что следующий миллиард долларов, который будет потрачен на снижение экзистенциального риска, потенциально может дать больший эффект, чем предполагает Грег (обратите внимание, что это утверждение справедливо только при условии, что этот миллиард будет потрачен на самые недооценённые риски, такие, как безопасность в сфере ИИ и биориск). Таким образом, мы бы не удивились, если бы для следующего миллиарда долларов, вложенного в снижение экзистенциальных рисков, стоимость спасения одной нынешней жизни была бы менее $100.",
    "articles--existential-risks:552": "Первой благотворительной организацией, которую рекомендует GiveWell, является Against Malaria Foundation (AMF). О ней часто говорят как об одной из лучших возможностей для помощи нынешнему поколению, поскольку она спасает жизни где-то за $7500 (цифры на 2017 год).[^:`> По нашим оценкам, AMF тратит примерно $7500 на спасение одной жизни (с учётом транспорта, организационных издержек и так далее).",
    "articles--existential-risks:554": "_Your Dollar Goes Further Overseas_, GiveWell",
    "articles--existential-risks:556": "[Архивная ссылка](https://web.archive.org/web/20171021023916/https://www.givewell.org/giving101/Your-dollar-goes-further-overseas), доступ проверен 21.10.2017.`] Таким образом, эти оценки ставят \"снижение экзистенциальных рисков\" выше или по крайней мере на тот же уровень экономической эффективности по спасению жизней нынешнего поколения, где сейчас находится AMF — благотворительная организация, которую выбрали именно из-за её выдающихся успехов в этой области.",
    "articles--existential-risks:558": "Точно также мы считаем, что если 10 000 талантливых молодых людей сосредоточили бы свою карьеру на этих рисках, они смогли бы снизить их где-то на 1%. Это значит, что каждый из них смог бы спасти 1000 жизней на протяжении своей карьеры, что скорее всего больше, чем если бы они просто зарабатывали и жертвовали деньги в Against Malaria Foundation.[^:` Если на протяжении своей жизни вы пожертвуете миллион долларов (что составляет где-то треть среднего дохода выпускника вуза), и мы расчитаем их эффект, исходя из стоимости жизни для Against Malaria Foundation, то это спасёт где-то 130 жизней.",
    "articles--existential-risks:560": "Источник информации о доходе выпускников вузов:",
    "articles--existential-risks:562": "Carnevale, Anthony P., Stephen J. Rose, and Ban Cheah. \"The college payoff: Education, occupations, lifetime earnings.\" (2011).",
    "articles--existential-risks:564": "[Архивная ссылка](https://web.archive.org/web/20171021024644/https://repository.library.georgetown.edu/bitstream/handle/10822/559300/collegepayoff-complete.pdf?sequence=1), доступ проверен 21.10.2017.`]",
    "articles--existential-risks:566": "С одной стороны, это несправедливые сравнения, потому что оценка GiveWell гораздо лучше обоснована и исследована, в то время как наша оценка скорее является разумным предположением. Также могут быть более хорошие способы помочь нынешнему поколению, чем AMF (например, активизм в сфере законодательства).",
    "articles--existential-risks:568": "Однако мы также _значительно_ преуменьшили полезность снижения экзистенциальных рисков. Основной причиной для защиты цивилизации является не выгода для нынешнего поколения, а помощь будущим поколениям. Мы не учитывали их в этой оценке.",
    "articles--existential-risks:570": "Если учитывать и будущие поколения тоже, то эффективность снижения экзистенциальных рисков становится на порядки выше. В таком случае сложно представить что-то ещё более приоритетное на данный момент.",
    "articles--existential-risks:572": "Теперь вы можете либо прочитать некоторые ответы на эти аргументы, либо перейти к практическим способам принять участие в снижении экзистенциальных рисков.",
    "articles--existential-risks:574": "## Кому _не стоит_ фокусироваться на защите будущего?",
    "articles--existential-risks:576": "<div class=\"panel clearfix \">",
    "articles--existential-risks:578": "Аргументы, представленные в этой статье, основаны на некоторых предположениях, с которыми не все согласятся. Здесь мы разбираем несколько хороших ответов на наши аргументы.",
    "articles--existential-risks:580": "<div class=\"panel-group\"><div class=\"panel panel-default panel-collapse\"><div class=\"panel-heading\">",
    "articles--existential-risks:582": "#### <a class=\"collapsed\" data-toggle=\"collapse\" data-target=\"\\#-6\">Вам нужно уделять больше внимания своим друзьям и близким</a>",
    "articles--existential-risks:584": "</div>",
    "articles--existential-risks:586": "<div class=\"panel-body-collapse collapse\"><div class=\"panel-body\">\nМы говорим только о том, каким должен быть приоритет, если вы пытаетесь помочь людям в целом, в равной степени рассматривая интересы каждого (то, что философы иногда называют \"беспристрастным альтруизмом\").",
    "articles--existential-risks:589": "Большинство людей в какой-то степени хотят помогать другим: если вы можете помочь незнакомцу без особых затрат для себя, то это хороший поступок. Люди также заботятся о своих собственных жизнях, заботятся о своих друзьях и близких — и тут мы ничем не отличаемся.",
    "articles--existential-risks:591": "Как найти баланс между этими приоритетами — сложный вопрос. Если вам повезло, и вы можете себе позволить делать вклад в улучшение мира, то мы считаем, что защита будущего должна быть главным приоритетом. В следующем разделе мы предлагаем несколько конкретных способов поучаствовать в этой инициативе.",
    "articles--existential-risks:593": "В противном случае, вам может быть нужно сосредоточиться на своей личной жизни, делая свой вклад в сохранение цивилизации в свободное время (сейчас или в будущем).",
    "articles--existential-risks:595": "</div>",
    "articles--existential-risks:597": "</div>",
    "articles--existential-risks:599": "</div>",
    "articles--existential-risks:601": "<div class=\"panel panel-default panel-collapse\"><div class=\"panel-heading\">",
    "articles--existential-risks:603": "#### <a class=\"collapsed\" data-toggle=\"collapse\" data-target=\"\\#-7\">Вы считаете, что риски гораздо ниже, чем мы утверждали</a>",
    "articles--existential-risks:605": "</div>",
    "articles--existential-risks:607": "<div class=\"panel-body-collapse collapse\"><div class=\"panel-body\">",
    "articles--existential-risks:609": "У нас нет надежных оценок многих рисков, вызванных деятельностью человека, поэтому вы можете попробовать оценить их самостоятельно и прийти к выводу, что они гораздо ниже, чем предполагаем мы. Если бы они были достаточно низкими, то их снижение перестало бы быть главным приоритетом.",
    "articles--existential-risks:611": "Более низкие оценки не кажутся нам правдоподобными по причинам, о которых мы говорим в статье. Если рассмотреть все потенциальные риски, то трудно утверждать, что они не превысят 1% в течение века, и даже риск в 1% скорее всего требует гораздо более активного вмешательства, чем то, что мы наблюдаем сегодня.",
    "articles--existential-risks:613": "</div>",
    "articles--existential-risks:615": "</div>",
    "articles--existential-risks:617": "</div>",
    "articles--existential-risks:619": "<div class=\"panel panel-default panel-collapse\"><div class=\"panel-heading\">",
    "articles--existential-risks:621": "#### <a class=\"collapsed\" data-toggle=\"collapse\" data-target=\"\\#-8\">Вы считаете, что с этими рисками почти ничего нельзя поделать</a>",
    "articles--existential-risks:623": "</div>",
    "articles--existential-risks:625": "<div class=\"panel-body-collapse collapse\"><div class=\"panel-body\">",
    "articles--existential-risks:627": "Мы оцениваем эти риски как менее \"разрешимые\" по сравнению с проблемами вроде глобального здравоохранения, поэтому стоит ожидать более медленного прогресса за единицу вложений. Тем не менее, мы считаем, что их масштабы и недооценённость с лихвой компенсируют этот факт, и поэтому в итоге работа над ними даёт больше пользы (согласно математическому ожиданию). Многие люди считают, что [эффективный альтруизм](https://www.effectivealtruism.org/) заключается в поддержке только \"проверенных\" вмешательств, но это миф. Над проектами с небольшим шансом успеха стоит работать, если ожидаемая выгода достаточно высока. Ведущий инвестор в нашем сообществе теперь выступает за подход [\"оправданных рисков\"](https://www.openphilanthropy.org/blog/hits-based-giving) в филантропии.",
    "articles--existential-risks:629": "Однако если вы _гораздо_ более пессимистично оцениваете вероятность прогресса, чем мы, то возможно стоит работать над более популярными проблемами, вроде глобального здравоохранения.",
    "articles--existential-risks:631": "Лично мы могли бы переключиться на другую проблему, если бы в снижение экзистенциальных рисков вкладывалось в 100 раз больше ресурсов. Но до этого ещё далеко.",
    "articles--existential-risks:633": "Ещё один ответ на наши аргументы может заключаться в том, что мы уже принимаем наилучшие меры для снижения этих рисков. Это значит, что на практике не требуется смены приоритетов. Например, раньше мы упоминали, что образование скорее всего помогает снижать эти риски. Если вы считаете, что образование является _лучшим_ способом с ними справиться (например, потому что вы очень неуверены в том, какие риски будут наиболее актуальными), то тогда, поскольку человечество уже вкладывает огромное количество ресурсов в образование, вы можете считать, что ситуация и так под контролем. Такой вывод не кажется нам правдоподобным, потому что, как уже было сказано, существует множество неиспользованных возможностей для снижения этих рисков, которые выглядят более прицельными и недооценёнными.",
    "articles--existential-risks:635": "Другой пример: экономисты иногда утверждают, что нам следует просто сосредоточиться на экономическом росте, поскольку это позволит нам наилучшим образом справиться с рисками в будущем. Это не кажется нам убедительным, поскольку некоторые виды экономического роста увеличивают риски (например, открытие нового оружия), поэтому нельзя утверждать, что экономический рост однозначно является лучшим способом снижения рисков. Вместо этого мы бы сфокусировались хотя бы на [избирательном технологическом развитии](https://en.wikipedia.org/wiki/Differential_technological_development) или других более прицельных усилиях, перечисленных выше.",
    "articles--existential-risks:637": "</div>",
    "articles--existential-risks:639": "</div>",
    "articles--existential-risks:641": "</div>",
    "articles--existential-risks:643": "<div class=\"panel panel-default panel-collapse\"><div class=\"panel-heading\">",
    "articles--existential-risks:645": "#### <a class=\"collapsed\" data-toggle=\"collapse\" data-target=\"\\#-9\">Вы считаете, что есть более хороший способ помочь будущему</a>",
    "articles--existential-risks:647": "</div>",
    "articles--existential-risks:649": "<div class=\"panel-body-collapse collapse\"><div class=\"panel-body\">",
    "articles--existential-risks:651": "Хотя снижение этих рисков стоит того для нынешнего поколения, большая часть их значимости обусловлена [их долосрочными последствиями](https://80000hours.org/articles/future-generations/) — если цивилизации придёт конец, мы распрощаемся со всем возможным будущим.",
    "articles--existential-risks:653": "Вы можете считать, что у текущего поколения есть и другие доступные действия с очень долгосрочными последствиями, которые могут быть столь же важны для снижения риска вымирания. В частности, мы могли бы улучшить _качество_ будущего, предотвратив возможность необратимого сокращения возможных сценариев для нашей цивилизации до одних лишь плохих.",
    "articles--existential-risks:655": "Это может выглядеть как научная фантастика, но всё же. Один из возможных сценариев, которые были озвучены, заключается в изобретении новых технологий, таких как радикальная массовая слежка или психологический контроль, которые позволяют создать бессменное тоталитарное правительство. Эти сценарии были описаны в \"_1984_\" и в \"_Дивном новом мире_\", соответственно. Если такое правительство окажется плохим, то цивилизацию может ждать судьба ещё хуже, чем вымирание — тысячи лет страданий.",
    "articles--existential-risks:657": "Другие высказывают опасения, что разработка продвинутых ИИ-систем может нанести страшный вред, если будет вестись безответственно: например, из-за конфликта между несколькими группами, ставящими своей целью разработку этой технологии. В частности, если в будущем разработка этих систем повлечет за собой создание сознательных цифровых разумов, их благополучие может стать невероятно важным приоритетом.",
    "articles--existential-risks:659": "Риски сценариев будущего, в которых присутствует невероятно большое количество страданий, были названы \"s-рисками\".[^:`S-риски — это риски плохих исходов, которые влекут за собой страдания астрономических масштабов, чье количество непомерно превышает все страдания, когда-либо существовавшие на Земле.\n>\n> S-риски являются подгруппой экзистенциальных рисков (или x-рисков)... Ник Бостром определяет x-риск следующим образом:\n>\n> \"Экзистенциальный риск — риск плохого исхода, который приведёт к вымиранию разумной жизни родом с нашей планеты или необратимо и радикально ограничит её потенциал.\"",
    "articles--existential-risks:665": "_S-risks: Why they are the worst existential risks, and how to prevent them_, Max Daniel, Foundational Research Institute, 2017,",
    "articles--existential-risks:667": "[Архивная ссылка](https://web.archive.org/web/20171021025032/https://foundational-research.org/s-risks-talk-eag-boston-2017/), доступ проверен 21.10.2017.`] Если сегодня можно сделать что-то для предотвращения s-риска (например, при помощи специальных исследований в областях техбезопасности ИИ и контроля ИИ), это может быть ещё более важным.",
    "articles--existential-risks:669": "Еще одна область, на которую следует обратить внимание — крупные технологические переходы. В этой статье мы говорили об опасностях генной инженерии и искусственного интеллекта, но внедрение этих технологий также может привести к ещё одной индустриальной революции и сделать очень много хорошего. Возможно есть действия, которые позволят нам повысить вероятность благополучного перехода, а не снизить риски плохого. Это называют попытками увеличить \"экзистенциальную надежду\", а не снизить \"экзистенциальные риски.\"[^:`_Existential Risk and Existential Hope: Definitions_, Owen Cotton-Barratt & Toby Ord, Future of Humanity Institute, 2015",
    "articles--existential-risks:671": "[Архивная ссылка](https://web.archive.org/web/20171021025945/https://www.fhi.ox.ac.uk/Existential-risk-and-existential-hope.pdf), доступ проверен 21.10.2017`]",
    "articles--existential-risks:673": "Мы согласны с тем, что могут быть и другие способы добиться очень долгосрочного эффекта, и что они могут быть более важными, чем снижение риска вымирания. Однако большинство этих предложений ещё недостаточно хорошо проработаны, и мы не уверены в том, что с ними делать.",
    "articles--existential-risks:675": "Для нас главный практический результат рассмотрения других способов влияния на будущее заключается в том, что мы считаем _ещё более_ важным положительно влиять на переходы к новым революционным технологиям, таким как ИИ. Мы также хотели бы видеть больше исследований глобальных приоритетов, посвящённых этим вопросам.",
    "articles--existential-risks:677": "В целом, мы по-прежнему считаем, что сначала стоит сосредоточиться на снижении экзистенциальных рисков, после чего можно будет перейти к другим способам помочь будущему.",
    "articles--existential-risks:679": "Одним из способов помочь будущему, который _не кажется_ нам хорошей альтернативой, является его ускорение. Некоторые люди, стремящиеся помочь будущему, фокусируются на технологическом прогрессе (например, разрабатывают вакцины), и у этого правда есть хорошие долгосрочные последствия. Но мы считаем, что с долгосрочной точки зрения более важным является то, где мы окажемся, а не то, как быстро мы туда попадём. Открытие новой вакцины скорее всего означает, что мы получим её раньше, а не то, что иначе она вообще бы не появилась.",
    "articles--existential-risks:681": "Более того, поскольку технологии также являются причиной многих из этих рисков, мы не знаем, насколько ускорение их развития будет положительным в краткосрочной перспективе.",
    "articles--existential-risks:683": "Ускорение прогресса также получает намного больше внимания, поскольку приносит пользу и нынешнему поколению. Как мы уже писали, более 1 триллиона долларов ежегодно тратится на развитие новых технологий. Поэтому ускорение одновременно и менее приоритетно, и менее недооценено.",
    "articles--existential-risks:685": "О других способах помощи будущим поколениям можно прочитать в третьей главе диссертации Ника Бекстеда, [_On the Overwhelming Importance of Shaping the Far Future_](https://rucore.libraries.rutgers.edu/rutgers-lib/40469/).",
    "articles--existential-risks:687": "</div>",
    "articles--existential-risks:689": "</div>",
    "articles--existential-risks:691": "</div>",
    "articles--existential-risks:693": "<div class=\"panel panel-default panel-collapse\"><div class=\"panel-heading\">",
    "articles--existential-risks:695": "#### <a class=\"collapsed\" data-toggle=\"collapse\" data-target=\"\\#-10\">Вы уверены, что будущее будет коротким или плохим</a>",
    "articles--existential-risks:697": "</div>",
    "articles--existential-risks:699": "<div class=\"panel-body-collapse collapse\"><div class=\"panel-body\">",
    "articles--existential-risks:701": "Если вы считаете, что недолгое существование цивилизации практически гарантированно, то ценность снижения экзистенциальных рисков значительно снижается (хотя быть может всё равно стоит того, чтобы помочь нынешнему поколению и любому небольшому числу будущих поколений).",
    "articles--existential-risks:703": "Мы согласны, что существует значительная вероятность скорого конца цивилизации (именно поэтому этот вопрос так важен), но мы также считаем, что существует _достаточно_ большая вероятность того, что она может просуществовать очень долго, что делает [сохранение будущего оправданной целью](https://80000hours.org/articles/future-generations/).",
    "articles--existential-risks:705": "Аналогично, если вы считаете, что будущее скорее будет плохим, чем хорошим, то ценность снижения этих рисков снижается (или если считаете, что намного более важно снижать страдания, чем увеличивать благополучие). Однако нам не кажется, что это вероятно, поскольку люди _хотят_ хорошего будущего, поэтому _постараются_ сделать его таким. Мы также думаем, что за последние несколько столетий произошел значительный моральный прогресс (благодаря тенденциям, отмеченным ранее), и мы полагаем, что он продолжится. См. более подробное обсуждение в сноске 11.[^:`Становится ли мир лучше?",
    "articles--existential-risks:707": "Хотя есть причины считать, что большинство показателей прогресса увеличивается (как показано в статье), есть некоторые аспекты, в которых жизнь могла стать хуже. Например, в книге [_Sapiens_](https://www.amazon.com/Sapiens-Humankind-Yuval-Noah-Harari/dp/0062316095) Юваль Харари утверждает, что в современную эпоху усилились проблемы одиночества и психического здоровья, в то время как ощущения значимости и смысла могли снизиться. Мы скептически относимся к тому, что эти минусы перевешивают плюсы, но сложно сказать наверняка, какова ситуация на самом деле.",
    "articles--existential-risks:709": "Более весомые аргументы в пользу того, что мир становится хуже, возникают в контексте нашего воздействия на животных. В частности, с 1960-х годов резко выросло промышленное животноводство, и сейчас [где-то более 30 миллиардов животных](https://80000hours.org/problem-profiles/factory-farming/) ежегодно живут в ужасных условиях на фабриках. Если нас волнуют страдания этих животных, то это может перевесить наши успехи в сфере человеческого благополучия.",
    "articles--existential-risks:711": "Учитывая все эти аргументы, мы не можем однозначно утверждать, что суммарное благополучие выросло. Однако более важным является вопрос того, что нас ждет в будущем.",
    "articles--existential-risks:713": "Станет ли мир лучше?",
    "articles--existential-risks:715": "Мы считаем, что до тех пор, пока человечество существует, развитие технологий и моральный прогресс дают нам возможности справиться с самыми серьёзными социальными проблемами, а также жить гораздо лучше в будущем. Если отодвинуть в сторону экзистенциальные угрозы, то многие конкретные глобальные проблемы могут быть решены, если уровни богатства, технологического развития, а также морального и политического прогресса будут и дальше повышаться.",
    "articles--existential-risks:717": "Например, в случае с промышленным животноводством мы ожидаем, что по мере того, как люди будут становиться богаче, проблема будет уменьшаться. Во-первых, богатые люди более склонны к \"этичному\" потреблению, потому что могут себе это позволить. Во-вторых, технологии способны положить конец промышленному животноводству при помощи заменителей мяса, искуственно выращенного мяса или более гуманных методов ведения сельского хозяйства. В-третьих, как мы видим, забота о других живых существах увеличилась с течением времени (\"расширяющийся круг заботы\"), поэтому мы ожидаем, что в будущем люди будут ещё больше заботиться о благополучии животных.",
    "articles--existential-risks:719": "Если посмотреть ещё шире, то в целом мы ожидаем, что будущее будет лучше, потому что люди сами этого хотят. Чем больше технологической мощи и личных свобод мы имеем, тем проще людям реализовывать свои ценности. Поскольку люди _хотят_ жить хорошо, улучшение будущего — более вероятный сценарий, чем его ухудшение.",
    "articles--existential-risks:721": "При этом остаётся много вопросов. Например, многие наши ценности в какой-то степени противоречат другим, и это может привести к конфликтам. Вопросы о том, что ждет нас в будущем, также мало изучены. Поэтому хотя мы и ожидаем, что в будущем будет лучше, мы также признаём, что в нашем суждении присутствует значительная степень неопределённости.`]",
    "articles--existential-risks:723": "Более того, даже если вы не уверены, насколько хорошим будет будущее, или подозреваете, что оно окажется плохим (в тех аспектах, с которыми мы в принципе можем справиться), у вас могут быть причины хотеть, чтобы цивилизация выжила и сохранила возможность выбора. У людей будущего будет гораздо больше времени на изучение того, стоит ли цивилизации расширяться, оставаться тех же размеров или уменьшиться. Если вы считаете, что есть неплохие шансы того, что мы сможем учесть эти этические опасения, то это будет хорошей причиной оставить последнее слово за разумными людьми будущих поколений. В целом, степень нашей неопределённость по поводу этих больших вопросов крайне высока, но это скорее _увеличивает_ наши опасения по поводу любых необратимых сценариев.[^:`Это справедливо лишь в том случае, если вы при этом считаете, что этические размышления имеют смысл, что моральный прогресс на самом деле возможен, и ваша теория моральной неопределённости не противоречит идее о том, что оставлять возможность для разных сценариев будущего — хорошо.`]",
    "articles--existential-risks:727": "</div>",
    "articles--existential-risks:729": "</div>",
    "articles--existential-risks:731": "</div>",
    "articles--existential-risks:733": "<div class=\"panel panel-default panel-collapse\"><div class=\"panel-heading\">",
    "articles--existential-risks:735": "#### <a class=\"collapsed\" data-toggle=\"collapse\" data-target=\"\\#-11\">Вы уверены, что с этической точки зрения помогать текущему поколению гораздо важнее</a>",
    "articles--existential-risks:737": "</div>",
    "articles--existential-risks:739": "<div class=\"panel-body-collapse collapse\"><div class=\"panel-body\">",
    "articles--existential-risks:741": "Если вы уверены, что у нас гораздо больше обязательств перед нынешним поколением, чем перед предыдущими (например, потому что поддерживаете личностно-ориентированный подход к этике), то важность снижения экзистенциальных рисков уменьшится. Лично нам [этот подход не кажется особо убедительным](https://80000hours.org/articles/future-generations/#3-do-we-have-moral-obligations-to-future-generations).",
    "articles--existential-risks:743": "Тем не менее, мы утверждали, что даже если игнорировать будущие поколения, эти риски кажутся достойными внимания. Борьба с ними всё равно может относительно дёшево спасти жизни нынешнего поколения, а ещё избежать многих страданий от катастроф среднего масштаба.",
    "articles--existential-risks:745": "Более того, если вы не уверены насчёт того, есть ли у нас моральные обязательства перед будущими поколениями, тогда вам опять же следует оставить возможность для различных сценариев, а для этого нужно сохранить цивилизацию.",
    "articles--existential-risks:747": "Тем не менее, если сложить взгляд о том, что у нас нет серьёзных обязательств перед будущими поколениями, и позицию, согласно которой с экзистенциальными рисками мало что можно сделать, или что нет полезных исследований, которые можно было бы провести, то какой-то другой способ помощи текущим поколениям может выйти на первое место. Это может быть работа в области глобального здравоохранения, ментального здоровья или ускорение развития технологий. Или же вы можете решить, что есть более важные моральные проблемы, такие как промышленное животноводство.",
    "articles--existential-risks:749": "</div>",
    "articles--existential-risks:751": "</div>",
    "articles--existential-risks:753": "</div>",
    "articles--existential-risks:755": "</div>",
    "articles--existential-risks:757": "</div>",
    "articles--existential-risks:759": "* * *",
    "articles--existential-risks:761": "## Хотите помочь снизить экзистенциальные риски?",
    "articles--existential-risks:763": "!img~ class=\"aligncenter size-large wp-image-40225\" width=\"1024\" height=\"783\" ~[](https://80000hours.org/wp-content/uploads/2017/10/8e397f553a7e23150e0304341d908e98-1024x783.jpg)",
    "articles--existential-risks:765": "Наше поколение может либо подтолкнуть нас к концу нашего мира, либо стать тем поколением, которое проведёт человечество через самый опасный период и станет одним из самых важных поколений в истории.",
    "articles--existential-risks:767": "Мы можем стать либо тем поколением, что сделает возможным достижение поразительного и процветающего мира, либо тем, что поставит всё под угрозу.",
    "articles--existential-risks:769": "Если мы хотим помочь миру, то нужно направить свои усилия именно сюда.",
    "articles--existential-risks:771": "Если вы хотите сфокусировать свою карьеру на снижении экзистенциальных рисков и защите нашего будущего, мы хотим вам помочь. Мы написали статью, в которой описаны различные возможности, а также первые шаги, которые помогут вам начать.",
    "articles--existential-risks:773": "<a href=\"https://80000hours.org/articles/how-to-reduce-existential-risk/\" class=\"btn btn-primary\">Как задействовать свою карьеру для снижения экзистенциальных рисков</a>",
    "articles--existential-risks:775": "После прочтения этой статьи (или если вы уже подумали о том, чем именно хотите заниматься), вы можете поговорить с нами лично. Мы поможем вам обдумать различные решения и сформулировать ваш план.",
    "articles--existential-risks:777": "<a href=\"https://80000hours.org/speak-with-us/?int_campaign=existential-risks\" class=\"btn btn-primary\">Подать заявку на индивидуальное консультирование</a>",
    "articles--existential-risks:779": "## Дальнейшее чтение",
    "articles--existential-risks:781": "- Прочитайте о [десятках идей в областях регулирования и научных исследований](https://80000hours.org/2020/04/longtermist-policy-ideas/), которые могут помочь снизить экзистенциальные риски.\n- Ознакомьтесь с академической версией наших доводов в [этой статье](http://www.existential-risk.org/concept.html) профессора Ника Бострома.\n- [Прочитайте, почему нужно сфокусироваться на будущих поколениях](https://80000hours.org/articles/future-generations/).\n- Прочитайте [альтернативную вводную статью про то, почему наше влияние на будущие поколения имеет огромную моральную важность](https://www.effectivealtruism.org/articles/longtermism/) — в ней также есть раздел о [возражениях](https://www.effectivealtruism.org/articles/longtermism/#objections).",
    "articles--existential-risks:786": "- Мы обсуждаем эту тему в наших подкастах с [Тоби Ордом](https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/) и [Ником Бекстедом](https://80000hours.org/2017/10/nick-beckstead-giving-billions/), а также в [других эпизодах подкаста](/podcast/) про конкретные виды рисков.",
    "articles--existential-risks:788": "- Послушайте это интервью с Томасом Мойнханом про [историю развития мысли в области экзистенциального риска](https://hearthisidea.com/episodes/thomas).\n- Мы также рекомендуем наш подкаст с [Карлом Шульманом, где мы обсуждаем простые аргументы в пользу работы над экзистенциальными рисками и её практическую значимость](https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/).",
    "articles--existential-risks:791": "<div class=\"tw--mt-6 tw--p-3 tw--pt-2 tw--bg-gray-lighter tw--rounded-md \">",
    "articles--existential-risks:793": "### <a class=\"tw--text-off-black hover:tw--text-off-black hover:tw--no-underline focus:tw--text-off-black\" href=\"https://80000hours.org/articles/the-most-important-century/\"> <small>Читайте дальше: </small> Это может быть самый важный век </a>",
    "articles--existential-risks:795": "<div class=\"tw--grid xs:tw--grid-flow-col tw--gap-3\"><div class=\"xs:tw--order-last tw--pt-1\">\n[!img~ width=\"720\" height=\"448\" ~[Decorative post preview](https://80000hours.org/wp-content/uploads/2021/09/this-cant-go-on-720x448.png)](https://80000hours.org/articles/the-most-important-century/)\n</div>",
    "articles--existential-risks:799": "<div><div class=\"tw--pb-3\">",
    "articles--existential-risks:801": "Почему разработка продвинутых ИИ-систем (вместе со стремительным экономическим ростом и научным прогрессом, которые за этим последуют) может привести нас к крайне необычному будущему значительно быстрее, чем может казаться — и почему это делает 21-ый век самым важным в истории.",
    "articles--existential-risks:803": "</div>",
    "articles--existential-risks:805": "<div>\n<a href=\"https://80000hours.org/articles/the-most-important-century/\" class=\"btn btn-primary\">Продолжить →</a>\n</div>",
    "articles--existential-risks:809": "</div>",
    "articles--existential-risks:811": "</div>",
    "articles--existential-risks:813": "</div>",
    "articles--existential-risks:815": "<div class=\"well bg-gray-lighter margin-bottom margin-top padding-top-small padding-bottom-small\">",
    "articles--existential-risks:817": "### Оставьте свой имейл, и мы пришлём вам книгу (бесплатно)",
    "articles--existential-risks:819": "Подпишитесь на нашу рассылку, и мы пришлем вам бесплатную копию книги \"_Пропасть_\" философа Тоби Орда — про то, как справиться с главными угрозами, стоящими перед человечеством.",
    "articles--existential-risks:821": "!newsletter-subscribe",
    "articles--existential-risks:823": "</div>",
    "articles--existential-risks:825": "<div class=\"margin-top-large margin-bottom-large\">\n!social-share",
    "articles--existential-risks:828": "</div>",
    "articles--existential-risks:830": "<footer><div class=\"margin-top margin-bottom-large\"><ul class=\"post-categories tags\"><li class=\"tags__tag\"><a href=\"https://80000hours.org/topic/causes/catastrophic-risks/\" title=\"View all posts on topic: Catastrophic risks\" rel=\"category tag\">Catastrophic risks</a></li><li class=\"tags__tag\"><a href=\"https://80000hours.org/topic/causes/future-generations/\" title=\"View all posts on topic: Future Generations\" rel=\"category tag\">Future Generations</a></li><li class=\"tags__tag\"><a href=\"https://80000hours.org/topic/big-picture/longtermism/\" title=\"View all posts on topic: Longtermism\" rel=\"category tag\">Longtermism</a></li></ul></div></footer>\n"
}
