{
    "articles--existential-risks:000": "# Почему нужно снижать экзистенциальные риски\n## !empty\n<p class=\"entry-meta small\">Автор: <span class=\"byline author vcard\"><a href=\"https://80000hours.org/author/benjamin-todd/\" rel=\"author\" class=\"fn\">Бенджамин Тодд</a></span> · Опубликовано в <time class=\"published\" datetime=\"2017-10-27T23:41:47+00:00\">октябре 2017</time> · Обновлено <time class=\"update\" datetime=\"2022-06-08T00:00:00+00:00\">8 июля 2022</time></p>",
    "articles--existential-risks:004": "!podcast-player",
    "articles--existential-risks:006": "В 1939 году Эйнштейн пишет Рузвельту:[^:``]",
    "articles--existential-risks:008": "> Есть причины считать, что запуск цепной ядерной реакции в большой массе урана возможен ... и можно предположить, хоть и с меньшей степенью уверенности, что это позволит создавать чрезвычайно мощные бомбы нового типа.",
    "articles--existential-risks:010": "Всего через несколько лет эти бомбы были созданы. Спустя десять с небольшим лет их число достигло такого уровня, что впервые в истории маленькая группа людей, принимающих решения, могла полностью уничтожить цивилизацию.",
    "articles--existential-risks:012": "Человечество вступило в новую эру. Теперь, помимо природных экзистенциальных рисков,[^:`Ник Бостром определяет понятие [экзистенциального риска](https://www.nickbostrom.com/existential/risks.html) как некое событие, которое \"может привести к вымиранию человечества или необратимо и радикально ограничить его потенциал\". Экзистенциальный риск отличается от [риска глобальной катастрофы (РГК)](https://en.wikipedia.org/wiki/Global_catastrophic_risk) по масштабу — РГК является катастрофичным в планетарном масштабе, но включает возможность последующего восстановления. Насколько мы можем судить, термин [\"экзистенциальная угроза\"](https://www.theatlantic.com/ideas/archive/2019/06/2020-candidates-say-everything-existential-threat/591967/) обычно используется как усиление, с целью представить некую проблему как более ужасную.`] мы встретились с риском самоуничтожения.",
    "articles--existential-risks:014": "<div class=\"well bg-gray-lighter margin-bottom margin-top padding-top-small padding-bottom-small\">",
    "articles--existential-risks:016": "### Предпочитаете подкаст?",
    "articles--existential-risks:018": "После публикации этой статьи мы записали два подкаста на тему экзистенциальных рисков с доктором Тоби Ордом, философом из Оксфорда и доверенным лицом проекта \"80 000 часов\". Мы считаем, что они являются столь же хорошим введением в эту тему, как и данная статья, если не лучшим. Их можно послушать тут:",
    "articles--existential-risks:020": "- [Тоби Орд: о \"пропасти\" и различных сценариях будущего человечества](https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/)",
    "articles--existential-risks:022": "- [Почему долгосрочное будущее человечества имеет первостепенную важность, и что с этим делать](https://80000hours.org/podcast/episodes/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/)\n",
    "articles--existential-risks:025": "</div>",
    "articles--existential-risks:027": "<div class=\"panel clearfix \">",
    "articles--existential-risks:029": "[!img~ style=\"width: 30%; margin-left: 10%; margin-top:-10px; float: right;\" ~[book cover](https://80000hours.org/wp-content/uploads/2020/03/the-precipice-3d.jpg)](/the-precipice/)",
    "articles--existential-risks:031": "<div style=\"margin-top: -10px;\">",
    "articles--existential-risks:033": "### Предпочитаете книгу?",
    "articles--existential-risks:035": "У доктора Тоби Орда, философа из Оксфорда и доверенного лица проекта \"80 000 часов\", недавно вышла книга _[\"Пропасть: Экзистенциальный риск и будущее человечества\"](/the-precipice/)_, в которой даётся обзор моральной значимости будущих поколений и того, что мы можем сделать сегодня, чтобы помочь им.",
    "articles--existential-risks:037": "#### Мы вышлем вам книгу по почте (бесплатно)",
    "articles--existential-risks:039": "Присоединяйтесь к рассылке \"80 000 часов\", и мы пришлём вам бесплатный экземпляр книги.",
    "articles--existential-risks:041": "Также мы будем присылать свежую информацию о наших исследованиях, вакансии по работе над экзистенциальными рисками и новости от автора.",
    "articles--existential-risks:043": "!newsletter-subscribe",
    "articles--existential-risks:045": "Если вы уже подписаны на нашу рассылку, напишите нам на [book.giveaway@80000hours.org](/cdn-cgi/l/email-protection#89ebe6e6e2a7eee0ffece8fee8f0afeae6e4e4e8fdb2b1b9b9b9b9e1e6fcfbfaa7e6fbee), чтобы получить экземпляр книги.",
    "articles--existential-risks:047": "</div>",
    "articles--existential-risks:049": "</div>",
    "articles--existential-risks:051": "Что должно быть главным приоритетом нашей цивилизации в этих новых условиях? Совершенствование технологий? Помощь малоимущим? Изменение политической системы?",
    "articles--existential-risks:053": "Вот предложение, о котором говорят не так часто: нашим главным приоритетом должно быть _выживание_.",
    "articles--existential-risks:055": "Пока цивилизация продолжает существовать, у нас есть шанс решить все остальные проблемы и прийти к гораздо лучшему будущему. Но если мы вымрем, то это будет конец.",
    "articles--existential-risks:057": "Почему об этом мало говорят? Вот одна из причин: многие люди ещё не успели заметить, что общая ситуация в мире значительно изменилась, и поэтому не считают, что наше будущее находится под угрозой.",
    "articles--existential-risks:059": "Исследователь в области социальных наук [Спенсер Гринберг](http://www.spencergreenberg.com/) провел опрос среди американцев, чтобы узнать, как они оценивают шансы вымирания человечества в течение 50 лет. Результаты показали, что многие считают шансы крайне низкими: более 30% полагают, что они меньше 1 к 10 миллионам.[^:`Гринберг опрашивал пользователей Mechanical Turk, чей средний возраст — 20-40 лет, а уровень образования, как правило, выше среднего, поэтому опрос не отражает мнения всех американцев. Подробнее смотрите в этом видео:",
    "articles--existential-risks:061": "[Social Science as Lens on Effective Charity: results from four new studies - Spencer Greenberg](https://www.youtube.com/watch?v=tOSpj19eows), таймкод 12:15.",
    "articles--existential-risks:063": "Первоначальный опрос показал, что медианная оценка шансов вымирания человечества в течение 50 лет составляет 1 к 10 миллионам. Гринберг провел три повторных исследования, которые дали более высокие оценки. Самая высокая из них показала медиану 1 к 100. Однако даже в этом случае 39% респондентов все равно предположили, что шансы были ниже 1 к 10 000 (что примерно равно шансам столкновения Земли с астероидом диаметром в 1 км). Во всех случаях более 30% считали, что шансы меньше 1 к 10 миллионам. Сводку всех опросов можно посмотреть [здесь](http://spencergreenberg.com/documents/Comparison%20of%20study%20results%20about%20probability%20of%20human%20extinction.xlsx).",
    "articles--existential-risks:065": "Обратите внимание: когда мы спрашивали людей о шансах вымирания без конкретных временных рамок, оценки были гораздо выше. В одном из опросов медианное значение ответов составило 75%. Их можно понять: человечество _когда-нибудь_ вымрет. Это помогает объяснить расхождение с некоторыми другими опросами. Например, исследование \"Climate Change in the American Mind\" (май 2017 года, [архивная ссылка](https://web.archive.org/web/20171031033116/http://climatecommunication.yale.edu/wp-content/uploads/2017/07/Climate-Change-American-Mind-May-2017.pdf)) показало, что средний американец считает, что шансы вымирания в результате изменения климата составляют примерно 1 к 3. Однако в этом исследовании не задавался вопрос о конкретных сроках. Когда Гринберг попытался воспроизвести результат с тем же вопросом, он получил аналогичную цифру. Но когда Гринберг спросил о шансах вымирания в результате изменения климата в ближайшие 50 лет, медиана упала до 1%. Многие другие исследования также некорректно обращаются с низкими оценками вероятности — люди обычно не дают оценку в 0,00001%, если такой вариант не был изначально предложен.",
    "articles--existential-risks:067": "Однако, как можно заметить, подобные опросы обычно дают очень ненадёжные результаты. Ответы зависят от конкретной формулировки вопроса и от контекста. Отчасти это связано с тем, что люди очень плохо оценивают низкие вероятности. Из-за этого трудно дать точную оценку того, что думает население в целом, однако ничто из полученных данных не опровергает идею о том, что значительное число людей (скажем, более 25%) считает, что шансы вымирания человечества в краткосрочной перспективе крайне малы и скорее всего даже ниже, чем вероятность столкновения с астероидом. Более того, флуктуации этих оценок не добавляют уверенности в том, что человечество рационально учитывает эти риски.`]",
    "articles--existential-risks:069": "Сначала мы тоже считали, что подобные риски крайне малы, но, изучив этот вопрос, мы стали думать иначе. Далее мы увидим, что исследователи, изучающие эти вопросы, считают, что реальный риск вымирания в 1000+ раз выше оценки из опроса и скорее всего лишь продолжает расти.",
    "articles--existential-risks:071": "Эти опасения положили начало новому движению по защите цивилизации, к которому присоединились Стивен Хокинг, Макс Тегмарк и новые институты, основанные исследователями из [Кембриджа](https://www.cser.ac.uk/about-us/), [Массачусетского технологического института](https://futureoflife.org/), [Оксфорда](https://www.fhi.ox.ac.uk/) и других мест.",
    "articles--existential-risks:073": "В остальной части этой статьи мы расскажем о самых больших рисках для цивилизации, в том числе о потенциально более серьёзных, чем ядерная война или изменение климата. Затем мы приведём доводы в пользу того, что снижение этих рисков может быть самым важным делом вашей жизни, и объясним, что конкретно вы можете для этого сделать. Если вы захотите использовать свою карьеру для работы над этими проблемами, мы также можем предоставить вам [индивидуальную поддержку](https://80000hours.org/speak-with-us/?int_campaign=existential-risks).",
    "articles--existential-risks:075": "_Время чтения: 25 минут_",
    "articles--existential-risks:077": "## Какова вероятность того, что вас убьет астероид? Обзор природных экзистенциальных угроз",
    "articles--existential-risks:079": "Шанс вымирания 1 к 10 миллионам в ближайшие 50 лет — оценка, которую дают многие люди — не соответствует реальности. Природные экзистенциальные угрозы можно довольно точно оценить из истории, и их шансы гораздо выше.",
    "articles--existential-risks:081": "Столкновение Земли с километровым астероидом может привести к уничтожению цивилизации. На основе исторических данных и наблюдений за небесными телами, астрономы оценивают риск столкновения такого астероида с Землёй как 1 к 5000 в столетие.[^:`> Чтобы привести к вымиранию человечества, столкнувшийся с Землёй объект скорее всего должен быть больше 1 км в диаметре (а если точнее, то где-то в районе 3-10 км). На Земле произошло по крайней мере пять, а может и более десятка массовых вымираний, и по крайней мере некоторые из них скорее всего были вызваны падениями метеоритов (\\[9\\], стр. 81 и далее). В частности, К-Т вымирание (в результате которого вымерли динозавры), случившееся 65 миллионов лет назад, связывают с падением астероида диаметром от 10 до 15 км на полуостров Юкатан. По оценкам, тело диаметром от 1 км сталкивается с Землёй примерно раз в 0,5 миллиона лет. Мы знаем лишь о небольшой части потенциально опасных тел.",
    "articles--existential-risks:083": "Bostrom, Nick. \"Existential risks: Analyzing human extinction scenarios and related hazards.\" (2002). [Архивная ссылка](https://web.archive.org/web/20171022043143/https://nickbostrom.com/existential/risks.html), получена 21.10.2017.`] Это выше, чем шансы большинства людей попасть в авиакатастрофу (примерно 1 к 5 миллионам за рейс), и уже примерно в 1000 раз выше, чем шанс 1 к 10 миллионам, который дали некоторые люди в качестве своей оценки.[^:` Указаны шансы упасть в Атлантический океан на самолете A330, на рейсе от компании Virgin, летевшем из Хитроу в Кеннеди (1 к 5,4 миллионам). Таким образом, вам нужно сделать 1000 перелётов, чтобы для вас вероятность попадания в авиакатастрофу сравнялась с вероятностью попасть в катастрофу из-за удара астероида.",
    "articles--existential-risks:085": "_A crash course in probability_, The Economist, 2015.",
    "articles--existential-risks:087": "[Ссылка](https://www.economist.com/blogs/gulliver/2015/01/air-safety), получена 14.10.2017`]",
    "articles--existential-risks:089": "Некоторые утверждают, что хотя столкновение с километровым объектом было бы катастрофой, такого размера недостаточно, чтобы привести к вымиранию человечества, поэтому такая оценка риска скорее из высоких. Но с другой стороны, существуют и другие природные риски, такие как супервулканы.[^:` Достаточно большой супервулкан тоже может вызвать долгую зиму, которая положит конец жизни. К другим природным рискам можно отнести особо смертоносную пандемию, вспышку близкой к нам сверхновой или гамма-всплеск, или же вызванное естественными причинами резкое изменение климата.`]",
    "articles--existential-risks:091": "При всём этом естественные риски всё ещё довольно малы в абсолютных числах. В статье доктора Тоби Орда, которая выйдет в ближайшее время, подсчитано, что если сложить все естественные риски вместе, то полученные шансы вымирания человечества вряд ли превысят 1 к 300 за столетие.[^:`С кратким изложением этой работы можно ознакомиться в лекции \"Dr Toby Ord - Will We Cause Our Own Extinction? Natural versus Anthropogenic Extinction Risks\", прочитанной в CSER в Кембридже в 2015 году. [Ссылка](https://www.youtube.com/watch?v=DCfLheUxHEI).`]",
    "articles--existential-risks:093": "К сожалению, как мы сейчас покажем, естественные риски ничтожны по сравнению с антропогенными. Именно поэтому риск вымирания стал особенно острой проблемой.",
    "articles--existential-risks:095": "## История прогресса, вплоть до начала самой опасной эпохи в истории человечества",
    "articles--existential-risks:097": "Если посмотреть на историю человечества в масштабе тысячелетий, то суть будет примерно такой: долгое время почти все были бедными, а затем, в 18-м веке, всё изменилось.[^:` График взят из Maddison, Angus (2007): \"Contours of the World Economy, 1-2030 AD. Essays in Macro-Economic History,\" Oxford University Press, ISBN 978-0-19-922721-1, стр. 379, таблица A.4.`]",
    "articles--existential-risks:099": "![Стремительный экономический рост создал условия, из-за которых мы сейчас сталкиваемся с антропогенными экзистенциальными угрозами](https://80000hours.org/wp-content/uploads/2017/10/something-weird-happened.jpg)[` `]",
    "articles--existential-risks:101": "Это было вызвано промышленной революцией — возможно, самым важным событием в истории.",
    "articles--existential-risks:103": "Росло не только богатство. Следующий график показывает, что в масштабе истории продолжительность жизни, энергопотребление и демократия быстро выросли, в то время как процент живущих в бедности резко сократился.[^:`_How big a deal was the Industrial Revolution?_, by Luke Muehlhauser, 2017, [Архивная ссылка](https://web.archive.org/web/20171022033906/http://lukemuehlhauser.com/industrial-revolution/), получена 21.10.2017.`]",
    "articles--existential-risks:105": "![](https://80000hours.org/wp-content/uploads/2017/10/luke1.jpg)[`График подготовлен Люком Мелхаузером в 2017 году.`]",
    "articles--existential-risks:107": "Уровни грамотности и образования также значительно выросли:",
    "articles--existential-risks:109": "![](https://80000hours.org/wp-content/uploads/2017/10/Literate-and-illiterate-world-population.jpg)[` `]",
    "articles--existential-risks:111": "Насколько мы знаем, с повышением уровня богатства люди также [становятся счастливее](https://80000hours.org/articles/money-and-happiness/).",
    "articles--existential-risks:113": "В книге _\"Лучшее в нас\"_ Стивен Пинкер утверждает, что уровень насилия снижается.[^:`Pinker, S., 2011. The better angels of our nature: The decline of violence in history and its causes. Penguin uk. [Web](https://www.amazon.com/Better-Angels-Our-Nature-Violence/dp/0143122010)`]",
    "articles--existential-risks:115": "Личных свобод стало больше, а уровни расизма, сексизма и гомофобии снизились.",
    "articles--existential-risks:117": "Многие люди считают, что мир становится хуже,[^:`Результаты разных опросов значительно отличаются в том, насколько люди пессимистичны в отношении будущего, но многие из опросов показывают, что большинство считает, что мир становится хуже. Например, недавний государственный опрос в Великобритании показал, что 71% респондентов придерживаются такого мнения.",
    "articles--existential-risks:119": "_Declinism: is the world actually getting worse?_, Pete Etchells, The Guardian, 2015, [Архивная ссылка](https://web.archive.org/web/20171017102431/https://www.theguardian.com/science/head-quarters/2015/jan/16/declinism-is-the-world-actually-getting-worse), получена 17.10.2017`] и в этом есть своя правда — в современной цивилизации существуют ужасные вещи вроде промышленного животноводства. Но, как видно из приведённых данных, многие важные показатели прогресса значительно улучшились.",
    "articles--existential-risks:121": "Так или иначе, что бы вы ни думали про прошлое, если смотреть в будущее, то улучшение технологий, политической организации и свободы общества сможет дать нашим потомкам возможность решить наши текущие проблемы и жить намного лучше.[^:`Становится ли мир лучше?",
    "articles--existential-risks:123": "Хотя есть причины считать, что большинство показателей прогресса растут (как показано в статье), есть также некоторые аспекты жизни, которые могли стать хуже. Например, в книге [_Sapiens_](https://www.amazon.com/Sapiens-Humankind-Yuval-Noah-Harari/dp/0062316095) Юваль Харари утверждает, что в современную эпоху усилились проблемы одиночества и психического здоровья, в то время как ощущения значимости и смысла могли снизиться. Мы скептически относимся к тому, что эти минусы могут перевешивать плюсы, но сложно сказать наверняка.",
    "articles--existential-risks:125": "Более сильные аргументы в пользу того, что мир становится хуже, возникают в контексте нашего воздействия на животных. В частности, с 1960-х годов резко выросло промышленное животноводство, и сейчас [где-то 30+ миллиардов животных](https://80000hours.org/problem-profiles/factory-farming/) ежегодно живут в ужасных условиях на промышленных фермах. Если нам небезразличны страдания этих животных, то это может перевесить наши успехи в области человеческого благополучия.",
    "articles--existential-risks:127": "Учитывая все эти аргументы, мы не можем однозначно утверждать, что суммарное благополучие возросло. Однако более важным является вопрос о том, что нас ждет в будущем.",
    "articles--existential-risks:129": "Станет ли мир лучше?",
    "articles--existential-risks:131": "Мы считаем, что до тех пор, пока человечество существует, развитие технологий и моральный прогресс дают нам возможности справиться с самыми серьёзными социальными проблемами, а также жить гораздо лучше в будущем. Если не считать экзистенциальные угрозы, то многие конкретные глобальные проблемы могут быть решены, если уровни богатства, технологического развития, а также морального и политического прогресса будут и дальше повышаться.",
    "articles--existential-risks:133": "Например, в случае с промышленным животноводством мы ожидаем, что по мере того, как люди будут становиться богаче, проблема будет уменьшаться. Во-первых, богатые люди более склонны к этичному потреблению, потому что в большей степени могут себе это позволить. Во-вторых, технологии способны положить конец промышленному животноводству при помощи заменителей мяса, искусственно выращенного мяса или более гуманных методов ведения сельского хозяйства. В-третьих, моральная значимость других живых существ возросла с течением времени (\"расширяющийся круг заботы\"), поэтому мы ожидаем, что в будущем люди будут ещё больше заботиться о благополучии животных.",
    "articles--existential-risks:135": "Если посмотреть ещё шире, то в конечном счёте мы ожидаем, что будущее будет лучше, потому что люди сами этого хотят. Чем больше технологической мощи и личных свобод мы имеем, тем проще людям реализовывать свои ценности. Поскольку люди _хотят_ жить хорошо, улучшение будущего — более вероятный сценарий, чем его ухудшение.",
    "articles--existential-risks:137": "При этом остаётся много вопросов. Например, многие наши ценности в какой-то степени противоречат друг другу, и это может привести к конфликтам. Вопросы о том, что ждёт нас в будущем, мало изучены. Поэтому хотя мы и ожидаем, что будущее будет лучше, чем настоящее, мы также признаём значительную степень неопределённости.`] Можно будет покончить с бедностью, предотвратить изменение климата, облегчить многие страдания — и этот список лишь продолжается.",
    "articles--existential-risks:139": "Но также обратите внимание на фиолетовую линию на втором графике: _военный потенциал_. Эти данные были взяты из оценок глобальной военной мощи историка Иэна Морриса. Как видите, эта линия тоже резко стремится вверх.",
    "articles--existential-risks:141": "Проблема заключается в следующем: развитие технологий может таить в себе как огромные выгоды, так и огромные риски.",
    "articles--existential-risks:143": "Когда мы открываем новые технологии, в большинстве случаев они приносят огромную пользу. Но есть также шанс, что эти технологии будут обладать настолько высоким разрушительным потенциалом, что мы не сможем обеспечить достаточно безопасные условия для их использования.",
    "articles--existential-risks:145": "И поэтому, хотя нынешнее поколение живет в самый благополучный период в истории человечества, он, возможно, также является самым опасным.",
    "articles--existential-risks:147": "Первой разрушительной технологией такого рода стало ядерное оружие.",
    "articles--existential-risks:149": "## История ядерного оружия: череда крайне опасных ситуаций",
    "articles--existential-risks:151": "Сегодня мы все думаем о ядерной программе Северной Кореи, но нынешние события — лишь одна глава в длинной истории крайне опасных ситуаций.",
    "articles--existential-risks:153": "Только во время Кубинского ракетного кризиса мы несколько раз были близки к ядерной войне.[^:` Больше об истории подобных случаев можно узнать в [нашем подкасте с Тоби Ордом.](https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/)`] В одном из случаев американцы решили, что если один из их самолетов-шпионов будет сбит, то они немедленно начнут вторжение на Кубу без дополнительного заседания Военного совета. На следующий день самолет-шпион был сбит. Кеннеди всё равно созвал совет и принял решение не вводить войска.",
    "articles--existential-risks:155": "Вторжение на Кубу вполне могло привести к ядерной войне; позже выяснилось, что Кастро выступал за ядерное возмездие, даже если \"это привело бы к полному уничтожению Кубы\". Некоторые из командиров пусковых установок на Кубе также имели независимые полномочия наносить удары по американским силам тактическим ядерным оружием в случае вторжения.",
    "articles--existential-risks:157": "В другом инциденте российская атомная подводная лодка пыталась тайком провезти материалы на Кубу, когда её обнаружил американский флот. Флот начал сбрасывать фиктивные глубинные бомбы, чтобы заставить подводную лодку всплыть. Русский капитан подумал, что это настоящие глубинные бомбы и что, пока не было радиосвязи, началась третья мировая война. Он приказал нанести ядерный удар по американскому флоту одной из ядерных торпед.",
    "articles--existential-risks:159": "К счастью, для такого приказа было необходимо согласие от других старших офицеров. Один их них, Василий Архипов, был против и таким образом предотвратил войну.",
    "articles--existential-risks:161": "![Благодаря Василию Архипову мы чудом предотвратили глобальный риск катастрофы от ядерного оружия](https://80000hours.org/wp-content/uploads/2017/10/VA.jpg)[`Спасибо вам, Василий Архипов.`]",
    "articles--existential-risks:163": "Сопоставив все эти события вместе, Кеннеди позже говорил, что вероятность ядерной войны была \"между 33-мя и 50-ю процентами\".[^:`> Пятьдесят лет назад Кубинский ракетный кризис поставил мир на грань ядерной катастрофы. Во время конфликта президент Джон Ф. Кеннеди считал, что вероятность начала войны была \"между 33-мя и 50-ю процентами\", и в последующие десятилетия ничто не дало нам причин оценить эту вероятность как более низкую. Такой конфликт мог бы привести к гибели 100 миллионов американцев и более 100 миллионов россиян.",
    "articles--existential-risks:165": "_At 50, the Cuban Missile Crisis as Guide_, Graham Allison, The New York Times, 2012,",
    "articles--existential-risks:167": "[Архивная ссылка](https://web.archive.org/web/20171017104052/http://www.nytimes.com/2012/06/16/opinion/at-50-the-cuban-missile-crisis-as-guide.html), получена 17.10.2017`]",
    "articles--existential-risks:169": "Подобных опасных столкновений с Россией было немало, даже после холодной войны: на Википедии [есть релевантный список](https://en.wikipedia.org/wiki/List_of_nuclear_close_calls). И это лишь те, о которых нам известно.",
    "articles--existential-risks:171": "Сегодня эксперты по ядерным вопросам обеспокоены напряжённостью между Индией и Пакистаном (обе страны обладают ядерным оружием) не меньше, чем Северной Кореей.[^:`Оценки шансов ядерного удара по гражданской цели указаны по ссылке ниже, на рисунке в разделе \"What is the probability that a nuclear bomb will be dropped on a civilian target in the next decade?” Обратите внимание, что один эксперт оценил шанс ядерного удара по гражданской цели в ближайшие 10 лет менее чем в 1%.",
    "articles--existential-risks:173": "Правда ли, что экспертов больше беспокоит индийско-пакистанский конфликт, чем Северная Корея?",
    "articles--existential-risks:175": "> Первое место в списке конфликтов, вызывающих опасения экспертов, занимает Индия-Пакистан. Оба государства разработали ядерное оружие вне юрисдикции Договора о нераспространении ядерного оружия, оба государства имеют ограниченные военные возможности, что может стать причиной раннего применения, и известно, что оба государства (хоть их публичные заявления и намеренно расплывчаты) имеют планы на случай чрезвычайных обстоятельств, предполагающие первые ядерные удары по военным целям.",
    "articles--existential-risks:177": "_We’re Edging Closer To Nuclear War_, Milo Beckman, FiveThirtyEight, 2017,",
    "articles--existential-risks:179": "[Архивная ссылка](https://web.archive.org/web/20171017104351/https://fivethirtyeight.com/features/were-edging-closer-to-nuclear-war/), получена 17.10.2017`]",
    "articles--existential-risks:181": "Главная проблема заключается в том, что несколько стран располагают крупными ядерными арсеналами и могут применить их в считанные минуты. Это означает, что ложная тревога или иная ошибка может быстро перерасти в полномасштабную ядерную войну, особенно в период напряженных международных отношений.",
    "articles--existential-risks:183": "Приведёт ли ядерная война к гибели цивилизации? Первоначально считалось, что ядерный взрыв может быть настолько горячим, что воспламенит атмосферу и сделает Землю непригодной для жизни. Но учёные решили, что такой исход достаточно маловероятен. Это позволило нам провести \"безопасные\" испытания ядерного оружия, и теперь мы знаем, что такого не случится.",
    "articles--existential-risks:185": "В 1980-х годах люди опасались, что пепел от горящих зданий погрузит Землю в очень долгую зиму, которая сделает невозможным выращивание сельскохозяйственных культур в течение десятилетий.[^:`Когда \"ядерная зима\" стала предметом беспокойства?",
    "articles--existential-risks:187": "> \"Ядерная зима\" и предшествующая ей концепция, \"ядерные сумерки\", относятся к ядерным событиям. Ядерная зима стала интересна науке в 1980-х годах, после того как стало ясно, что более ранняя гипотеза про разрушение озонового слоя из-за выбросов оксидов азота от огненных шаров начинает терять свою достоверность. Именно в этом контексте климатическое воздействие сажи от пожаров было \"случайно обнаружено\" и вскоре стало основной теорией в области климатических последствий ядерной войны. В этих модельных сценариях предполагалось, что различные облака, содержащие сажу в некотором количестве, образуются над городами, нефтеперерабатывающими заводами и ракетными шахтами в сельских районах. После того как исследователи задают количество сажи, моделируются климатические эффекты этих облаков. Термин «ядерная зима» был придуман в 1983 году Ричардом П. Турко в связи с одномерной компьютерной моделью, созданной для изучения идеи «ядерных сумерек». На основе этой модели был сделан вывод, что огромное количество сажи и дыма будет годами держаться в воздухе и станет причиной сильного падения температуры на всей планете. Турко позже дистанцировался от этих радикальных 1-D выводов.\"",
    "articles--existential-risks:189": "Статья \"Nuclear Winter\" на Википедии, [архивная ссылка](https://web.archive.org/web/20171030214107/https://en.wikipedia.org/wiki/Nuclear_winter), получена 30.10.2017.`] Современные климатические модели предполагают, что сильная ядерная зима, в результате которой все умрут, очень маловероятна (хотя из-за [неопределённости моделей](https://concepts.effectivealtruism.org/concepts/uncertainty-about-models/) мы не можем полностью доверять этим оценкам.[^:`Климатические модели содержат значительную неопределённость, и это значит, что реальные риски запросто могут быть выше. Более того, из-за самого факта наличия неопределённости в моделях сложно приписывать очень низкие вероятности большинству рисков. Об этом можно почитать в следующей работе:",
    "articles--existential-risks:191": "Ord, T., Hillerbrand, R., & Sandberg, A. (2010). Probing the improbable: methodological challenges for risks with low probabilities and high stakes. Journal of Risk Research, 13(2), стр. 191-205. arXiv:0810.5515v1, [ссылка](http://amirrorclear.net/files/probing-the-improbable.pdf).`])",
    "articles--existential-risks:193": "Однако даже \"умеренная\" ядерная зима всё равно может привести к массовому голоду.[^:`Ожидаемая тяжесть ядерной зимы всё ещё обсуждается, и Open Philantropy [недавно выделила средства](https://www.openphilanthropy.org/focus/global-catastrophic-risks/miscellaneous/rutgers-university-nuclear-conflict-climate-modeling) на дальнейшее исследование этой темы.`] По этой и другим причинам ядерная война была бы чрезвычайно дестабилизирующей для мира, и неизвестно, сможет ли цивилизация восстановиться после неё.",
    "articles--existential-risks:195": "Какова вероятность того, что ядерная война навсегда уничтожит цивилизацию? Это очень трудно оценить, но, скорее всего, вероятность такого события в следующем столетии как минимум превышает 0,3%. Если это так, то риски от ядерного оружия будут выше, чем все природные риски, вместе взятые. ([Подробнее о ядерных рисках](https://80000hours.org/problem-profiles/nuclear-security/).)",
    "articles--existential-risks:197": "Вот почему 1950-е годы стали началом новой эры для человечества. Впервые в истории у маленькой группы людей, принимающих решения, появилась возможность разрушить весь мир. Теперь самой большой угрозой для собственного выживания являемся мы сами, и это делает наше время самым опасным в истории человечества.",
    "articles--existential-risks:199": "И ядерное оружие — не единственный способ, при помощи которого мы можем уничтожить цивилизацию.",
    "articles--existential-risks:201": "## Насколько велики риски, связанные с изменением климата?",
    "articles--existential-risks:203": "В 2015 году президент Обама [заявил в своем обращении к Конгрессу](https://web.archive.org/web/20171017105713/http://edition.cnn.com/2015/01/21/us/climate-change-us-obama/index.html), что \"ни одна проблема не представляет столь большой угрозы для будущих поколений, как изменение климата\".",
    "articles--existential-risks:205": "Безусловно, изменение климата представляет собой серьёзный риск для цивилизации.",
    "articles--existential-risks:207": "Наиболее вероятный результат — 2-4 градуса потепления.[^:` См. Box SPM.1.1 в разделе B, [Summary for Policymakers](https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_SPM_final.pdf) of the Working Group I Contribution to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change.`] Это плохо, но такие изменения не ставят под угрозу выживание нашего вида.",
    "articles--existential-risks:209": "Однако [некоторые оценки](https://www.huffingtonpost.com/michael-e-mann/the-fat-tail-of-climate-change-risk_b_8116264.html) дают 10% вероятность потепления свыше 6 градусов и где-то 1% шанс потепления на 9 градусов.",
    "articles--existential-risks:211": "Таким образом, получается, что вероятность масштабной климатической катастрофы в результате выбросов CO2 примерно равна вероятности ядерной войны.",
    "articles--existential-risks:213": "Но, как мы утверждаем в нашем [обзоре проблемы изменения климата](https://80000hours.org/problem-profiles/climate-change/), даже потепление в 13 градусов вряд ли само по себе приведёт к вымиранию человечества. Поэтому исследователи, изучающие эти вопросы, считают, что ядерная война с большей вероятностью может непосредственно привести к вымиранию (из-за возможности ядерной зимы), и поэтому мы считаем, что ядерное оружие представляет собой ещё более серьёзный риск, чем изменение климата.",
    "articles--existential-risks:215": "Тем не менее, изменение климата является серьёзной проблемой, и его дестабилизирующие последствия могут усугубить другие риски, в том числе риск ядерного конфликта, что лишь повышает наши оценки рисков данной проблемы.",
    "articles--existential-risks:217": "## Какие новые технологии могут быть столь же опасными, как ядерное оружие?",
    "articles--existential-risks:219": "Изобретение ядерного оружия привело к возникновению антиядерного движения всего десятилетие спустя, а вскоре после этого движение защитников окружающей среды взялось за борьбу с изменением климата.",
    "articles--existential-risks:221": "Менее очевидный вывод заключается в том, что новые технологии будут создавать новые катастрофические риски. Вот почему нам необходимо движение, которое занимается защитой цивилизации в целом.",
    "articles--existential-risks:223": "Предсказывать будущее технологий сложно, но поскольку цивилизация у нас всего одна, нам нужно делать всё, что в наших силах. Вот несколько кандидатов на роль следующей технологии, потенциально столь же опасной, как ядерное оружие.",
    "articles--existential-risks:225": "В 1918-1919 годах от испанского гриппа умерло более 3% населения Земли.[^:`> По разным оценкам, от него погибло от 3% до 6% мирового населения.",
    "articles--existential-risks:227": "_World War One’s role in the worst ever flu pandemic_, John Mathews, The Conversation, 2014, [архивная ссылка](https://web.archive.org/web/20171027035359/https://theconversation.com/world-war-ones-role-in-the-worst-ever-flu-pandemic-29849), получена 27.10.2017.",
    "articles--existential-risks:229": "> При населении мира в 1811 миллионов человек, 30 миллионов смертей привели бы к уровню смертности в 16,6 человек на каждую тысячу. Это в три раза выше, чем в богатых странах, но вполне в пределах нормы для бедных стран. Если оценивать количество смертей от гриппа в 50-100 миллионов, то числа были бы в районе 27,6-55,2 смертей на тысячу.",
    "articles--existential-risks:231": "Patterson, K.D. and Pyle, G.F., 1991. The geography and mortality of the 1918 influenza pandemic. Bulletin of the History of Medicine, 65(1), p.4.",
    "articles--existential-risks:233": "[Архивная ссылка](https://web.archive.org/web/20171022101640/https://pida.nihlibrary.com/sites/pida.nihlibrary.com/files/pdf_files/1991_K.David%20Patterson_The%20geography%20and%20mortality%20of%20the%201918%20influenza%20pandemic..pdf), получена 22.10.2017.",
    "articles--existential-risks:235": "> Дальнейшие исследования на тему пандемии испанского гриппа приводили к регулярному пересмотру значений предполагаемой глобальной смертности в сторону её увеличения. Согласно изначальным расчётам 1920-х годов, смертность оценивалась примерно в 21,5 миллиона человек. В работе 1991 года смертность была пересчитана, с новым значением в диапазоне 24,7-39,3 миллиона человек. В данной работе предполагается, что она была порядка 50 миллионов. Однако следует признать, что даже эта огромная цифра может быть значительно ниже реальной — вплоть до разницы в 100%.",
    "articles--existential-risks:237": "Johnson, N.P. and Mueller, J., 2002. Updating the accounts: global mortality of the 1918-1920\" Spanish\" influenza pandemic. Bulletin of the History of Medicine, 76(1), стр. 105-115.",
    "articles--existential-risks:239": "[Ссылка](https://muse.jhu.edu/article/4826/summary)`] Если бы такая пандемия вспыхнула сегодня, то её было бы ещё труднее сдерживать из-за скоростного глобального транспорта.",
    "articles--existential-risks:241": "Однако больше тревоги вызывает то, что вскоре может появиться возможность генетически сконструировать вирус, который будет таким же заразным, как испанский грипп, но при этом более смертоносным, и который сможет годами распространяться незамеченным.",
    "articles--existential-risks:243": "Это было бы оружие с разрушительной мощью ядерного, но чьё применение гораздо труднее предотвратить. Ядерное оружие требует огромных заводов и редких материалов для производства, что делает его относительно легким для контроля. Искусственные вирусы же можно потенциально создать в лаборатории с парой докторов биологических наук. Более того, в 2006 году The Guardian смогли заказать по почте сегменты вымершего вируса оспы.[^:`_Revealed: the lax laws that could allow assembly of deadly virus DNA: Urgent calls for regulation after Guardian buys part of smallpox genome through mail order_, The Guardian, 2006,",
    "articles--existential-risks:245": "[Архивная ссылка](https://web.archive.org/web/20171022042133/https://www.theguardian.com/world/2006/jun/14/terrorism.topstories3), получена 21.10.2017.`] Некоторые террористические группировки уже заявляли о своём интересе к подобному оружию неизбирательного действия. ([Подробнее о рисках пандемий](https://80000hours.org/problem-profiles/biosecurity/).)",
    "articles--existential-risks:247": "![В 2006 году The Guardian смогли заказать по почте сегменты вымершего вируса оспы. Эксперты предполагают, что синтетические патогены потенциально могут представлять глобальный катастрофический риск.](https://80000hours.org/wp-content/uploads/2017/10/smallpox.png)[`Кто заказывал оспу? Источник: The Guardian`]",
    "articles--existential-risks:249": "Ещё одна новая технология с огромной потенциальной мощью — искусственный интеллект.",
    "articles--existential-risks:251": "Причина, по которой на планете стали главными люди, а не шимпанзе, кроется исключительно в интеллекте. Размеры и мощь наших мозгов позволили нам установить поразительный контроль над миром, хотя физически мы намного слабее шимпанзе.",
    "articles--existential-risks:253": "Что же тогда произойдёт, если однажды мы создадим нечто гораздо более разумное, чем мы сами?",
    "articles--existential-risks:255": "В 2017 году 350 исследователей, публиковавших свои рецензируемые статьи на тему ИИ на ведущих конференциях, были опрошены о том, когда, по их мнению, будет разработан компьютер с человеческим уровнем интеллекта: то есть машина, способная выполнять все рабочие задачи лучше, чем человек.",
    "articles--existential-risks:257": "Медианной оценкой была 50% вероятность того, что мы разработаем высокоуровневый машинный интеллект (ВМИ) в течение 45 лет, и 75% — что это произойдёт к концу века.[^:`> Исследователи считают, что существует 50% вероятность того, что ИИ превзойдет человека во всех задачах через 45 лет.\n>\n> Респондентов спросили, какое влияние ВМИ окажет на человечество в долгосрочной перспективе: положительное или отрицальное. Им нужно было оценить вероятности различных исходов по пятибальной шкале. Медианными оценками были 25% для \"хорошего исхода\" и 20% для \"чрезвычайно хорошего исхода\". В свою очередь, медианная оценка для \"плохого\" исхода была 10%, и 5% для исхода, сформулированного как \"чрезвычайно плохой, вроде вымирания человечества\".",
    "articles--existential-risks:261": "Grace, K., Salvatier, J., Dafoe, A., Zhang, B. and Evans, O., 2017. When Will AI Exceed Human Performance? Evidence from AI Experts. arXiv preprint arXiv:1705.08807.",
    "articles--existential-risks:263": "[Ссылка](https://arxiv.org/abs/1705.08807)`]",
    "articles--existential-risks:265": "!img~ class=\"alignnone size-full wp-image-40209\" width=\"602\" height=\"389\" ~[График прогнозов экспертов, Grace et al: медианной оценкой был 50% шанс того, что мы разработаем высокоуровневый машинный интеллект в течение 45 лет](https://80000hours.org/wp-content/uploads/2017/10/probability-of-HLMI.png)",
    "articles--existential-risks:267": "Подобные вероятности сложно оценивать, и исследователи давали очень разные цифры в зависимости от конкретных формулировок вопроса.[^:`Если вам интересно почитать обсуждение подобной непоследовательности в оценках, есть пост от AI Impacts, \"Some Survey Results,\" [архивная ссылка](https://web.archive.org/web/20171030220008/https://aiimpacts.org/some-survey-results/), получена 30.10.2017. К примеру:",
    "articles--existential-risks:269": "> Вопросы, связанные с конкретными видами работы, очень сильно влияют на прогнозы касательно ВМИ. Когда мы задавали некоторым людям вопросы про то, когда ИИ начнёт успешно выполнять несколько конкретных видов деятельности, а потом и про все виды работ, которыми занимаются люди (что считается подвидом всех возможных задач вообще), мы получали сильно более поздние сроки, чем когда мы просто спрашивали про ВМИ в целом. Если при этом мы просили назвать вероятности для конкретных сроков, то для срока \"через 20 лет\" цифры отличались в 1000 раз! (10% для общего вопроса про сильный ИИ против 0,01% для конкретных видов деятельности.) Если же мы просили назвать сроки для конкретных значений вероятности, общий вопрос получал оценку \"через 40 лет\" для 50% вероятности, а формулировки про конкретные виды деятельности получали ответ \"через 90 лет\" для тех же 50%.\n>\n> Люди регулярно дают более поздние прогнозы, если спрашивать их про \"вероятность через N лет\", чем если спрашивать про \"год, в который вероятность будет M\". Мы наблюдали это в контексте общего вопроса про ВМИ и вопросов про конкретные виды деятельности, а также в большинстве случаев, когда мы тестировали эти вопросы на респондентах в MTurk. Например, для общего вопроса про ВМИ, если спрашивать, когда вероятность создания ВМИ достигнет 50%, то медианным ответом будет \"через 40 лет\", но если спрашивать, какова вероятность создания ВМИ через 40 лет, медианный ответ будет \"30%\".`] Тем не менее, есть причины полагать, что существует как минимум резонная вероятность создания революционного машинного интеллекта в ближайшие сто лет. Более того, из высокой степени неопределённости следует, что это может случиться раньше, чем мы полагаем.",
    "articles--existential-risks:273": "Какие риски может представлять такая технология? Первопроходцы в области вычислительной техники, такие как Алан Тьюринг и Марвин Мински, высказывали опасения по поводу рисков, связанных с мощными компьютерными системами,[^:`> Некоторые заявления таких ученых, как Алан Тьюринг, Ирвинг Гуд и Марвин Мински, указывали на философские опасения насчёт того, что сверхинтеллект может захватить контроль.",
    "articles--existential-risks:275": "См. сноски 15-18 в статье _Existential risk from artificial intelligence_ на Википедии, [архивная ссылка](https://web.archive.org/web/20171022041500/https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence), получена 21.10.2018.`] и эти риски до сих пор актуальны. Мы говорим не о \"злых\" компьютерах, а, скорее, о том, что мощная ИИ-система может быть использована какой-то группой людей для установления контроля над миром или как-то иначе задействована в плохих целях. Если бы СССР разработал ядерное оружие на 10 лет раньше, чем США, то СССР мог бы стать доминирующей мировой державой. Мощные компьютерные технологии могут представлять аналогичные риски.",
    "articles--existential-risks:277": "Другая проблема заключается в том, что запуск такой системы может привести к непредвиденным последствиям, поскольку сложно предсказать действия чего-то более умного, чем мы сами. Достаточно мощной системой также может быть сложно управлять, и поэтому её запуск может оказаться труднообратимым. Подобные сценарии были описаны оксфордским профессором Ником Бостромом в книге [\"Искусственный интеллект\"](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742) и пионером ИИ [Стюартом Расселом](https://www.ted.com/talks/stuart_russell_how_ai_might_make_us_better_people).",
    "articles--existential-risks:279": "Большинство экспертов считают, что совершенствование ИИ приведёт к чрезвычайно положительным результатам, но при этом также не забывают о рисках. В вышеупомянутом опросе эксперты в области ИИ оценили, что разработка высокоуровневого машинного интеллекта с 10% шансом может привести к \"плохому исходу\" и с 5% шансом к \"чрезвычайно плохому исходу\", такому как вымирание человечества.[^:`> Исследователи считают, что существует 50% вероятность того, что ИИ превзойдет человека во всех задачах через 45 лет.\n>\n> Респондентов спросили, какое влияние ВМИ окажет на человечество в долгосрочной перспективе: положительное или отрицательное. Они оценивали вероятности различных исходов по пятибалльной шкале. Медианными оценками были 25% для \"хорошего исхода\" и 20% для \"чрезвычайно хорошего исхода\". В свою очередь, медианная оценка для \"плохого\" исхода была 10%, и 5% для исхода, сформулированного как \"чрезвычайно плохой (например, вымирание человечества)\".",
    "articles--existential-risks:283": "Grace, K., Salvatier, J., Dafoe, A., Zhang, B. and Evans, O., 2017. When Will AI Exceed Human Performance? Evidence from AI Experts. arXiv preprint arXiv:1705.08807.",
    "articles--existential-risks:285": "[Ссылка](https://arxiv.org/abs/1705.08807)`] Причём нам, вероятно, следует ожидать излишней оптимистичности от этой группы людей, поскольку они зарабатывают на жизнь благодаря этой технологии.",
    "articles--existential-risks:287": "Резюмируя все полученные оценки, мы получим, что при 75% шансе создания высокоуровневого машинного интеллекта в течение следующих ста лет, вероятность крупной катастрофы, связанной с ИИ, составляет 5% от этих 75%, то есть в итоге около 4%. ([Подробнее о рисках, связанных с искусственным интеллектом](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/).)",
    "articles--existential-risks:289": "Люди выражали беспокойство и по поводу других новых технологий, таких как некоторые формы геоинженерии и атомного производства, но их изобретение кажется значительно менее \"близким\" и поэтому они в целом считаются менее опасными, чем другие технологии, о которых мы говорили. Более длинный список экзистенциальных рисков можно посмотреть [здесь](https://nickbostrom.com/existential/risks.html).",
    "articles--existential-risks:291": "Что вызывает больше беспокойств, так это риски, о которых мы ещё даже не знаем. Если бы вы спросили людей в 1900-м году, что представляет наибольшую угрозу для цивилизации, они вряд ли бы упомянули атомное оружие, генную инженерию или искусственный интеллект, потому что ничто из этого ещё не было изобретено. Вполне возможно, что мы находимся в аналогичной ситуации, если говорить о следующем веке. Грядущие \"неизвестные неизвестные\" могут представлять больше опасности, чем уже известные нам риски.",
    "articles--existential-risks:293": "Каждый раз, когда мы открываем новую технологию, это немного похоже на ставку _против_ одного числа в рулетке. В большинстве случаев мы выигрываем, и технология в целом имеет хорошие последствия. Но каждый раз есть небольшой шанс, что технология будет обладать настолько большим разрушительным потенциалом, что мы не сможем с ней совладать, и тогда мы потеряем всё.",
    "articles--existential-risks:295": "![](https://80000hours.org/wp-content/uploads/2017/10/roulette.jpg)[`У каждой новой технологии беспрецедентными являются и потенциал, и опасности.`]",
    "articles--existential-risks:297": "### Каков суммарный риск вымирания человечества, если учесть все эти факторы?",
    "articles--existential-risks:299": "По оценкам многих экспертов, изучающих эти вопросы, суммарная вероятность вымирания человечества в следующем столетии составляет от 1 до 20%.",
    "articles--existential-risks:301": "Например, согласно неофициальному опросу, проведенному в 2008 году на конференции по катастрофическим рискам, эксперты считают довольно вероятным сценарий катастрофы, в результате которой погибнет более миллиарда человек, и оценивают вероятность вымирания человечества до 2100 года в 19%.[^:`Sandberg, A. & Bostrom, N. (2008): “Global Catastrophic Risks Survey”, Technical",
    "articles--existential-risks:303": "Report #2008-1, Future of Humanity Institute, Oxford University: стр. 1-5.",
    "articles--existential-risks:305": "[Ссылка](https://www.fhi.ox.ac.uk/reports/2008-1.pdf)`]",
    "articles--existential-risks:307": "<div class=\"tablepress-scroll-wrapper\">",
    "articles--existential-risks:309": "| Риск | Минимум миллиард смертей | Вымирание человечества |\n| --- | --- | --- |\n| Количество смертей от молекулярного нанооружия | 10% | 5% |\n| Суммарное количество смертей из-за сильного ИИ | 5% | 5% |\n| Суммарное количество смертей из-за всех войн (включая гражданские) | 30% | 4% |\n| Количество смертей из-за самой большой искусственной пандемии | 10% | 2% |\n| Суммарное количество смертей из-за всех атомных конфликтов | 10% | 1% |\n| Количество смертей из-за самой большой нанотехнологической аварии | 1% | 0.5% |\n| Количество смертей из-за самой большой естественной пандемии | 5% | 0.05% |\n| Суммарное количество смертей из-за всех террористических актов с применением атомного оружия | 1% | 0.03% |\n| Общий риск вымирания до 2100 года | n/a | 19% |",
    "articles--existential-risks:321": "</div>",
    "articles--existential-risks:323": "Эти цифры примерно в миллион раз превышают оценки большинства людей.",
    "articles--existential-risks:325": "В [нашем подкасте с Уиллом Макаскиллом](https://80000hours.org/podcast/episodes/will-macaskill-paralysis-and-hinge-of-history/) мы обсуждаем, почему он оценивает риск вымирания в этом веке примерно в 1%.",
    "articles--existential-risks:327": "В своей книге [\"Пропасть: Экзистенциальный риск и будущее человечества\"](https://80000hours.org/the-precipice/) Тоби Орд предполагает, что общий экзистенциальный риск в этом веке составляет 1 к 5, что равноценно броску игральной кости. [Послушать наш подкаст с Тоби можно тут.](https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/)",
    "articles--existential-risks:329": "Какие выводы стоит делать из этих оценок? Предположительно, исследователи работают над этими вопросами в первую очередь потому что считают их очень важными, поэтому мы должны ожидать, что их оценки будут высокими (см. [\"систематическая ошибка отбора\"](https://en.wikipedia.org/wiki/Selection_bias)). Но значит ли это, что мы можем полностью игнорировать их опасения?",
    "articles--existential-risks:331": "С учётом всего этого, какова наша собственная оценка? Это очень сложный вопрос, но нам кажется, что нельзя с уверенностью игнорировать эти риски. В целом, мы полагаем, что общий риск, скорее всего, превышает 3%.",
    "articles--existential-risks:333": "## Почему участие в защите будущего может быть самым важным делом вашей жизни",
    "articles--existential-risks:335": "Насколько приоритетной должна быть работа по снижению этих рисков по сравнению с другими вопросами, такими как глобальная бедность, борьба с раком или улучшение политической системы?",
    "articles--existential-risks:337": "В рамках проекта \"80 000 часов\" мы проводим исследования, цель которых — помогать людям с поиском работы, которая приносит пользу обществу. Для этого мы стараемся находить наиболее актуальные проблемы в мире, над которыми необходимо работать. Мы оцениваем различные глобальные проблемы, используя нашу [модель](/articles/problem-framework/), которая сравнивает их по следующим критериям:",
    "articles--existential-risks:339": "- Масштаб — сколько людей затронуто этой проблемой\n- Недооценённость — сколько людей уже работают над ней\n- Разрешимость — насколько легко продвинуться в её решении",
    "articles--existential-risks:343": "Мы считаем, что если придерживаться этой модели, то защита будущего становится самым главным приоритетом общества. Поэтому если вы хотите принести много пользы миру при помощи своей работы, в первую очередь стоит сфокусироваться на этой области.",
    "articles--existential-risks:345": "В следующих нескольких разделах мы рассмотрим эту проблему с точки зрения масштаба, недооценённости и разрешимости, в значительной степени опираясь на _\"Existential Risk Prevention as a Global Priority\"_ Ника Бострома и [неопубликованную работу Тоби Орда](https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/), а также на наши собственные исследования.",
    "articles--existential-risks:347": "Давайте начнём с масштаба проблемы. Мы утверждали, что вероятность вымирания в следующем столетии составляет более 3%. Насколько это серьёзно?",
    "articles--existential-risks:349": "Одно из чисел, на которые мы можем взглянуть, — это количество людей, которые могут погибнуть в результате такой катастрофы. Население планеты в середине этого века будет составлять примерно 10 миллиардов человек, поэтому 3% шанс полного вымирания даёт математическое ожидание где-то в 300 миллионов смертей. Судя по всему, это больше смертей, чем можно ожидать за весь следующий век от \"болезней бедности\" вроде малярии.[^:`Каждый год миллионы людей умирают от легко предотвратимых болезней, таких как малярия и диарея. Но поскольку эти числа стремительно падают, мы не ожидаем, что они превысят 300 миллионов за следующее столетие.",
    "articles--existential-risks:351": "> Ежегодная смертность от малярии сократилась с 3,8 миллионов до 0,7 миллиона человек\n>\n> Ежегодная смертность от диареи сократилась с 4,6 миллионов до 1,6 миллионов человек",
    "articles--existential-risks:355": "_Aid Works (On Average)_, Dr Toby Ord, Giving What We Can, [Ссылка](http://studylib.net/doc/13259236/aid-works--on-average--toby-ord-president--giving-what-we)`]",
    "articles--existential-risks:357": "Помимо гибели цивилизации, многие из рассмотренных нами рисков также могут привести к катастрофе \"средней\" тяжести, и это может быть намного более вероятным. Опрос, о котором мы рассказывали ранее, показал более 10% шанса катастрофы, из-за которой в следующем веке может погибнуть более 1 миллиарда людей. Это даёт нам математическое ожидание как минимум ещё в 100 миллионов смертей, а также намного больше страданий среди тех, кто выживет.",
    "articles--existential-risks:359": "Таким образом, даже если мы сосредоточимся только на последствиях для ныне живущего поколения, то эти катастрофические риски являются одной из самых серьёзных проблем, стоящих перед человечеством.",
    "articles--existential-risks:361": "Но в этом случае масштабы проблемы будут очень сильно недооценены, потому что если нашей цивилизации придёт конец, то мы полностью теряем и наше будущее.",
    "articles--existential-risks:363": "Большинство людей хотят оставить лучший мир для своих внуков, и многие также считают, что нам нужно в какой-то мере заботиться и о [будущих поколениях](/future-generations/) в целом. В будущем может быть намного больше людей, довольных своей жизнью, чем всех людей, живущих сейчас, и [нам нужно в какой-то степени учитывать их интересы](https://80000hours.org/articles/future-generations/). Есть шанс, что человеческая цивилизация просуществует миллионы лет, поэтому если учитывать последствия рисков для всех будущих поколений, то ставки увеличиваются в миллионы раз — как для хороших исходов, так и для плохих. [Карл Саган писал](http://www.jstor.org/stable/20041818) о цене ядерной войны в журнале _Foreign Affairs_:",
    "articles--existential-risks:365": "> Ядерная война ставит под угрозу жизни всех наших потомков на протяжении всего времени потенциального существования человечества. Даже если наше население перестанет расти, а средняя продолжительность жизни будет порядка 100 лет, то в условиях типичного периода времени, необходимого для цикла биологической эволюции успешного вида (примерно 10 миллионов лет), речь идёт о 500 триллионах человек, которые ещё не родились. С учётом этого, исход полного вымирания будет в миллион раз хуже, чем более скромные ядерные войны, которые убивают \"всего лишь\" сотни миллионов людей. Есть также много других возможных способов измерять потенциальные потери, включая культуру и науку, эволюционную историю планеты и значимость жизней всех наших предков, сделавших вклад в будущее своих потомков. Вымирание сотрёт всё, что построило человечество.",
    "articles--existential-risks:367": "Мы рады, что римляне не позволили человечеству исчезнуть, ведь благодаря этому существует вся современная цивилизация. Мы считаем, что на нас лежит аналогичная ответственность за людей будущего, если предполагать, что их жизни скорее будут хорошими (а мы думаем, что будут). Было бы необдуманно и несправедливо подвергать их жизни опасности лишь для того, чтобы сделать себе получше в краткосрочной перспективе.",
    "articles--existential-risks:369": "Дело не только в том, что в будущем может быть больше людей. Как отмечал и Саган, вне зависимости от того, что вы цените, [в будущем этого может быть гораздо больше](https://80000hours.org/articles/future-generations). Цивилизация будущего может построить мир без нужды или добиться потрясающих интеллектуальных и художественных успехов. Мы можем построить намного более справедливое и нравственное общество. И в принципе ничто не мешает нашей цивилизации достичь других планет, которых в нашей галактике около 100 миллиардов.[^:`_The Milky Way Contains at Least 100 Billion Planets According to Survey_, Hubblesite",
    "articles--existential-risks:371": "[Архивная ссылка](https://web.archive.org/web/20140723213047/http://hubblesite.org/newscenter/archive/releases/2012/07/full/), получена 22.10.2017`] Если мы позволим человечеству погибнуть, то все эти возможности будут навсегда перечёркнуты.",
    "articles--existential-risks:373": "Мы не уверены, что это замечательное будущее действительно наступит, но это лишь добавляет причин сохранить цивилизацию, ведь тогда у нас будет шанс узнать, как всё обернётся. Пожалуй, худшее, что мы можем сделать — это не передать эстафету следующему поколению.",
    "articles--existential-risks:375": "Таким образом, пара процентов риска уничтожения цивилизации — это, по всей видимости, самая серьёзная проблема человечества на данный момент. Поражает также то, насколько этими рисками пренебрегают.",
    "articles--existential-risks:377": "## Почему эти риски являются одними из самых недооценённых глобальных проблем",
    "articles--existential-risks:379": "Вот сколько денег ежегодно тратится на некоторые из важных целей:[^:`> Глобальные валовые расходы на исследования и разработки в 2013 году составили 1,48 триллиона долларов по ППС (паритету покупательной способности).",
    "articles--existential-risks:381": "_Facts and figures: R&D expenditure_, UNESCO,",
    "articles--existential-risks:383": "[Архивная ссылка](https://web.archive.org/web/20171020233546/https://en.unesco.org/node/252279), получена 21.10.2017",
    "articles--existential-risks:385": "> В целом индустрия демонстрирует устойчивый рост в 4% — в 2016 году объем розничных продаж оценивался в 1,08 триллиона евро.",
    "articles--existential-risks:387": "_Luxury Goods Worldwide Market Study, Fall-Winter 2016_, Claudia D’Arpizio, Federica Levato, Daniele Zito, Marc-André Kamel and Joëlle de Montgolfier, Bain & Company,",
    "articles--existential-risks:389": "[Архивная ссылка](https://web.archive.org/web/20171020234247/http://www.bain.com/publications/articles/luxury-goods-worldwide-market-study-fall-winter-2016.aspx), получена 21.10.2017",
    "articles--existential-risks:391": "> По данным Heritage Foundation, самая точная оценка стоимости 185 федеральных программ социального обеспечения (включающих проверку на нуждаемость) на 2010 год составляет почти 700 миллиардов долларов, что на треть больше, чем в 2008 году — и это только расходы федерального правительства. С учетом расходов штатов, общие расходы на социальное обеспечение в 2010 году достигли почти 900 миллиардов долларов, увеличившись почти на четверть с 2008 года (24,3%).",
    "articles--existential-risks:393": "_America's Ever Expanding Welfare Empire_, Peter Ferrara, Forbes,",
    "articles--existential-risks:395": "[Архивная ссылка](https://web.archive.org/web/20160126160330/http://www.forbes.com/sites/peterferrara/2011/04/22/americas-ever-expanding-welfare-empire/#48b5521855a6), получена 02.03.2016",
    "articles--existential-risks:397": "> После стабилизации в 2012 году и снижения в 2013 году, в 2014-ом общемировой объем климатического финансирования увеличился на 18% — с $331 млрд до $391 млрд, согласно текущим оценкам.",
    "articles--existential-risks:399": "_Global Landscape of Climate Finance 2015_, Climate Policy Initiative, 2015,",
    "articles--existential-risks:401": "[Архивная ссылка](https://web.archive.org/web/20171027031731/http://climatepolicyinitiative.org/wp-content/uploads/2015/11/Global-Landscape-of-Climate-Finance-2015.pdf), получена 27.10.2017. Вы также можете прочитать наш [обзор проблемы изменения климата](https://80000hours.org/problem-profiles/climate-change/), там есть больше данных.",
    "articles--existential-risks:403": "Для глобальной бедности, даже лишь в рамках глобального здравоохранения:",
    "articles--existential-risks:405": "> Наименее развитые страны (плюс Индия) ежегодно тратят на здравоохранение около 300 миллиардов долларов (ППС).",
    "articles--existential-risks:407": "_Здоровье в бедных странах_, наш [обзор проблемы](https://80000hours.org/problem-profiles/health-in-poor-countries/)",
    "articles--existential-risks:409": "Если более широко рассматривать ресурсы, поступающие к бедным слоям населения мира, то нам стоит учитывать все расходы на помощь, денежные переводы, а также их собственные доходы. В мире насчитывается более 700 миллионов человек, живущих за чертой бедности (т.е. с доходом ниже $1,75 в день). Если считать, что их средний доход равен $1 в день, то это составит $256 млрд суммарного дохода в год.",
    "articles--existential-risks:411": "> Ресурсы, глобально выделяемые на предотвращение рисков ядерной войны, с учётом всех государственных и негосударственных источников, скорее всего составляют $10 млрд в год или выше. Однако мы снижаем эту цифру до $1-10 млрд в год с поправкой на качество, потому что большая часть этих расходов направлена не на снижение риска применения ядерного оружия в целом, а на защиту какой-то одной страны или предоставление одной стране преимущества над другой. Много средств также тратится на меры по нераспространению, а они не связаны с наиболее опасными сценариями, в которых используются сотни боеголовок.",
    "articles--existential-risks:413": "_Ядерная безопасность_, наш [обзор проблемы](https://80000hours.org/problem-profiles/nuclear-security/)",
    "articles--existential-risks:415": "Что касается профилактики пандемий, то здесь сложно делать точные оценки, поскольку многие расходы имеют к этому косвенное отношение (например, больницы тоже снижают риски пандемий). Если оценивать достаточно широко, то $10+ млрд в год будет вполне разумной цифрой. Если оценивать более узко, фокусируясь на более прицельных мерах, то может получиться где-то $1 млрд в год. Если же говорить именно о целенаправленных усилиях по снижению экзистенциальных рисков от искусственных пандемий, то существует лишь несколько экспертов, специализирующихся на этой теме. Подробнее в нашей статье: _Биобезопасность_, [обзор проблемы](https://80000hours.org/problem-profiles/biosecurity/)",
    "articles--existential-risks:417": "> Глобальные расходы на исследования и другую деятельность, связанную с безопасностью разработки машинного интеллекта, составят всего 9 миллионов долларов в 2017 году.",
    "articles--existential-risks:419": "_Позитивное влияние на разработку искусственного интеллекта_, наш [обзор проблемы](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/).`]",
    "articles--existential-risks:421": "<div class=\"tablepress-scroll-wrapper\">",
    "articles--existential-risks:423": "| Цель | Ежегодные целевые расходы из всех источников (очень приблизительно) |\n| --- | --- |\n| Глобальные исследования и разработки | $1.5 триллиона |\n| Предметы роскоши | $1.3 триллиона |\n| Социальное обеспечение в США | $900 миллиардов |\n| Изменение климата | >$300 миллиардов |\n| Помощь бедным слоям населения мира | >$250 миллиардов |\n| Ядерная безопасность | $1-10 миллиардов |\n| Предотвращение экстремальных пандемий | $1 миллиард |\n| Исследования в области безопасности ИИ | $10 миллионов |",
    "articles--existential-risks:434": "</div>",
    "articles--existential-risks:436": "Как видите, мы тратим огромное количество ресурсов на разработку ещё более мощных технологий. Мы также тратим много средств на предметы роскоши в попытке улучшить свои жизни (что, возможно, не так уж и хорошо работает).",
    "articles--existential-risks:438": "Гораздо меньше средств уходит на снижение катастрофических рисков, связанных с изменением климата. Только лишь в США расходы на социальное обеспечение затмевают расходы на борьбу с изменением климата _во всём мире_.",
    "articles--existential-risks:440": "Но на проблему изменения климата всё равно выделяются огромные суммы денег по сравнению с некоторыми другими рисками, о которых мы говорили. По нашим приблизительным оценкам, на предотвращение экстремальных глобальных пандемий тратится в 300 раз меньше средств, хотя масштаб риска выглядит примерно таким же.",
    "articles--existential-risks:442": "Исследованиям в области безопасности ИИ уделяется меньше всего внимания, и на них выделяется ещё где-то в 100 раз меньше средств, чем на пандемии: [около 10 млн долларов в год](https://www.centreforeffectivealtruism.org/blog/changes-in-funding-in-the-ai-safety-field/).",
    "articles--existential-risks:444": "Аналогичную картину можно увидеть, если вместо выделенных денег посмотреть на _количество людей_, работающих над этими рисками, но для денег проще получить цифры.",
    "articles--existential-risks:446": "Если мы посмотрим на внимание научного сообщества к различным вопросам, то и там увидим похожую картину (хотя некоторым отдельным рискам уделяется значительное внимание — например, изменению климата):",
    "articles--existential-risks:448": "![Исследования экзистенциальных рисков получают меньше финансирования, чем исследования навозных жуков.](https://80000hours.org/wp-content/uploads/2017/10/Publications-by-topicArtboard-2-1.jpg)[`Источник: Ник Бостром`]",
    "articles--existential-risks:450": "Насколько мы понимаем, если посмотреть на распределение внимания в политике, то всё будет примерно так же, как и с финансированием. Подавляющее количество политического внимания уделяется конкретным вопросам, нацеленным на краткосрочную помощь ныне живущим поколениям, поскольку именно это приносит голоса. Катастрофические риски получают намного меньше внимания: из них в первую очередь говорят об изменении климата, в то время как о проблемах вроде пандемий и искусственного интеллекта говорят меньше всего.",
    "articles--existential-risks:452": "Такая нехватка ресурсов, а также научного и политического внимания — это именно то, чего следовало бы ожидать из текущих экономических предпосылок, а также то, почему область этих рисков является отличной возможностью для тех, кто хочет сделать мир лучше.",
    "articles--existential-risks:454": "Во-первых, ответственность за эти риски не лежит на каком-либо одном государстве. Предположим, США инвестируют значительные средства в предотвращение изменения климата. Это будет выгодно всему миру, но в США проживает только около 5% населения планеты, поэтому граждане США получат только 5% выгоды от этих расходов. Поэтому США будет значительно занижать свои инвестиции в эту область относительно её реальной значимости для всего мира. И то же самое можно сказать обо всех других странах.",
    "articles--existential-risks:456": "Эту проблему можно решить, если все _скоординируются_. Если каждая страна согласится внести соответствующий вклад в уменьшение изменения климата, то все страны выиграют и смогут избежать наихудших последствий.",
    "articles--existential-risks:458": "К сожалению, с точки зрения каждой отдельной страны будет лучше, если _все другие_ страны сократят свои выбросы, а её собственная экономика останется нетронутой. Поэтому у каждой страны есть стимул не соблюдать климатические соглашения, и именно поэтому в этом вопросе достигнуто так мало прогресса (это называется [\"дилемма заключённого\"](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma)).",
    "articles--existential-risks:460": "Более того, в таком её описании проблема всё равно будет сильно преуменьшена. Больше всего пользы от снижения катастрофических рисков получают _будущие_ поколения. У них нет возможности отстаивать свои интересы — будь то экономически или политически.",
    "articles--existential-risks:462": "Если бы [будущие поколения](/future-generations/) могли голосовать на наших выборах, подавляющее большинство из них голосовало бы за принятие законов, повышающих безопасность человечества. А если бы будущие поколения могли присылать деньги в прошлое, они были бы готовы заплатить нам огромные суммы денег за снижение этих рисков. (Технически говоря, снижение этих рисков является глобальным общественным благом, идущим сквозь поколения, что вполне логично делает его одним из самых недооценённых способов приносить пользу миру.)",
    "articles--existential-risks:464": "Наша нынешняя система плохо справляется с защитой будущих поколений. Мы знаем людей, которые разговаривали с высокопоставленными лицами в правительстве Великобритании, и многие из этих чиновников хотят что-то сделать с этими рисками, но, по их словам, давление новостных и избирательных циклов мешает уделять должное внимание этим вопросам. В большинстве стран нет государственных учреждений, в прямые обязанности которых входит снижение этих рисков.",
    "articles--existential-risks:466": "Это удручающая ситуация, но это также и возможность. Для людей, которые действительно хотят сделать мир лучше, такой недостаток внимания означает, что есть много способов принести огромную пользу обществу.",
    "articles--existential-risks:468": "## Что можно сделать с этими рисками?",
    "articles--existential-risks:470": "Мы рассмотрели масштабы и недооценённость этих вопросов, но что насчёт третьего критерия — разрешимости?",
    "articles--existential-risks:472": "Прогресс в решении этих проблем кажется менее достижимым, чем в более привычных областях вроде глобального здравоохранения. Измерять влияние действий на здоровье (по крайней мере в краткосрочной перспективе) намного проще, и за десятки лет было получено много данных о том, что лучше всего работает. Поэтому работа по снижению катастрофических рисков выглядит хуже в плане разрешимости.",
    "articles--existential-risks:474": "Однако мы всё равно можем многое сделать, и с учётом гигантских масштабов и недооценённости этих рисков, они по-прежнему выглядят самыми важными из всех проблем.",
    "articles--existential-risks:476": "Мы обрисуем некоторые способы снижения этих рисков, разделив их на три широкие категории:",
    "articles--existential-risks:478": "### 1\\. Прицельные усилия для снижения отдельных рисков",
    "articles--existential-risks:480": "Один из подходов заключается в непосредственной работе над каким-то из рисков. Существует множество конкретных предложений по борьбе с каждым из них, например:",
    "articles--existential-risks:482": "1. Многие эксперты считают, что повышение эффективности эпидемиологического надзора снизит риски пандемий. Сюда могут входить технологические улучшения или более хорошие способы сбора и обработки уже существующих данных, чтобы мы могли быстрее замечать вспышки новых пандемий. А чем быстрее будет замечена новая пандемия, тем проще будет с ней справиться.",
    "articles--existential-risks:484": "2. Существует множество способов повлиять на изменение климата: например, помощь в разработке более эффективных солнечных батарей или введение углеродного налога.",
    "articles--existential-risks:486": "3. Если говорить про ИИ, то можно заниматься исследованиями \"проблемы контроля\" в информатике, чтобы снизить вероятность непреднамеренного вреда от мощных ИИ-систем. В недавней статье, [\"Concrete problems in AI safety\"](https://blog.openai.com/concrete-ai-safety-problems/), описано несколько конкретных тем для исследований, но на данный момент всего около 20 человек в мире полноценно изучает подобные вопросы.",
    "articles--existential-risks:488": "4. В области ядерной безопасности многие эксперты считают, что для использования ядерного оружия в качестве сдерживающего фактора достаточно гораздо меньшего количества боеголовок. Но уменьшение запасов боеголовок также снизит риски от аварий и потенциальную опасность ядерных войн для выживания цивилизации.\n",
    "articles--existential-risks:491": "В наших обзорах проблем мы подробнее говорим о том, что вы можете сделать для борьбы с каждым из рисков:",
    "articles--existential-risks:493": "1. [Безопасность в области ИИ](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/)\n2. [Предотвращение пандемий](https://80000hours.org/problem-profiles/biosecurity/)\n3. [Ядерная безопасность](https://80000hours.org/problem-profiles/nuclear-security/)\n4. [Изменение климата](https://80000hours.org/problem-profiles/climate-change/)",
    "articles--existential-risks:498": "В этой категории мы не говорим о природных рисках, потому что они намного менее вероятны, и потому что человечество уже многое делает для борьбы с некоторыми из них. Повышение уровней богатства и технологического развития делает нас более устойчивыми к природным рискам, а на эти цели и так уже уходит огромное количество наших ресурсов.",
    "articles--existential-risks:500": "### 2\\. Общие усилия по снижению рисков",
    "articles--existential-risks:502": "Вместо того чтобы пробовать снизить каждый риск по отдельности, мы можем попытаться сделать так, чтобы цивилизация в целом лучше с ними справлялась. \"Общие\" усилия помогают снизить все риски сразу — даже те, о которых мы ещё не думали.",
    "articles--existential-risks:504": "Например, есть лица, принимающие ключевые решения, — обычно работающие в правительствах, — которым придётся работать с этими рисками по мере их возникновения. Если мы могли бы улучшить процессы принятия решений этими людьми и учреждениями, то это бы сделало общество более устойчивым в целом, а также решило бы многие другие проблемы.",
    "articles--existential-risks:506": "Недавние исследования выявили множество способов улучшить качество принятия решений, но большинство из них ещё не было внедрено. При этом мало кто работает над этой задачей. Мы более подробно разбираем эту тему в нашей статье об [улучшении институционального принятия решений](https://80000hours.org/problem-profiles/improving-institutional-decision-making/).",
    "articles--existential-risks:508": "Другой пример: мы можем попытаться сделать так, чтобы цивилизации было легче восстановиться после катастрофы. Всемирное хранилище семян — это замороженное хранилище в Арктике, в котором находятся семена многих важных сортов сельскохозяйственных культур. Его существование снижает шанс того, что мы навсегда потеряем какой-то из них. [Недавно туннель, ведущий к хранилищу, был залит](https://www.theguardian.com/environment/2017/may/19/arctic-stronghold-of-worlds-seeds-flooded-after-permafrost-melts) талой водой, которая, по иронии судьбы, образовалась в результате климатических изменений, поэтому проекту могло бы пригодиться дополнительное финансирование. Можно было бы разработать много других подобных проектов для сохранения наших знаний.",
    "articles--existential-risks:510": "Также мы могли бы [построить более хорошие убежища на случай чрезвычайных ситуаций](https://www.openphilanthropy.org/research/cause-reports/disaster-shelters), что снизило бы шансы вымирания человечества в результате пандемий, ядерной зимы и ударов астероидов (но не ИИ), а ещё повысило бы шансы успешного восстановления после катастрофы. На данный момент подобные меры выглядят менее эффективными, чем усилия по снижению самих рисков, но они всё равно полезны. Более недооценённым, а также возможно намного более дешёвым вариантом будет создание [альтернативных источников пищи](https://en.wikipedia.org/wiki/Feeding_Everyone_No_Matter_What) — например, таких, которым не нужен свет, и чьё производство можно быстро нарастить в условиях длительной зимы.",
    "articles--existential-risks:512": "Поскольку общие усилия приносят пользу даже если мы не уверены в деталях рисков, их привлекательность возрастает параллельно с вашей неопределённостью. Соответственно, чем более ясными и актуальными становятся для вас риски, тем больше ресурсов нужно перенаправлять с общих усилий на прицельные ([подробнее об этом](https://www.fhi.ox.ac.uk/the-timing-of-labour-aimed-at-reducing-existential-risk/)).",
    "articles--existential-risks:514": "Мы ожидаем, что есть много других перспективных направлений для общих усилий, но эта область крайне мало изучена. К примеру, ещё один подход мог бы быть основан на улучшении международной координации. Поскольку эти риски вызваны человечеством, человечество может их предотвратить, но нам мешают [проблемы координации](https://conceptually.org/concepts/coordination-problems/). К примеру, Россия не хочет разоружаться, потому что это поставит её в невыгодное положение по сравнению с США, и наоборот, хотя обе страны выиграли бы от отсутствия возможности ядерной войны.",
    "articles--existential-risks:516": "Однако можно попробовать улучшить нашу способность координироваться на уровне цивилизации: например, при помощи улучшения международных отношений или создания более эффективных международных институтов. Мы хотели бы видеть больше исследований в области этих направлений.",
    "articles--existential-risks:518": "Популярные способы делать мир лучше, вроде работы над образованием и международным развитием, тоже могут помочь обществу стать более устойчивым и разумным, поэтому также делают вклад в снижение катастрофических рисков. Например, более образованное население скорее всего голосовало бы за более разумных лидеров, а более развитые страны (при прочих равных) лучше справляются с пандемиями — неслучайно вспышки вируса Эбола сконцентрировались в некоторых из беднейших районов Западной Африки.",
    "articles--existential-risks:520": "Но мы не считаем образование и здравоохранение лучшими направлениями для приложения усилий по двум причинам. Во-первых, этим областям и так уделяется гораздо больше внимания, чем более нестандартными подходам, о которых мы говорили. К примеру, работа над улучшением системы образования [является чуть ли не самым популярным направлением](https://80000hours.org/2017/01/5-reasons-not-to-go-into-education/) среди людей, которые хотят приносить пользу миру. В одних лишь Штатах образование получает 800 миллиардов долларов в год от государства, а также ещё триллион долларов частного финансирования. Во-вторых, эти подходы оказывают намного более размытое влияние на снижение экзистенциальных рисков — чтобы иметь заметный эффект, нужно улучшить образование в очень больших масштабах. Поэтому мы предпочитаем работать над более прицельными и недооценёнными решениями.",
    "articles--existential-risks:522": "### 3\\. Расширение знаний и наращивание потенциала",
    "articles--existential-risks:524": "Мы крайне не уверены в том, какие риски являются наиболее значимыми, что с ними лучше всего делать, и какова вероятность того, что мы кардинально ошибаемся в наших представлениях о глобальных приоритетах. Это означает, что ещё одна ключевая цель — получше разобраться во всех этих вопросах.",
    "articles--existential-risks:526": "Мы можем получить больше информации, просто пробуя снизить эти риски и наблюдая за тем, какого прогресса получается достичь. Однако мы считаем, что наиболее недооценённым и важным способом расширить наши знания сейчас являются [исследования глобальных приоритетов](https://80000hours.org/problem-profiles/global-priorities-research/).",
    "articles--existential-risks:528": "Эта область находится на пересечении экономики и моральной философии, а её цель — найти ответы на общие вопросы о наиболее важных проблемах для человечества. Лишь маленькая группа исследователей работает над этими вопросами на постоянной основе.",
    "articles--existential-risks:530": "Другой способ работать с неопределённостью — накапливать ресурсы для использования в будущем, когда будет больше информации. Например, можно зарабатывать и откладывать деньги. Также можно инвестировать в свой [карьерный капитал](/articles/career-capital) (особенно в переносимые навыки и полезные связи), чтобы добиться большего в будущем.",
    "articles--existential-risks:532": "Однако мы считаем, что ещё более эффективным подходом может быть создание _сообщества_, ориентированного на снижение этих рисков — какими бы они не были. Причина заключается в том, что развивать потенциал сообщества можно быстрее, чем в одиночку наращивать свой финансовый или карьерный капитал. Например, если потратить год на нетворкинг с целью привлечь больше людей к работе над глобальными проблемами, вполне возможно найти ещё одного человека с релевантной экспертизой, который будет готов работать с вами. Таким образом, за год работы вы получите выгоду примерно в 100% — если говорить об усилиях, которые благодаря вам идут на решение важнейших проблем.",
    "articles--existential-risks:534": "На данный момент мы вкладываем наши усилия в [развитие сообщества эффективного альтруизма](https://80000hours.org/problem-profiles/promoting-effective-altruism/), в котором много людей, стремящихся снизить экзистенциальные риски. Более того, недавние темпы роста и исследования конкретных инициатив по развитию сообщества говорят о потенциально высокой эффективности этого подхода для улучшения мира.",
    "articles--existential-risks:536": "Однако мы ожидаем, что другие усилия по созданию сообществ тоже будут ценными. Мы были бы рады увидеть сообщество учёных, стремящихся продвигать культуру безопасности в академических кругах. Было бы здорово увидеть сообщество политиков, которые хотят снизить эти риски и сделать так, чтобы правительство начало больше заботиться о будущих поколениях.",
    "articles--existential-risks:538": "С учётом того, как мало людей активно работает над снижением экзистенциальных рисков, мы ожидаем, что можно многое сделать для создания движения вокруг них.",
    "articles--existential-risks:540": "### В целом, насколько эффективным является снижение этих рисков?",
    "articles--existential-risks:542": "Учитывая все подходы к снижению этих рисков, а также то, как мало ресурсов выделяется на борьбу с некоторыми из них, кажется, что существенный прогресс возможен.",
    "articles--existential-risks:544": "На самом деле, даже если рассматривать влияние этих рисков только на _нынешнее_ поколение (игнорируя любые выгоды для будущих поколений), то они с большой вероятностью будут наиболее приоритетными.",
    "articles--existential-risks:546": "Чтобы это проиллюстрировать, мы приведём несколько грубых и упрощённых цифр, которые, тем не менее, кажутся нам реалистичными: если потратить $100 миллиардов на борьбу с экзистенциальными рисками, то за следующий век можно было бы снизить их шансы более чем на 1%. Снижение этих рисков на единицу процента спасло бы, согласно математическому ожиданию, около 100 миллионов жизней в нынешнем поколении (1% от ~10 миллиардов людей, живущих сейчас). Такое вложение означало бы, что мы спасаем жизни по $1000 за каждую.[^:`Со времени последнего большого обновления этой статьи мы поменяли цифры в этом абзаце на более умеренные и понятные. Мы считаем, что так лучше передаётся смысл по сравнению с изначальными грубыми оценками, которые мы использовали, особенно с учётом всей неопределённости. Изначально было так: \"По нашим грубым оценкам, $10 миллиардов разумных вложений в борьбу с этими рисками могло бы снизить их на 1% за столетие. Другими словами, если сейчас риск составляет 4%, то он стал бы 3%. Снижение этих рисков на единицу процента спасло бы, согласно математическому ожиданию, 100 миллионов жизней (1% от 10 миллиардов). Это бы значило, что таким образом можно спасти жизни, отдав всего $100 за каждую из них.”`]",
    "articles--existential-risks:548": "[Грег Льюис](http://effective-altruism.com/ea/1n0/the_personaffecting_value_of_existential_risk/) сделал более подробные расчёты, согласно которым средняя стоимость года жизни для человека нынешнего поколения составляла бы $9200, или где-то $300 000 за одну жизнь.[^:`[The Person-Affecting Value of Reducing Existential Risk](https://forum.effectivealtruism.org/posts/dfiKak8ZPa46N7Np6/the-person-affecting-value-of-existential-risk-reduction) by Greg Lewis, [архивная версия](https://web.archive.org/web/20180808133606/http://effective-altruism.com/ea/1n0/the_personaffecting_value_of_existential_risk/), получена в августе 2018.",
    "articles--existential-risks:550": "> Учитывая всё это, модель выдаёт среднюю \"стоимость года жизни\" как интервал $1500-$26 000 (среднее значение $9200).`] Там также можно найти и другие оценки. Мы считаем, что Грег, скорее, слишком консервативен, потому что он предполагает, что риск вымирания в течение следующего столетия равен 1%, в то время как наша оценка в несколько раз выше. Мы также считаем, что следующий миллиард долларов, который будет потрачен на снижение экзистенциальных рисков, может дать больший эффект, чем предполагает Грег (стоит отметить, что это утверждение справедливо только при условии, что этот миллиард будет потрачен на самые недооценённые риски, такие как безопасность в сфере ИИ и биориски). Таким образом, мы бы не удивились, если бы для следующего миллиарда долларов, вложенного в снижение экзистенциальных рисков, стоимость спасения жизни одного человека из нынешнего поколения была бы менее $100.",
    "articles--existential-risks:552": "GiveWell рекомендует благотворительную организацию Against Malaria Foundation (AMF) как одну из лучших возможностей помочь нынешнему поколению, поскольку она спасает одну жизнь где-то за $7500 (цифры на 2017 год).[^:`> По нашим оценкам, AMF тратит примерно $7500 на спасение одной жизни (с учётом транспорта, организационных издержек и так далее).",
    "articles--existential-risks:554": "_Your Dollar Goes Further Overseas_, GiveWell",
    "articles--existential-risks:556": "[Архивная ссылка](https://web.archive.org/web/20171021023916/https://www.givewell.org/giving101/Your-dollar-goes-further-overseas), получена 21.10.2017.`] Таким образом, эти оценки ставят снижение экзистенциальных рисков выше или по крайней мере на тот же уровень экономической эффективности по спасению жизней нынешнего поколения, на котором сейчас находится AMF — благотворительная организация, которую выбрали именно из-за её выдающихся успехов в этой области.",
    "articles--existential-risks:558": "Точно также мы считаем, что если 10 000 талантливых молодых людей сосредоточили бы свою карьеру на этих рисках, они смогли бы снизить их где-то на 1%. Это значит, что каждый из них смог бы, согласно математическому ожиданию, спасти 1000 жизней на протяжении своей карьеры, что скорее всего больше, чем если бы они просто зарабатывали и жертвовали деньги в Against Malaria Foundation.[^:` Если на протяжении своей жизни вы пожертвуете миллион долларов (что составляет где-то треть среднего дохода выпускника вуза), и мы проведём расчеты исходя из стоимости спасения жизни в Against Malaria Foundation, то это спасёт около 130 жизней.",
    "articles--existential-risks:560": "Источник информации о доходе выпускников вузов:",
    "articles--existential-risks:562": "Carnevale, Anthony P., Stephen J. Rose, and Ban Cheah. \"The college payoff: Education, occupations, lifetime earnings.\" (2011).",
    "articles--existential-risks:564": "[Архивная ссылка](https://web.archive.org/web/20171021024644/https://repository.library.georgetown.edu/bitstream/handle/10822/559300/collegepayoff-complete.pdf?sequence=1), получена 21.10.2017.`]",
    "articles--existential-risks:566": "С одной стороны, это несправедливые сравнения, потому что оценка GiveWell гораздо лучше обоснована и исследована, в то время как наша оценка скорее является разумным предположением. Также могут быть и более эффективные способы помочь нынешнему поколению, чем AMF (например, активизм в сфере политики).",
    "articles--existential-risks:568": "Однако мы также _значительно_ приуменьшили полезность снижения экзистенциальных рисков. Основной причиной для защиты цивилизации является не выгода для нынешнего поколения, а помощь будущим поколениям. Мы не учитывали их в этой оценке.",
    "articles--existential-risks:570": "Если учитывать и будущие поколения тоже, то эффективность снижения экзистенциальных рисков становится на порядки выше. В таком случае сложно представить что-то ещё более приоритетное на данный момент.",
    "articles--existential-risks:572": "А теперь вы можете либо почитать разбор некоторых ответов на эти аргументы, либо сразу перейти к практическим способам принять участие в снижении этих рисков.",
    "articles--existential-risks:574": "## Кому _не стоит_ фокусироваться на защите будущего?",
    "articles--existential-risks:576": "<div class=\"panel clearfix \">",
    "articles--existential-risks:578": "Аргументы, представленные в этой статье, основаны на предположениях, с которыми не все согласятся. Здесь мы разберём некоторые ответы на эти аргументы.",
    "articles--existential-risks:580": "<div class=\"panel-group\"><div class=\"panel panel-default panel-collapse\"><div class=\"panel-heading\">",
    "articles--existential-risks:582": "#### <a class=\"collapsed\" data-toggle=\"collapse\" data-target=\"\\#-6\">Вам нужно уделять больше внимания своим друзьям и близким</a>",
    "articles--existential-risks:584": "</div>",
    "articles--existential-risks:586": "<div class=\"panel-body-collapse collapse\"><div class=\"panel-body\">\nМы говорим только о том, какими должны быть приоритеты в том случае, если вы стараетесь помочь людям в целом, в равной степени рассматривая интересы каждого (то, что философы иногда называют \"беспристрастным альтруизмом\").",
    "articles--existential-risks:589": "Большинство людей считают важным помогать другим хотя бы в какой-то степени: если вы можете помочь незнакомцу без особых затрат для себя, то это стоит сделать. Люди также заботятся о своих собственных жизнях и о своих друзьях и близких — и тут мы ничем не отличаемся.",
    "articles--existential-risks:591": "Как найти баланс между этими приоритетами — сложный вопрос. Если вам повезло и вы можете позволить себе делать вклад в улучшение мира, то мы считаем, что защита будущего должна быть главным приоритетом. В следующем разделе мы предлагаем несколько конкретных способов поучаствовать в этой инициативе.",
    "articles--existential-risks:593": "В противном случае, вам может быть нужно сосредоточиться на своей личной жизни, а поучаствовать в сохранении цивилизации можно в свободное время или в будущем.",
    "articles--existential-risks:595": "</div>",
    "articles--existential-risks:597": "</div>",
    "articles--existential-risks:599": "</div>",
    "articles--existential-risks:601": "<div class=\"panel panel-default panel-collapse\"><div class=\"panel-heading\">",
    "articles--existential-risks:603": "#### <a class=\"collapsed\" data-toggle=\"collapse\" data-target=\"\\#-7\">Вы считаете, что риски гораздо ниже, чем мы утверждаем</a>",
    "articles--existential-risks:605": "</div>",
    "articles--existential-risks:607": "<div class=\"panel-body-collapse collapse\"><div class=\"panel-body\">",
    "articles--existential-risks:609": "У нас нет надежных оценок многих рисков, вызванных деятельностью человека, поэтому вы можете попробовать оценить их самостоятельно и прийти к выводу, что они гораздо ниже, чем предполагаем мы. Если бы они были достаточно низкими, то их снижение перестало бы быть главным приоритетом.",
    "articles--existential-risks:611": "Более низкие оценки не кажутся нам правдоподобными по причинам, описанным выше. Если рассмотреть все потенциальные риски, то трудно утверждать, что они составят менее 1% на протяжении ста лет, а даже риск в 1% скорее всего требует гораздо более активного вмешательства, чем то, что мы наблюдаем сегодня.",
    "articles--existential-risks:613": "</div>",
    "articles--existential-risks:615": "</div>",
    "articles--existential-risks:617": "</div>",
    "articles--existential-risks:619": "<div class=\"panel panel-default panel-collapse\"><div class=\"panel-heading\">",
    "articles--existential-risks:621": "#### <a class=\"collapsed\" data-toggle=\"collapse\" data-target=\"\\#-8\">Вы считаете, что с этими рисками почти ничего нельзя поделать</a>",
    "articles--existential-risks:623": "</div>",
    "articles--existential-risks:625": "<div class=\"panel-body-collapse collapse\"><div class=\"panel-body\">",
    "articles--existential-risks:627": "Мы оцениваем эти риски как менее \"разрешимые\" по сравнению с проблемами вроде глобального здравоохранения, поэтому ожидаем более медленного прогресса за единицу вложений. Тем не менее, мы считаем, что их масштабы и недооценённость с лихвой компенсируют этот факт, и по сумме факторов работа над ними даёт больше пользы. Многие считают, что [эффективный альтруизм](https://www.effectivealtruism.org/) поддерживает только \"проверенные\" действия, но это миф. Имеет смысл браться за задачи даже с маленьким шансом успеха, если ожидаемые результаты достаточно высоки. Ведущий инвестор в нашем сообществе выступает за подход [\"оправданных рисков\"](https://www.openphilanthropy.org/blog/hits-based-giving) в филантропии.",
    "articles--existential-risks:629": "Однако если вы _гораздо_ более пессимистично оцениваете вероятность достижения прогресса в снижении экзистенциальных рисков, то, возможно, стоит работать над более стандартными проблемами вроде глобального здравоохранения.",
    "articles--existential-risks:631": "Мы сами могли бы переключиться на другую проблему, если бы в снижение экзистенциальных рисков вкладывалось в 100 раз больше ресурсов, чем сейчас. Но до этого ещё далеко.",
    "articles--existential-risks:633": "Ещё один схожий ответ на наши аргументы может заключаться в том, что мы уже и так принимаем наилучшие меры для снижения этих рисков. Такая ситуация означала бы, что смены приоритетов не требуется. Например, раньше мы упоминали, что образование, скорее всего, помогает снижать эти риски. Если вы считаете, что улучшение образования является _лучшим_ из вариантов (например, потому что у вас очень высокая степень неопределённости о том, какие риски будут наиболее неотложными), то тогда, поскольку человечество уже вкладывает огромное количество ресурсов в образование, вы можете считать, что ситуация уже и так под контролем. Такой вывод не кажется нам правдоподобным, поскольку, как уже было сказано, существует множество неиспользованных возможностей для снижения этих рисков, которые выглядят более прицельными и недооценёнными.",
    "articles--existential-risks:635": "Другой пример: экономисты иногда утверждают, что нам следует просто сосредоточиться на экономическом росте, поскольку это позволит нам наилучшим образом справиться с рисками в будущем. Это не кажется нам убедительным, поскольку некоторые виды экономического роста увеличивают риски (например, открытие нового оружия), поэтому нельзя утверждать, что экономический рост однозначно является одним из лучших способов снижения рисков. Вместо этого мы бы сфокусировались хотя бы на [избирательном технологическом развитии](https://en.wikipedia.org/wiki/Differential_technological_development) или других более прицельных усилиях, перечисленных выше.",
    "articles--existential-risks:637": "</div>",
    "articles--existential-risks:639": "</div>",
    "articles--existential-risks:641": "</div>",
    "articles--existential-risks:643": "<div class=\"panel panel-default panel-collapse\"><div class=\"panel-heading\">",
    "articles--existential-risks:645": "#### <a class=\"collapsed\" data-toggle=\"collapse\" data-target=\"\\#-9\">Вы считаете, что есть более эффективные способы помочь будущему</a>",
    "articles--existential-risks:647": "</div>",
    "articles--existential-risks:649": "<div class=\"panel-body-collapse collapse\"><div class=\"panel-body\">",
    "articles--existential-risks:651": "Хотя снижение этих рисков имеет смысл и для нынешнего поколения, большая часть их важности обусловлена [долгосрочными последствиями](https://80000hours.org/articles/future-generations/) — если цивилизации придёт конец, мы потеряем всё возможное будущее.",
    "articles--existential-risks:653": "Вы можете считать, что нашему поколению доступны и другие действия с очень долгосрочными последствиями, которые могут быть столь же важны для снижения риска вымирания. В частности, мы могли бы улучшить _качество_ будущего, защитив нашу цивилизацию от возможности необратимого \"застревания\" в исключительно плохих сценариях.",
    "articles--existential-risks:655": "Это может быть немного похоже на научную фантастику, но всё же: один из возможных сценариев, которые были озвучены, заключается в том, что новые технологии, такие как радикальная массовая слежка или психологический контроль, могут привести к созданию тоталитарного правительства, которое никогда не получится свергнуть. Подобные сценарии были описаны в книгах \"_1984_\" и в \"_Дивном новом мире_\", соответственно. Если такое правительство окажется плохим, то цивилизацию может ждать судьба ещё _хуже_, чем вымирание — тысячи лет страданий.",
    "articles--existential-risks:657": "Другие высказывают опасения, что разработка продвинутых ИИ-систем может нанести страшный вред, если будет вестись безответственно: например, из-за конфликта между несколькими группами, ставящими своей целью разработку этой технологии. В частности, если в будущем разработка этих систем повлечет за собой создание разумных цифровых существ, то вопрос их благополучия может стать крайне важным.",
    "articles--existential-risks:659": "Риски сценариев будущего, в которых присутствует астрономическое количество страданий, были названы \"s-рисками\".[^:`S-риски — это риски плохих исходов, которые влекут за собой страдания астрономических масштабов, чьё количество непомерно превышает все страдания, когда-либо существовавшие на Земле до сих пор.\n>\n> S-риски являются подгруппой экзистенциальных рисков (или x-рисков)... Ник Бостром определяет x-риск следующим образом:\n>\n> \"Экзистенциальный риск — это риск плохого исхода, который приведёт к вымиранию земной разумной жизни или же необратимо и радикально ограничит её потенциал.\"",
    "articles--existential-risks:665": "_S-risks: Why they are the worst existential risks, and how to prevent them_, Max Daniel, Foundational Research Institute, 2017,",
    "articles--existential-risks:667": "[Архивная ссылка](https://web.archive.org/web/20171021025032/https://foundational-research.org/s-risks-talk-eag-boston-2017/), получена 21.10.2017.`] Если сегодня можно сделать что-то для предотвращения s-рисков (например, при помощи целенаправленных исследований в областях технической безопасности и регулирования ИИ), то это может быть ещё более важным.",
    "articles--existential-risks:669": "Ещё одна область, на которую следует обратить внимание — крупные технологические переходы. В этой статье мы говорили об опасностях генной инженерии и искусственного интеллекта, но внедрение этих технологий также может привести к ещё одной индустриальной революции и сделать очень много хорошего. Возможно, есть действия, которые позволят нам повысить вероятность благополучного перехода, а не снизить риски плохого. Это называют попытками повысить \"экзистенциальную надежду\", в отличие от снижения \"экзистенциальных рисков\".[^:`_Existential Risk and Existential Hope: Definitions_, Owen Cotton-Barratt & Toby Ord, Future of Humanity Institute, 2015",
    "articles--existential-risks:671": "[Архивная ссылка](https://web.archive.org/web/20171021025945/https://www.fhi.ox.ac.uk/Existential-risk-and-existential-hope.pdf), получена 21.10.2017`]",
    "articles--existential-risks:673": "Мы согласны с тем, что могут быть и другие способы добиться очень долгосрочного эффекта, и что они могут быть более важными, чем снижение риска вымирания. Однако большинство этих предложений ещё недостаточно хорошо проработаны, и у нас нет чёткого понимания о том, что с ними делать.",
    "articles--existential-risks:675": "Для нас главный практический результат рассмотрения других способов влияния на будущее заключается в том, что мы считаем _ещё более_ важным положительно влиять на переходы к новым революционным технологиям, таким как ИИ. Мы также хотели бы видеть больше исследований глобальных приоритетов, посвящённых этим вопросам.",
    "articles--existential-risks:677": "В целом, мы по-прежнему считаем, что сначала стоит сосредоточиться на снижении экзистенциальных рисков, после чего можно будет перейти к другим способам помочь будущему.",
    "articles--existential-risks:679": "Одним из способов помочь будущему, который _не кажется_ нам хорошей альтернативой — это его ускорение. Некоторые люди, стремящиеся помочь будущему, фокусируются на технологическом прогрессе (например, разрабатывают вакцины), и у этого действительно есть позитивные долгосрочные последствия. Но мы считаем, что с долгосрочной точки зрения более важным является то, где мы окажемся, а не то, как быстро мы туда попадём. Открытие новой вакцины, скорее всего, означает, что мы получим её раньше, а не то, что иначе она вообще бы не появилась.",
    "articles--existential-risks:681": "Более того, поскольку технологии также являются причиной многих из этих рисков, мы не знаем, насколько ускорение их развития будет полезным в краткосрочной перспективе.",
    "articles--existential-risks:683": "Ускорение прогресса также получает намного больше внимания, поскольку приносит пользу и нынешнему поколению. Как мы уже писали, ежегодно на исследование и развитие новых технологий тратится более 1 триллиона долларов. Поэтому ускорение прогресса одновременно и менее приоритетно, и менее недооценено.",
    "articles--existential-risks:685": "О других способах помощи будущим поколениям можно прочитать в третьей главе диссертации Ника Бекстеда, [_On the Overwhelming Importance of Shaping the Far Future_](https://rucore.libraries.rutgers.edu/rutgers-lib/40469/).",
    "articles--existential-risks:687": "</div>",
    "articles--existential-risks:689": "</div>",
    "articles--existential-risks:691": "</div>",
    "articles--existential-risks:693": "<div class=\"panel panel-default panel-collapse\"><div class=\"panel-heading\">",
    "articles--existential-risks:695": "#### <a class=\"collapsed\" data-toggle=\"collapse\" data-target=\"\\#-10\">Вы уверены, что будущее будет коротким или плохим</a>",
    "articles--existential-risks:697": "</div>",
    "articles--existential-risks:699": "<div class=\"panel-body-collapse collapse\"><div class=\"panel-body\">",
    "articles--existential-risks:701": "Если вы считаете, что недолгое существование цивилизации практически гарантировано, то ценность снижения экзистенциальных рисков значительно снижается (хотя эти усилия всё равно могут быть осмысленны для помощи нынешнему поколению и любому небольшому числу будущих поколений).",
    "articles--existential-risks:703": "Мы согласны, что существует значительная вероятность скорого конца цивилизации (именно поэтому этот вопрос так важен), но мы также считаем, что существует _достаточно_ большая вероятность того, что она может просуществовать очень долго, и поэтому [за будущее стоит бороться](https://80000hours.org/articles/future-generations/).",
    "articles--existential-risks:705": "Аналогично, если вы считаете, что будущее скорее будет плохим, чем хорошим, то ценность снижения этих рисков падает (или если считаете, что намного более важно снижать страдания, чем повышать благополучие). Однако нам не кажется, что это вероятно, поскольку люди _хотят_ хорошего будущего и поэтому _постараются_ сделать его таковым. Мы также считаем, что за последние несколько столетий произошел значительный моральный прогресс (благодаря тенденциям, отмеченным ранее), и мы полагаем, что он продолжится. См. более подробное обсуждение в сноске 11.[^:`Становится ли мир лучше?",
    "articles--existential-risks:707": "Хотя есть причины считать, что большинство показателей прогресса растёт (как показано в статье), есть некоторые аспекты, в которых жизнь могла стать хуже. Например, в книге [_Sapiens_](https://www.amazon.com/Sapiens-Humankind-Yuval-Noah-Harari/dp/0062316095) Юваль Харари утверждает, что в современную эпоху усилились проблемы одиночества и психического здоровья, в то время как ощущения значимости и смысла могли снизиться. Мы скептически относимся к тому, что эти минусы могут перевешивать плюсы, но сложно сказать наверняка.",
    "articles--existential-risks:709": "Более сильные аргументы в пользу того, что мир становится хуже, возникают в контексте нашего воздействия на животных. В частности, с 1960-х годов резко выросло промышленное животноводство, и сейчас [где-то 30+ миллиардов животных](https://80000hours.org/problem-profiles/factory-farming/) ежегодно живут в ужасных условиях на промышленных фермах. Если нам небезразличны страдания этих животных, то это может перевесить наши успехи в сфере человеческого благополучия.",
    "articles--existential-risks:711": "Учитывая все эти аргументы, мы не можем однозначно утверждать, что суммарное благополучие возросло. Тем не менее, более важным является вопрос о том, что нас ждет в будущем.",
    "articles--existential-risks:713": "Станет ли мир лучше?",
    "articles--existential-risks:715": "Мы считаем, что до тех пор, пока человечество существует, развитие технологий и моральный прогресс дают нам возможности справиться с самыми серьёзными социальными проблемами, а также жить гораздо лучше в будущем. Если не считать экзистенциальные угрозы, то многие конкретные глобальные проблемы могут быть решены, если уровни богатства, технологического развития, а также морального и политического прогресса будут и дальше повышаться.",
    "articles--existential-risks:717": "Например, в случае с промышленным животноводством мы ожидаем, что по мере того, как люди будут становиться богаче, проблема будет уменьшаться. Во-первых, богатые люди более склонны к \"этичному\" потреблению, потому что в большей степени могут себе это позволить. Во-вторых, технологии способны положить конец промышленному животноводству при помощи заменителей мяса, искусственно выращенного мяса или более гуманных методов ведения сельского хозяйства. В-третьих, моральная значимость других живых существ возросла с течением времени (\"расширяющийся круг заботы\"), поэтому мы ожидаем, что в будущем люди будут ещё больше заботиться о благополучии животных.",
    "articles--existential-risks:719": "Если посмотреть ещё шире, то в конечном счёте мы ожидаем, что будущее будет лучше, потому что люди сами этого хотят. Чем больше технологической мощи и личных свобод мы имеем, тем проще людям реализовывать свои ценности. Поскольку люди _хотят_ жить хорошо, улучшение будущего — более вероятный сценарий, чем его ухудшение.",
    "articles--existential-risks:721": "При этом остаётся много вопросов. Например, многие наши ценности в какой-то степени противоречат друг другу, и это может привести к конфликтам. Вопросы о том, что ждёт нас в будущем, мало изучены. Поэтому хотя мы и ожидаем, что будущее будет лучше, чем настоящее, мы также признаём значительную степень неопределённости.`]",
    "articles--existential-risks:723": "Более того, даже если вы не уверены, насколько хорошим будет будущее, или подозреваете, что оно окажется плохим (в тех аспектах, с которыми мы в принципе сможем справиться), у вас могут быть причины хотеть, чтобы цивилизация выжила и сохранила возможность различных сценариев развития. У людей будущего будет гораздо больше времени на изучение того, стоит ли цивилизации расширяться, оставаться тех же размеров или уменьшиться. Если вы считаете, что есть неплохие шансы того, что мы сможем учесть подобные этические вопросы, то это будет хорошей причиной положиться на мудрость будущих поколений и оставить последнее слово за ними. В целом, степень нашей неопределённость по поводу этих сложных вопросов крайне высока, но это скорее _увеличивает_ наше стремление избежать любых необратимых сценариев.[^:`Это справедливо лишь в том случае, если вы считаете, что этические размышления имеют смысл, что моральный прогресс на самом деле возможен, и ваши представления о моральной неопределённости не противоречат идее о том, что оставлять возможность для разных сценариев будущего — хорошо.`]",
    "articles--existential-risks:725": "Кроме этого, вам, скорее всего, стоит направить своё внимание на различные способы по снижению вероятности плохого будущего, такие как предотвращение s-рисков.",
    "articles--existential-risks:727": "</div>",
    "articles--existential-risks:729": "</div>",
    "articles--existential-risks:731": "</div>",
    "articles--existential-risks:733": "<div class=\"panel panel-default panel-collapse\"><div class=\"panel-heading\">",
    "articles--existential-risks:735": "#### <a class=\"collapsed\" data-toggle=\"collapse\" data-target=\"\\#-11\">Вы уверены, что с этической точки зрения гораздо важнее помогать текущему поколению</a>",
    "articles--existential-risks:737": "</div>",
    "articles--existential-risks:739": "<div class=\"panel-body-collapse collapse\"><div class=\"panel-body\">",
    "articles--existential-risks:741": "Если вы уверены, что у нас гораздо больше обязательств перед нынешним поколением, чем перед будущими (например, потому что придерживаетесь личностно-ориентированного подхода в этике), то важность снижения экзистенциальных рисков уменьшается. Лично нам [этот подход не кажется особо убедительным](https://80000hours.org/articles/future-generations/#3-do-we-have-moral-obligations-to-future-generations).",
    "articles--existential-risks:743": "При этом мы показали, что даже если игнорировать будущие поколения, эти риски кажутся достойными внимания. Борьба с ними в любом случае может относительно дёшево спасать жизни нынешнего поколения, а ещё избежать многих страданий от катастроф среднего масштаба.",
    "articles--existential-risks:745": "Более того, если вы не уверены насчёт того, есть ли у нас моральные обязательства перед будущими поколениями, тогда вам опять же следует оставить возможность для различных сценариев, а для этого нужно сохранить цивилизацию.",
    "articles--existential-risks:747": "Тем не менее, если одновременно придерживаться позиции о том, что у нас нет серьёзных обязательств перед будущими поколениями, и о том, что с экзистенциальными рисками мало что можно поделать, или же что нет полезных исследований, которые можно было бы провести, то тогда на первое место может выйти какой-то другой способ помощи текущим поколениям. Это может быть работа в области ускорения развития технологий, глобального здравоохранения или ментального здоровья. Или же вы можете решить, что есть более важные моральные проблемы, такие как промышленное животноводство.",
    "articles--existential-risks:749": "</div>",
    "articles--existential-risks:751": "</div>",
    "articles--existential-risks:753": "</div>",
    "articles--existential-risks:755": "</div>",
    "articles--existential-risks:757": "</div>",
    "articles--existential-risks:759": "* * *",
    "articles--existential-risks:761": "## Хотите поучаствовать в снижении экзистенциальных рисков?",
    "articles--existential-risks:763": "!img~ class=\"aligncenter size-large wp-image-40225\" width=\"1024\" height=\"783\" ~[](https://80000hours.org/wp-content/uploads/2017/10/8e397f553a7e23150e0304341d908e98-1024x783.jpg)",
    "articles--existential-risks:765": "Наше поколение может либо подтолкнуть нас к концу, либо стать тем поколением, которое проведёт человечество через самый опасный период и станет одним из самых важных поколений в истории.",
    "articles--existential-risks:767": "Мы можем стать либо тем поколением, которое сделает возможным достижение поразительного и процветающего мира, либо тем, что поставит всё под угрозу.",
    "articles--existential-risks:769": "Поскольку мы хотим помочь миру, этот выбор должен быть нашим главным приоритетом.",
    "articles--existential-risks:771": "Если вы хотите сфокусировать свою карьеру на снижении экзистенциальных рисков и защите нашего будущего, мы хотим вам помочь. Мы написали статью, в которой описаны различные варианты действий, а также первые шаги, которые вы можете сделать.",
    "articles--existential-risks:773": "<a href=\"https://80000hours.org/articles/how-to-reduce-existential-risk/\" class=\"btn btn-primary\">Как задействовать свою карьеру для снижения экзистенциальных рисков</a>",
    "articles--existential-risks:775": "После прочтения этой статьи (или если вы уже подумали о том, чем именно хотите заниматься), вы можете воспользоваться возможностью поговорить с нами лично. Мы поможем вам обдумать различные решения и сформулировать план.",
    "articles--existential-risks:777": "<a href=\"https://80000hours.org/speak-with-us/?int_campaign=existential-risks\" class=\"btn btn-primary\">Подать заявку на индивидуальное консультирование</a>",
    "articles--existential-risks:779": "## Дальнейшее чтение",
    "articles--existential-risks:781": "- Прочитайте о [51 идее в областях политики и научных исследований](https://80000hours.org/2020/04/longtermist-policy-ideas/), которые могут помочь снизить экзистенциальные риски.\n- Ознакомьтесь с академической версией наших доводов в [этой статье](http://www.existential-risk.org/concept.html) профессора Ника Бострома.\n- [Прочтите аргументацию о том, почему нужно сфокусироваться на будущих поколениях](https://80000hours.org/articles/future-generations/).\n- Прочитайте [альтернативную вводную статью о том, почему наше влияние на будущие поколения имеет огромную моральную важность](https://www.effectivealtruism.org/articles/longtermism/) — в ней также есть раздел с [возражениями](https://www.effectivealtruism.org/articles/longtermism/#objections).",
    "articles--existential-risks:786": "- Мы обсуждаем эту тему в наших подкастах с [Тоби Ордом](https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/) и [Ником Бекстедом](https://80000hours.org/2017/10/nick-beckstead-giving-billions/), а также в [других эпизодах подкаста](/podcast/) про конкретные виды рисков.",
    "articles--existential-risks:788": "- Послушайте это интервью с Томасом Мойниханом про [историю развития мысли в области экзистенциальных рисков](https://hearthisidea.com/episodes/thomas).\n- Мы также рекомендуем наш подкаст с [Карлом Шульманом, где мы обсуждаем понятные аргументы в пользу работы над экзистенциальными рисками и её практические последствия](https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/).",
    "articles--existential-risks:791": "<div class=\"tw--mt-6 tw--p-3 tw--pt-2 tw--bg-gray-lighter tw--rounded-md \">",
    "articles--existential-risks:793": "### <a class=\"tw--text-off-black hover:tw--text-off-black hover:tw--no-underline focus:tw--text-off-black\" href=\"https://80000hours.org/articles/the-most-important-century/\"> <small>Читайте дальше: </small> Это может быть самый важный век </a>",
    "articles--existential-risks:795": "<div class=\"tw--grid xs:tw--grid-flow-col tw--gap-3\"><div class=\"xs:tw--order-last tw--pt-1\">\n[!img~ width=\"720\" height=\"448\" ~[Декоративное превью поста](https://80000hours.org/wp-content/uploads/2021/09/this-cant-go-on-720x448.png)](https://80000hours.org/articles/the-most-important-century/)\n</div>",
    "articles--existential-risks:799": "<div><div class=\"tw--pb-3\">",
    "articles--existential-risks:801": "Почему разработка продвинутых ИИ-систем вместе со стремительным экономическим ростом и научным прогрессом, которые за этим последуют, может привести нас к крайне необычному будущему значительно быстрее, чем кажется большинству людей — и почему это делает 21-ый век самым важным в истории.",
    "articles--existential-risks:803": "</div>",
    "articles--existential-risks:805": "<div>\n<a href=\"https://80000hours.org/articles/the-most-important-century/\" class=\"btn btn-primary\">Продолжить →</a>\n</div>",
    "articles--existential-risks:809": "</div>",
    "articles--existential-risks:811": "</div>",
    "articles--existential-risks:813": "</div>",
    "articles--existential-risks:815": "<div class=\"well bg-gray-lighter margin-bottom margin-top padding-top-small padding-bottom-small\">",
    "articles--existential-risks:817": "### Оставьте свой имейл, и мы пришлём вам книгу (бесплатно)",
    "articles--existential-risks:819": "Подпишитесь на нашу рассылку, и мы пришлём вам бесплатный экземпляр «Пропасти» («The Precipice») — книги философа Тоби Орда о том, как подступиться к величайшим угрозам, с которыми сталкивается человечество.",
    "articles--existential-risks:821": "!newsletter-subscribe",
    "articles--existential-risks:823": "</div>",
    "articles--existential-risks:825": "<div class=\"margin-top-large margin-bottom-large\">\n!social-share",
    "articles--existential-risks:828": "</div>",
    "articles--existential-risks:830": "<footer><div class=\"margin-top margin-bottom-large\"><ul class=\"post-categories tags\"><li class=\"tags__tag\"><a href=\"https://80000hours.org/topic/causes/catastrophic-risks/\" title=\"View all posts on topic: Catastrophic risks\" rel=\"category tag\">Catastrophic risks</a></li><li class=\"tags__tag\"><a href=\"https://80000hours.org/topic/causes/future-generations/\" title=\"View all posts on topic: Future Generations\" rel=\"category tag\">Future Generations</a></li><li class=\"tags__tag\"><a href=\"https://80000hours.org/topic/big-picture/longtermism/\" title=\"View all posts on topic: Longtermism\" rel=\"category tag\">Longtermism</a></li></ul></div></footer>\n"
}
