{
    "problem-profiles--artificial-intelligence:0000": "# Предотвращение катастрофы, связанной с ИИ\n## ИИ может принести человечеству очень много пользы. Если мы избежим рисков\n<p class=\"entry-meta small\">By <span class=\"byline author vcard\"><a href=\"https://80000hours.org/author/benjamin-hilton/\" rel=\"author\" class=\"fn no-visited-styling\">Бенджамен Хилтон</a></span> · Опубликовано в августе 2022</p>",
    "problem-profiles--artificial-intelligence:0004": "<div class=\"problem-profile__introduction margin-top\"><div class=\"problem-profile-introduction\">",
    "problem-profiles--artificial-intelligence:0006": "##~class=\"no_toc\"~",
    "problem-profiles--artificial-intelligence:0008": "<div class=\"margin-bottom margin-top\" id=\"audio-player\"><div class=\"wrap-smart-track-player\"><div class=\"smart-track-player-container stp-color-60b86c-2a2e30\">\n</div></div></div>",
    "problem-profiles--artificial-intelligence:0011": "Почему судьбу мира определяют люди, а не шимпанзе?",
    "problem-profiles--artificial-intelligence:0013": "Люди решают, как будет выглядеть каждый уголок нашей планеты. Шимпанзе, конечно, очень умные по сравнению с другими животными, но им такая власть недоступна.",
    "problem-profiles--artificial-intelligence:0015": "(В общих чертах) такое положение дел вызвано интеллектом людей.[^:`Что мы подразумеваем здесь под \"интеллектом\"? Что-то вроде \"способности влиять на будущее предсказуемым образом\". Эта способность включает в себя понимание мира, достаточное для того, чтобы строить планы, которые будут работать, и способность претворять эти планы в жизнь. То, что люди способны влиять на будущее предсказуемым образом, означает, что они могут изменять мир вокруг себя, чтобы достигать своих целей и удовлетворять свои потребности. Важность способности строить и реализовывать планы мы более подробно обсудим [далее](#aps-systems).`]",
    "problem-profiles--artificial-intelligence:0017": "Компании и правительства тратят на разработку ИИ-систем [миллиарды долларов в год](https://web.archive.org/web/20221013005552/https://aiimpacts.org/funding-of-ai-research/). Когда эти системы станут достаточно продвинутыми, люди (рано или поздно) могут перестать быть самыми разумными существами на планете. [Как мы увидим](#making-advances-extremely-quickly), ИИ-системы развиваются. И быстро.",
    "problem-profiles--artificial-intelligence:0019": "Сколько именно времени займёт создание искусственного интеллекта, который будет справляться с подавляющим большинством задач лучше человека, — [предмет крайне оживлённых дискуссий](#when-can-we-expect-to-develop-transformative-AI). Но, судя по всему, появление такого ИИ возможно, и мы предполагаем, что оно случится в этом веке.",
    "problem-profiles--artificial-intelligence:0021": "Само по себе утверждение, что в этом веке искусственный интеллект превзойдёт человеческий, не влечёт за собой автоматически вывода, что искусственный интеллект — это очень важно или что он представляет собой угрозу для человечества. Ниже мы рассмотрим эти утверждения гораздо более подробно.",
    "problem-profiles--artificial-intelligence:0023": "Однако, судя по всему, вполне можно сказать, что потенциальное появление на Земле в ближайшем будущем интеллекта, соперничающего с человеческим, — это как минимум повод задуматься.",
    "problem-profiles--artificial-intelligence:0025": "Будут ли цели у систем, которые мы разрабатываем? И если да, то что это будут за цели?",
    "problem-profiles--artificial-intelligence:0027": "Будут ли они помогать человечеству в его стремлении творить добро? Или мы перестанем управлять нашим будущем и фактически история человечества на этом завершится?",
    "problem-profiles--artificial-intelligence:0029": "Честный ответ на эти вопросы: мы не знаем.",
    "problem-profiles--artificial-intelligence:0031": "Однако мы не можем просто ждать, скрестив пальцы, и беспристрастно наблюдать. Искусственный интеллект действительно может радикально изменить всё. Поэтому работа над тем, как он будет развиваться, — возможно, самое значимое, что мы можем делать.",
    "problem-profiles--artificial-intelligence:0033": "</div>",
    "problem-profiles--artificial-intelligence:0035": "<div class=\"problem-profile__summary panel margin-top\">",
    "problem-profiles--artificial-intelligence:0037": "## Краткое изложение",
    "problem-profiles--artificial-intelligence:0039": "Мы ожидаем, что в ближайшие десятилетия будет достигнут значительный прогресс в области искусственного интеллекта: возможно, машины даже превзойдут людей во многих — а возможно, даже во всех — задачах. Это может принести гигантскую пользу — например, поможет справиться с глобальными проблемами, которые сейчас неразрешимы, но также несёт в себе серьёзные риски. Эти риски могут породить опасные последствия как случайно (например, если мы не сможем решить задачу безопасности систем ИИ) или в результате человеческих решений (например, если системы ИИ ухудшат геополитический конфликт). Мы считаем, что для уменьшения этих рисков нужно сделать многое.",
    "problem-profiles--artificial-intelligence:0041": "Некоторые из рисков, которые несут в себе продвинутые ИИ, могут оказаться [экзистенциальными](/articles/existential-risks/) — то есть, они могут привести к исчезновению человечества или к тому, что человечество навсегда потеряет возможность управлять собственной судьбой.[^:`Также нас беспокоит вопрос о том, что системы ИИ, возможно, тоже следует учитывать с точки зрения этики — например, потому что они разумны. В этой статье мы не будем обсуждать этот вопрос, ему посвящена [отдельная статья](/problem-profiles/artificial-sentience/).`] Ниже обсуждаются важные вопросы о том, насколько так стремительно развивающаяся революционная технология может быть разработана безопасным образом и как она встроится в наше общество. Удовлетворительных ответов на эти вопросы пока нет, а задача поиска этих ответов крайне недооценена, но может быть разрешима. По нашим оценкам напрямую этим занимаются примерно 400 человек в мире.[^:`Количество таких людей оценить сложно.",
    "problem-profiles--artificial-intelligence:0043": "В идеале мы хотим оценить, сколько на задачу снижения экзистенциальных рисков от ИИ тратится ЭПЗ (\" [эквивалентов полной занятости](https://en.wikipedia.org/wiki/Full-time_equivalent)\").",
    "problem-profiles--artificial-intelligence:0045": "Однако в вопросе, кого считать работающим над этой задачей, есть множество неоднозначностей. Поэтому для своих оценок я пользовался следующими правилами:",
    "problem-profiles--artificial-intelligence:0047": "- Я не включал людей, которые, хотя, возможно, и предполагают, что они готовятся работать над предотвращением катастрофы, связанной с ИИ, но которые в настоящее время лишь учатся, а не работают над задачей напрямую.\n- Я включал исследователей, инженеров и прочий персонал, которые, судя по всему, напрямую занимаются исследованиями в области безопасности ИИ или вопросами регулирования ИИ и разработкой стратегий. Однако граница между этими людьми и теми, кого я решил не включать, довольно нечёткая. Например, я не включал специалистов по машинному обучению, разрабатывающих системы ИИ, которые потенциально можно использовать для исследований безопасности, но которые не разрабатывались в первую очередь именно для этой цели.\n- Я учитывал лишь время, потраченное на задачу снижения потенциальных [экзистенциальных рисков](/articles/existential-risks/) от ИИ, вроде тех, что обсуждаются в этой статье. Множество работ по безопасности и этике ИИ рассматривают более общие вопросы, а также другие риски, связанные с ИИ. Такие работы могут помогать снижению экзистенциальных рисков, и это усложняет подсчёты. Я решил учитывать только работы, которые напрямую связаны со снижением рисков от катастрофы, связанной с ИИ (подробнее читайте в [разделе, посвящённом нашей модели для оценки проблем](/articles/problem-framework/#a-challenge-direct-vs-indirect-future-effort)).\n- Аналогично я не учитывал людей, работающих над задачами, которые могут косвенно влиять на шансы катастрофы, связанной с ИИ: например, [улучшением эпистемологии и принятия решений в организациях]((/problem-profiles/improving-institutional-decision-making/), снижением вероятности [конфликта сверхдержав]((/problem-profiles/great-power-conflict/) или [распространением идей эффективного альтруизма](/problem-profiles/promoting-effective-altruism/).",
    "problem-profiles--artificial-intelligence:0052": "Определившись с этими правилами, я оценил количество ЭПЗ тремя способами.",
    "problem-profiles--artificial-intelligence:0054": "Во-первых, я оценил количество ЭПЗ, работающих напрямую над задачей снижения экзистенциальных рисков от ИИ, в каждой из организаций из базы данных [AI Watch](https://aiwatch.issarice.com/). Для этого я посмотрел, сколько в каждой из организаций числилось персонала — как всего, так и отдельно в 2022 году, — а также сколько в каждой из организаций числилось исследователей. В итоге я оценил количество людей, занимающихся техническими вопросами безопасности ИИ, в 76 — 536 ЭПЗ (90% доверительный интервал) при матожидании 196 ЭПЗ. Количество людей, занимающихся вопросами регулирования ИИ и разработкой стратегий, я оценил в 51 — 239 ЭПЗ (90% доверительный интервал) при матожидании в 151 ЭПЗ. Из-за неоднозначностей, описанных выше, на эти оценки значительно влияет субъективный фактор. Мои оценки могут оказаться сильно занижены, если в базе AI Watch отсутствуют данные по каким-то организациям, или значительно завышены, если данные учитывают каких-то людей несколько раз или включают людей, которые больше не занимаются упомянутыми задачами.",
    "problem-profiles--artificial-intelligence:0056": "Во-вторых, я взял методику, которой пользовался [Гэвин Лич для оценки количества людей, работающих над снижением экзистенциальных рисков от ИИ](https://forum.effectivealtruism.org/posts/8ErtxW7FRPGMtDqJy/the-academic-contribution-to-ai-safety-seems-large), и немного её доработал. Я разделил организации в оценках Лича на технические вопросы безопасности и регулирование/стратегии. Также я адаптировал числа Гэвина для доли научных работ в области информатики, которые относятся к теме безопасности ИИ и удовлетворяют ограничениям выше, и сделал соответствующие оценки для научных работ, которые не относятся к информатике, но относятся к нашей теме. В итоге у меня получилась оценка в 125 —1848 ЭПЗ (90% доверительный интервал) при матожидании 580 ЭПЗ для людей, которые занимаются техническими вопросами безопасности ИИ и 48 — 268 ЭПЗ (90% доверительный интервал) при матожидании 100 ЭПЗ для людей, которые занимаются регулированием и стратегиями.",
    "problem-profiles--artificial-intelligence:0058": "В-третьих, я посмотрел на оценки аналогичных чисел, сделанные [Стивеном МакЭлисом](https://forum.effectivealtruism.org/posts/3gmkrj3khJHndYGNe/estimating-the-current-and-future-number-of-ai-safety). В его подсчётах я немного по-другому распределил организации по категориям, чтобы результаты соответствовали предыдущим двум оценкам. В итоге у меня получилось 110 — 552 ЭПЗ (90% доверительный интервал) при матожидании 267 ЭПЗ для людей, которые работают над техническими вопросами безопасности ИИ, и 36 — 193 ЭПЗ (90% доверительный интервал) при матожидании 81 ЭПЗ для людей, которые занимаются регулированием и стратегиями.",
    "problem-profiles--artificial-intelligence:0060": "Для итоговой оценки я взял геометрическое среднее от полученных результатов и объединил доверительные интервалы в предположении, что распределение здесь приблизительно логнормальное.",
    "problem-profiles--artificial-intelligence:0062": "Наконец я оценил количество ЭПЗ для [вспомогательного персонала](#complementary-yet-crucial-roles) на основании базы данных AI Watch. Из релевантных организаций я выбрал те, для которых было достаточно данных о количестве исследователей. Затем я посчитал соотношения между числом исследователей в 2022 году и общим числом сотрудников в 2022 году в этих организациях, и для этих соотношений посчитал матожидание и доверительный интервал (исходя из дисперсии). Эти результаты я использовал, чтобы посчитать общее число вспомогательного персонала, исходя из предположения, что количество сотрудников распределено логнормально, а оценка упомянутых соотношений — нормально. В итоге у меня получилось 2 — 2357 ЭПЗ (90% доверительный интервал) с матожиданием 770 ЭПЗ для вспомогательного персонала.",
    "problem-profiles--artificial-intelligence:0064": "Вероятно, в этой методике много ошибок, однако я ожидаю, что эти ошибки малы по сравнению с неопределённостью в исходных данных, которые я использовал. Я по-прежнему очень не уверен по поводу общего количества ЭПЗ, занимающихся предотвращением катастрофы, связанной с ИИ, но я достаточно уверен, что это число достаточно мало, чтобы заявить о том, что проблема в целом не получает достаточно внимания.",
    "problem-profiles--artificial-intelligence:0066": "Я очень не уверен в своих оценках. В них использовалось очень много очень субъективных суждений. [Здесь](https://docs.google.com/spreadsheets/d/1e1Vh_nK_7VHKZUuQ9VNp3JWC2etjUAHVmVXbKarKMNw/edit) вы можете увидеть таблицы, которые я составил в процессе работы. Если у вас найдутся какие-то замечания, я буду очень рад, если вы сообщите их мне с помощью [этой формы](https://forms.gle/RRZaFTfdDkSQ6fJG8).`] Таким образом, возможность катастрофы, вызванной ИИ, вероятно, — самая важная проблема в мире. И тем, кто может внести вклад в её решение, лучше всего заниматься именно ей.",
    "problem-profiles--artificial-intelligence:0068": "Перспективные направления работы над этой проблемой включают в себя технические исследования — как создавать безопасные ИИ системы, — стратегические исследования — какие именно риски могут происходить от ИИ, — и исследования в области регулирования — как корпорации и правительства могут снизить эти риски. Если будут выработаны стоящие способы регулирования, нам понадобятся люди, которые смогут распространить их повсеместно. Также можно внести вклад на различных вспомогательных ролях: например, заниматься операционной деятельностью, освещать проблему в СМИ, жертвовать деньги и многое другое. Некоторые варианты мы перечисляем ниже.",
    "problem-profiles--artificial-intelligence:0070": "<div class=\"panel__highlight\">",
    "problem-profiles--artificial-intelligence:0074": "#### Рекомендуется с максимальным приоритетом",
    "problem-profiles--artificial-intelligence:0076": "Мы считаем, что это одна из самых важных проблем в мире.",
    "problem-profiles--artificial-intelligence:0078": "<div class=\"margin-top\"><div><h4 class=\"\">Масштаб  <i class=\"fas fa-question-circle text-primary icon-tooltip career-tooltip\" data-placement=\"right\" data-toggle=\"tooltip\" title=\"Если мы решим эту проблему, насколько улучшится мир? &lt;a href=&#34;/articles/problem-framework/#how-to-assess-scale&#34;&gt;Подробнее&lt;/a&gt;.\"> </i></h4><div class=\"\"><p>ИИ можно будет применить самыми разными способами, и потенциально он может принести очень много пользы. Однако нас сильно беспокоит вероятность чрезвычайно плохих последствий, в особенности экзистенциальной катастрофы. У нас есть лишь очень приблизительные оценки и мы в них сильно сомневаемся, однако сейчас мы полагаем, что в ближайшие 100 лет риск экзистенциальной катастрофы, вызванной искусственным интеллектом, составляет примерно 10%. Дальнейшие исследования могут значительно изменить эту оценку: некоторые эксперты в области ИИ-рисков полагают, что эта вероятность меньше 0,5%, другие же — что она значительно выше 50%, и мы готовы изменить своё мнение в любую сторону. В целом, сейчас мы считаем, что развитие ИИ представляет собой самый значительный риск к долгосрочному процветанию человечества, чем любая другая известная нам проблема.</p></div></div>",
    "problem-profiles--artificial-intelligence:0080": "<div class=\"margin-top\">",
    "problem-profiles--artificial-intelligence:0082": "#### Недооценённость",
    "problem-profiles--artificial-intelligence:0084": "<div>",
    "problem-profiles--artificial-intelligence:0086": "В 2020 году на уменьшение риска катастрофы от ИИ было потрачено 50 миллионов долларов. При этом на развитие способностей ИИ были потрачены миллиарды.[^:`Сложно точно сказать, сколько именно было потрачено на развитие способностей ИИ — частично из-за нехватки данных, частично из-за вопросов вроде:",
    "problem-profiles--artificial-intelligence:0088": "- Какие именно исследования в области ИИ действительно развивают его опасные способности, которые могут увеличить потенциальный экзистенциальный риск?\n- Считается ли развитием ИИ развитие компьютерных комплектующих или прогресс в сборе данных?\n- Как насчёт улучшений исследовательского процесса в целом или вклада во что-нибудь, что может увеличить инвестиции в развитие ИИ благодаря повышению экономического роста?",
    "problem-profiles--artificial-intelligence:0092": "Самое релевантное значение, которое мы смогли найти, — это расходы DeepMind в 2020 году, которые [согласно их годовому отчёту](https://web.archive.org/web/20221016011531/https://find-and-update.company-information.service.gov.uk/company/07386350/filing-history) составляли примерно 1 миллиард фунтов стерлингов. Мы ожидаем, что большая часть этих расходов — это в том или ином смысле вклад в \"развитие способностей ИИ\", ведь цель DeepMind — создание мощного ИИ общего назначения. (Впрочем, следует заметить, что DeepMind также вкладывается в работу по безопасности ИИ, что может уменьшать экзистенциальный риск.)",
    "problem-profiles--artificial-intelligence:0094": "Если расходы DeepMind — это примерно 10% от всего, что тратится на развитие способностей ИИ, мы получаем оценку примерно 10 миллиардов фунтов стерлингов. (Учитывая, что большинство компаний, занимающихся ИИ, находятся в США, и много усилий по созданию продвинутого ИИ тратятся в Китае, мы считаем, что 10% — это, наверное, неплохая оценка.)",
    "problem-profiles--artificial-intelligence:0096": "В качестве верхней оценки можно взять общий доход в секторе ИИ в 2021 году, который [примерно равнялся 340 миллиардам долларов](https://web.archive.org/web/20221016011608/https://www.idc.com/getdoc.jsp?containerId=prUS48127321).",
    "problem-profiles--artificial-intelligence:0098": "Таким образом, мы считаем, что на развитие способностей ИИ тратится от 1 до 340 миллиардов долларов в год. Даже если предположить, что тратится всего лишь 1 миллиард, это всё равно будет больше чем в 100 раз, чем расходы на снижение рисков от ИИ.`] Хотя мы видим, что эксперты по ИИ всё больше беспокоятся по этому поводу, по нашим оценкам над уменьшением вероятности связанной с ИИ экзистенциальной катастрофы работают лишь 400 человек (90% доверительный интервал — от 200 до 1000).[^:`См. оригинальную сноску, которая начинается в https://t.80000hours.ru/translate/80k/problem-profiles--artificial-intelligence/ru/?checksum=498ab4abcb09a42c и заканчивается в https://t.80000hours.ru/translate/80k/problem-profiles--artificial-intelligence/ru/?checksum=57843e78e5944b87",
    "problem-profiles--artificial-intelligence:0123": "`] Из них, судя по всему, примерно три четверти работают над техническими вопросами безопасности ИИ, а остальные делятся между разработкой стратегий (и других вопросов регулирования) и популяризацией.[^Заметим, что до 19 декабря 2022 года на этой странице число работающих над уменьшением экзистенциальных рисков оценивалось в 300 ЭПЗ, из которых две трети работали над техническими вопросами безопасности ИИ, а остальные делились между разработкой стратегий (и других вопросов регулирования) и популяризацией.",
    "problem-profiles--artificial-intelligence:0125": "Это изменение вызвано улучшенной (как мы надеемся!) оценкой, а не значительным увеличением количества исследователей.`]",
    "problem-profiles--artificial-intelligence:0127": "</div></div>",
    "problem-profiles--artificial-intelligence:0129": "<div class=\"margin-top\">",
    "problem-profiles--artificial-intelligence:0131": "#### Разрешимость",
    "problem-profiles--artificial-intelligence:0133": "<div>",
    "problem-profiles--artificial-intelligence:0135": "Похоже, добиться прогресса в предотвращении связанной с ИИ катастрофы довольно сложно, однако есть много направлений для дальнейших исследований и работа в этой области ведётся не так давно. Поэтому мы считаем, что эта проблема умеренно разрешима, хотя здесь мы очень не уверенны: повторимся, оценки разрешимости проблемы \"сделать безопасный ИИ\" различаются многократно.",
    "problem-profiles--artificial-intelligence:0137": "</div></div></div></div>",
    "problem-profiles--artificial-intelligence:0139": "<div><div class=\"row small\"><div class=\"col-sm-4 margin-top-smaller\"><h5>Проработанность профиля</h5><p>Глубокая <i class=\"fas fa-question-circle text-primary icon-tooltip career-tooltip\" data-placement=\"right\" data-toggle=\"tooltip\" title=\"Мы опросили как минимум десять людей с релевантным опытом в этой области, прочитали все лучшие существующие исследования, которые смогли найти, провели тщательное исследование практически всех основных причин нашей неуверенности, после чего описали всё, что мы нашли.\"> </i></p></div></div></div></div>",
    "problem-profiles--artificial-intelligence:0141": "<div class=\"no-print well margin-top\">",
    "problem-profiles--artificial-intelligence:0145": "</div></div>",
    "problem-profiles--artificial-intelligence:0147": "<div class=\"problem-profile-content padding-bottom-large\">\n!toc-container",
    "problem-profiles--artificial-intelligence:0150": "<div class=\"well bg-gray-lighter margin-bottom margin-top padding-top-small padding-bottom-small\">",
    "problem-profiles--artificial-intelligence:0162": "</div>",
    "problem-profiles--artificial-intelligence:0305": "<a id=\"what-is-deep-learning\" class=\"link-anchor\"></a>",
    "problem-profiles--artificial-intelligence:0325": "</aside>",
    "problem-profiles--artificial-intelligence:0400": "<a id=\"GPT-3\" class=\"link-anchor\"></a>",
    "problem-profiles--artificial-intelligence:0433": "</aside>",
    "problem-profiles--artificial-intelligence:0478": "<div class=\"container--page-width\"><div class=\"row\"><div class=\"tablepress-scroll-wrapper\" id=\"tablepress-189-no-2-scroll-wrapper\">",
    "problem-profiles--artificial-intelligence:0487": "</div></div></div>",
    "problem-profiles--artificial-intelligence:0611": "</div>",
    "problem-profiles--artificial-intelligence:0613": "<div class=\"panel-body-collapse collapse\" id=\"-20\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:0621": "</div></div></div>",
    "problem-profiles--artificial-intelligence:0626": "</div>",
    "problem-profiles--artificial-intelligence:0628": "<div class=\"panel-body-collapse collapse\" id=\"-21\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:0636": "</div></div></div>",
    "problem-profiles--artificial-intelligence:0641": "</div>",
    "problem-profiles--artificial-intelligence:0643": "<div class=\"panel-body-collapse collapse\" id=\"-22\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:0653": "</div></div></div></div></div>",
    "problem-profiles--artificial-intelligence:0724": "<div class=\"well bg-gray-lighter margin-bottom margin-top padding-top-small padding-bottom-small\">",
    "problem-profiles--artificial-intelligence:0735": "</div>",
    "problem-profiles--artificial-intelligence:0776": "<div class=\"panel clearfix \">",
    "problem-profiles--artificial-intelligence:0786": "</div>",
    "problem-profiles--artificial-intelligence:0941": "</div>",
    "problem-profiles--artificial-intelligence:1003": "</div>",
    "problem-profiles--artificial-intelligence:1005": "<div class=\"panel-body-collapse collapse\" id=\"-23\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1019": "</div></div></div>",
    "problem-profiles--artificial-intelligence:1024": "</div>",
    "problem-profiles--artificial-intelligence:1026": "<div class=\"panel-body-collapse collapse\" id=\"-24\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1038": "</div></div></div>",
    "problem-profiles--artificial-intelligence:1043": "</div>",
    "problem-profiles--artificial-intelligence:1045": "<div class=\"panel-body-collapse collapse\" id=\"-25\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1060": "</div></div></div>",
    "problem-profiles--artificial-intelligence:1065": "</div>",
    "problem-profiles--artificial-intelligence:1067": "<div class=\"panel-body-collapse collapse\" id=\"-26\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1111": "</div></div></div></div>",
    "problem-profiles--artificial-intelligence:1120": "</div>",
    "problem-profiles--artificial-intelligence:1122": "<div class=\"panel-body-collapse collapse\" id=\"-28\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1138": "</div></div></div>",
    "problem-profiles--artificial-intelligence:1143": "</div>",
    "problem-profiles--artificial-intelligence:1145": "<div class=\"panel-body-collapse collapse\" id=\"-29\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1168": "</div>",
    "problem-profiles--artificial-intelligence:1170": "<div class=\"panel-body-collapse collapse\" id=\"-30\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1187": "</div></div></div>",
    "problem-profiles--artificial-intelligence:1192": "</div>",
    "problem-profiles--artificial-intelligence:1194": "<div class=\"panel-body-collapse collapse\" id=\"-31\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1211": "</div>",
    "problem-profiles--artificial-intelligence:1213": "<div class=\"panel-body-collapse collapse\" id=\"-32\"><div class=\"panel-body\">"
}
