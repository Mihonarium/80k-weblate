{
    "problem-profiles--artificial-intelligence:0000": "# Предотвращение катастрофы, связанной с ИИ\n## ИИ может принести человечеству очень много пользы. Если мы избежим рисков\n<p class=\"entry-meta small\">By <span class=\"byline author vcard\"><a href=\"https://80000hours.org/author/benjamin-hilton/\" rel=\"author\" class=\"fn no-visited-styling\">Бенджамен Хилтон</a></span> · Опубликовано в августе 2022</p>",
    "problem-profiles--artificial-intelligence:0004": "<div class=\"problem-profile__introduction margin-top\"><div class=\"problem-profile-introduction\">",
    "problem-profiles--artificial-intelligence:0006": "##~class=\"no_toc\"~",
    "problem-profiles--artificial-intelligence:0008": "<div class=\"margin-bottom margin-top\" id=\"audio-player\"><div class=\"wrap-smart-track-player\"><div class=\"smart-track-player-container stp-color-60b86c-2a2e30\">\n</div></div></div>",
    "problem-profiles--artificial-intelligence:0011": "Почему судьбу мира определяют люди, а не шимпанзе?",
    "problem-profiles--artificial-intelligence:0013": "Люди решают, как будет выглядеть каждый уголок нашей планеты. Шимпанзе, конечно, очень умные по сравнению с другими животными, но им такая власть недоступна.",
    "problem-profiles--artificial-intelligence:0015": "(В общих чертах) такое положение дел вызвано интеллектом людей.[^:`Что мы подразумеваем здесь под \"интеллектом\"? Что-то вроде \"способности влиять на будущее предсказуемым образом\". Эта способность включает в себя понимание мира, достаточное для того, чтобы строить планы, которые будут работать, и способность претворять эти планы в жизнь. То, что люди способны влиять на будущее предсказуемым образом, означает, что они могут изменять мир вокруг себя, чтобы достигать своих целей и удовлетворять свои потребности. Важность способности строить и реализовывать планы мы более подробно обсудим [далее](#aps-systems).`]",
    "problem-profiles--artificial-intelligence:0017": "Компании и правительства тратят на разработку ИИ-систем [миллиарды долларов в год](https://web.archive.org/web/20221013005552/https://aiimpacts.org/funding-of-ai-research/). Когда эти системы станут достаточно продвинутыми, люди (рано или поздно) могут перестать быть самыми разумными существами на планете. [Как мы увидим](#making-advances-extremely-quickly), ИИ-системы развиваются. И быстро.",
    "problem-profiles--artificial-intelligence:0019": "Сколько именно времени займёт создание искусственного интеллекта, который будет справляться с подавляющим большинством задач лучше человека, — [предмет крайне оживлённых дискуссий](#when-can-we-expect-to-develop-transformative-AI). Но, судя по всему, появление такого ИИ возможно, и мы предполагаем, что оно случится в этом веке.",
    "problem-profiles--artificial-intelligence:0021": "Само по себе утверждение, что в этом веке искусственный интеллект превзойдёт человеческий, не влечёт за собой автоматически вывода, что искусственный интеллект — это очень важно или что он представляет собой угрозу для человечества. Ниже мы рассмотрим эти утверждения гораздо более подробно.",
    "problem-profiles--artificial-intelligence:0023": "Однако, судя по всему, вполне можно сказать, что потенциальное появление на Земле в ближайшем будущем интеллекта, соперничающего с человеческим, — это как минимум повод задуматься.",
    "problem-profiles--artificial-intelligence:0025": "Будут ли цели у систем, которые мы разрабатываем? И если да, то что это будут за цели?",
    "problem-profiles--artificial-intelligence:0027": "Будут ли они помогать человечеству в его стремлении творить добро? Или мы перестанем управлять нашим будущем и фактически история человечества на этом завершится?",
    "problem-profiles--artificial-intelligence:0029": "Честный ответ на эти вопросы: мы не знаем.",
    "problem-profiles--artificial-intelligence:0031": "Однако мы не можем просто ждать, скрестив пальцы, и беспристрастно наблюдать. Искусственный интеллект действительно может радикально изменить всё. Поэтому работа над тем, как он будет развиваться, — возможно, самое значимое, что мы можем делать.",
    "problem-profiles--artificial-intelligence:0033": "</div>",
    "problem-profiles--artificial-intelligence:0035": "<div class=\"problem-profile__summary panel margin-top\">",
    "problem-profiles--artificial-intelligence:0037": "## Краткое изложение",
    "problem-profiles--artificial-intelligence:0039": "Мы ожидаем, что в ближайшие десятилетия будет достигнут значительный прогресс в области искусственного интеллекта: возможно, машины даже превзойдут людей во многих — а возможно, даже во всех — задачах. Это может принести гигантскую пользу — например, поможет справиться с глобальными проблемами, которые сейчас неразрешимы, но также несёт в себе серьёзные риски. Эти риски могут породить опасные последствия как случайно (например, если мы не сможем решить задачу безопасности систем ИИ) или в результате человеческих решений (например, если системы ИИ ухудшат геополитический конфликт). Мы считаем, что для уменьшения этих рисков нужно сделать многое.",
    "problem-profiles--artificial-intelligence:0041": "Некоторые из рисков, которые несут в себе продвинутые ИИ, могут оказаться [экзистенциальными](/articles/existential-risks/) — то есть, они могут привести к исчезновению человечества или к тому, что человечество навсегда потеряет возможность управлять собственной судьбой.[^:`Также нас беспокоит вопрос о том, что системы ИИ, возможно, тоже следует учитывать с точки зрения этики — например, потому что они разумны. В этой статье мы не будем обсуждать этот вопрос, ему посвящена [отдельная статья](/problem-profiles/artificial-sentience/).`] Ниже обсуждаются важные вопросы о том, насколько так стремительно развивающаяся революционная технология может быть разработана безопасным образом и как она встроится в наше общество. Удовлетворительных ответов на эти вопросы пока нет, а задача поиска этих ответов крайне недооценена, но может быть разрешима. По нашим оценкам напрямую этим занимаются примерно 400 человек в мире.[^:`Количество таких людей оценить сложно.",
    "problem-profiles--artificial-intelligence:0043": "В идеале мы хотим оценить, сколько на задачу снижения экзистенциальных рисков от ИИ тратится ЭПЗ (\" [эквивалентов полной занятости](https://en.wikipedia.org/wiki/Full-time_equivalent)\").",
    "problem-profiles--artificial-intelligence:0045": "Однако в вопросе, кого считать работающим над этой задачей, есть множество неоднозначностей. Поэтому для своих оценок я пользовался следующими правилами:",
    "problem-profiles--artificial-intelligence:0047": "- Я не включал людей, которые, хотя, возможно, и предполагают, что они готовятся работать над предотвращением катастрофы, связанной с ИИ, но которые в настоящее время лишь учатся, а не работают над задачей напрямую.\n- Я включал исследователей, инженеров и прочий персонал, которые, судя по всему, напрямую занимаются исследованиями в области безопасности ИИ или вопросами регулирования ИИ и разработкой стратегий. Однако граница между этими людьми и теми, кого я решил не включать, довольно нечёткая. Например, я не включал специалистов по машинному обучению, разрабатывающих системы ИИ, которые потенциально можно использовать для исследований безопасности, но которые не разрабатывались в первую очередь именно для этой цели.\n- Я учитывал лишь время, потраченное на задачу снижения потенциальных [экзистенциальных рисков](/articles/existential-risks/) от ИИ, вроде тех, что обсуждаются в этой статье. Множество работ по безопасности и этике ИИ рассматривают более общие вопросы, а также другие риски, связанные с ИИ. Такие работы могут помогать снижению экзистенциальных рисков, и это усложняет подсчёты. Я решил учитывать только работы, которые напрямую связаны со снижением рисков от катастрофы, связанной с ИИ (подробнее читайте в [разделе, посвящённом нашей модели для оценки проблем](/articles/problem-framework/#a-challenge-direct-vs-indirect-future-effort)).\n- Аналогично я не учитывал людей, работающих над задачами, которые могут косвенно влиять на шансы катастрофы, связанной с ИИ: например, [улучшением эпистемологии и принятия решений в организациях]((/problem-profiles/improving-institutional-decision-making/), снижением вероятности [конфликта сверхдержав]((/problem-profiles/great-power-conflict/) или [распространением идей эффективного альтруизма](/problem-profiles/promoting-effective-altruism/).",
    "problem-profiles--artificial-intelligence:0052": "Определившись с этими правилами, я оценил количество ЭПЗ тремя способами.",
    "problem-profiles--artificial-intelligence:0054": "Во-первых, я оценил количество ЭПЗ, работающих напрямую над задачей снижения экзистенциальных рисков от ИИ, в каждой из организаций из базы данных [AI Watch](https://aiwatch.issarice.com/). Для этого я посмотрел, сколько в каждой из организаций числилось персонала — как всего, так и отдельно в 2022 году, — а также сколько в каждой из организаций числилось исследователей. В итоге я оценил количество людей, занимающихся техническими вопросами безопасности ИИ, в 76 — 536 ЭПЗ (90% доверительный интервал) при матожидании 196 ЭПЗ. Количество людей, занимающихся вопросами регулирования ИИ и разработкой стратегий, я оценил в 51 — 239 ЭПЗ (90% доверительный интервал) при матожидании в 151 ЭПЗ. Из-за неоднозначностей, описанных выше, на эти оценки значительно влияет субъективный фактор. Мои оценки могут оказаться сильно занижены, если в базе AI Watch отсутствуют данные по каким-то организациям, или значительно завышены, если данные учитывают каких-то людей несколько раз или включают людей, которые больше не занимаются упомянутыми задачами.",
    "problem-profiles--artificial-intelligence:0056": "Во-вторых, я взял методику, которой пользовался [Гэвин Лич для оценки количества людей, работающих над снижением экзистенциальных рисков от ИИ](https://forum.effectivealtruism.org/posts/8ErtxW7FRPGMtDqJy/the-academic-contribution-to-ai-safety-seems-large), и немного её доработал. Я разделил организации в оценках Лича на технические вопросы безопасности и регулирование/стратегии. Также я адаптировал числа Гэвина для доли научных работ в области информатики, которые относятся к теме безопасности ИИ и удовлетворяют ограничениям выше, и сделал соответствующие оценки для научных работ, которые не относятся к информатике, но относятся к нашей теме. В итоге у меня получилась оценка в 125 —1848 ЭПЗ (90% доверительный интервал) при матожидании 580 ЭПЗ для людей, которые занимаются техническими вопросами безопасности ИИ и 48 — 268 ЭПЗ (90% доверительный интервал) при матожидании 100 ЭПЗ для людей, которые занимаются регулированием и стратегиями.",
    "problem-profiles--artificial-intelligence:0058": "В-третьих, я посмотрел на оценки аналогичных чисел, сделанные [Стивеном МакЭлисом](https://forum.effectivealtruism.org/posts/3gmkrj3khJHndYGNe/estimating-the-current-and-future-number-of-ai-safety). В его подсчётах я немного по-другому распределил организации по категориям, чтобы результаты соответствовали предыдущим двум оценкам. В итоге у меня получилось 110 — 552 ЭПЗ (90% доверительный интервал) при матожидании 267 ЭПЗ для людей, которые работают над техническими вопросами безопасности ИИ, и 36 — 193 ЭПЗ (90% доверительный интервал) при матожидании 81 ЭПЗ для людей, которые занимаются регулированием и стратегиями.",
    "problem-profiles--artificial-intelligence:0060": "Для итоговой оценки я взял геометрическое среднее от полученных результатов и объединил доверительные интервалы в предположении, что распределение здесь приблизительно логнормальное.",
    "problem-profiles--artificial-intelligence:0062": "Наконец я оценил количество ЭПЗ для [вспомогательного персонала](#complementary-yet-crucial-roles) на основании базы данных AI Watch. Из релевантных организаций я выбрал те, для которых было достаточно данных о количестве исследователей. Затем я посчитал соотношения между числом исследователей в 2022 году и общим числом сотрудников в 2022 году в этих организациях, и для этих соотношений посчитал матожидание и доверительный интервал (исходя из дисперсии). Эти результаты я использовал, чтобы посчитать общее число вспомогательного персонала, исходя из предположения, что количество сотрудников распределено логнормально, а оценка упомянутых соотношений — нормально. В итоге у меня получилось 2 — 2357 ЭПЗ (90% доверительный интервал) с матожиданием 770 ЭПЗ для вспомогательного персонала.",
    "problem-profiles--artificial-intelligence:0064": "Вероятно, в этой методике много ошибок, однако я ожидаю, что эти ошибки малы по сравнению с неопределённостью в исходных данных, которые я использовал. Я по-прежнему очень не уверен по поводу общего количества ЭПЗ, занимающихся предотвращением катастрофы, связанной с ИИ, но я достаточно уверен, что это число достаточно мало, чтобы заявить о том, что проблема в целом не получает достаточно внимания.",
    "problem-profiles--artificial-intelligence:0066": "Я очень не уверен в своих оценках. В них использовалось очень много очень субъективных суждений. [Здесь](https://docs.google.com/spreadsheets/d/1e1Vh_nK_7VHKZUuQ9VNp3JWC2etjUAHVmVXbKarKMNw/edit) вы можете увидеть таблицы, которые я составил в процессе работы. Если у вас найдутся какие-то замечания, я буду очень рад, если вы сообщите их мне с помощью [этой формы](https://forms.gle/RRZaFTfdDkSQ6fJG8).`] Таким образом, возможность катастрофы, вызванной ИИ, вероятно, — самая важная проблема в мире. И тем, кто может внести вклад в её решение, лучше всего заниматься именно ей.",
    "problem-profiles--artificial-intelligence:0068": "Перспективные направления работы над этой проблемой включают в себя технические исследования — как создавать безопасные ИИ системы, — стратегические исследования — какие именно риски могут происходить от ИИ, — и исследования в области регулирования — как корпорации и правительства могут снизить эти риски. Если будут выработаны стоящие способы регулирования, нам понадобятся люди, которые смогут распространить их повсеместно. Также можно внести вклад на различных вспомогательных ролях: например, заниматься операционной деятельностью, освещать проблему в СМИ, жертвовать деньги и многое другое. Некоторые варианты мы перечисляем ниже.",
    "problem-profiles--artificial-intelligence:0070": "<div class=\"panel__highlight\">",
    "problem-profiles--artificial-intelligence:0072": "### Наша оценка в целом",
    "problem-profiles--artificial-intelligence:0074": "#### Рекомендуется с максимальным приоритетом",
    "problem-profiles--artificial-intelligence:0076": "Мы считаем, что это одна из самых важных проблем в мире.",
    "problem-profiles--artificial-intelligence:0078": "<div class=\"margin-top\"><div><h4 class=\"\">Масштаб  <i class=\"fas fa-question-circle text-primary icon-tooltip career-tooltip\" data-placement=\"right\" data-toggle=\"tooltip\" title=\"Если мы решим эту проблему, насколько улучшится мир? &lt;a href=&#34;/articles/problem-framework/#how-to-assess-scale&#34;&gt;Подробнее&lt;/a&gt;.\"> </i></h4><div class=\"\"><p>ИИ можно будет применить самыми разными способами, и потенциально он может принести очень много пользы. Однако нас сильно беспокоит вероятность чрезвычайно плохих последствий, в особенности экзистенциальной катастрофы. У нас есть лишь очень приблизительные оценки и мы в них сильно сомневаемся, однако сейчас мы полагаем, что в ближайшие 100 лет риск экзистенциальной катастрофы, вызванной искусственным интеллектом, составляет примерно 10%. Дальнейшие исследования могут значительно изменить эту оценку: некоторые эксперты в области ИИ-рисков полагают, что эта вероятность меньше 0,5%, другие же — что она значительно выше 50%, и мы готовы изменить своё мнение в любую сторону. В целом, сейчас мы считаем, что развитие ИИ представляет собой самый значительный риск к долгосрочному процветанию человечества, чем любая другая известная нам проблема.</p></div></div>",
    "problem-profiles--artificial-intelligence:0080": "<div class=\"margin-top\">",
    "problem-profiles--artificial-intelligence:0082": "#### Недооценённость",
    "problem-profiles--artificial-intelligence:0084": "<div>",
    "problem-profiles--artificial-intelligence:0086": "В 2020 году на уменьшение риска катастрофы от ИИ было потрачено 50 миллионов долларов. При этом на развитие способностей ИИ были потрачены миллиарды.[^:`Сложно точно сказать, сколько именно было потрачено на развитие способностей ИИ — частично из-за нехватки данных, частично из-за вопросов вроде:",
    "problem-profiles--artificial-intelligence:0088": "- Какие именно исследования в области ИИ действительно развивают его опасные способности, которые могут увеличить потенциальный экзистенциальный риск?\n- Считается ли развитием ИИ развитие компьютерных комплектующих или прогресс в сборе данных?\n- Как насчёт улучшений исследовательского процесса в целом или вклада во что-нибудь, что может увеличить инвестиции в развитие ИИ благодаря повышению экономического роста?",
    "problem-profiles--artificial-intelligence:0092": "Самое релевантное значение, которое мы смогли найти, — это расходы DeepMind в 2020 году, которые [согласно их годовому отчёту](https://web.archive.org/web/20221016011531/https://find-and-update.company-information.service.gov.uk/company/07386350/filing-history) составляли примерно 1 миллиард фунтов стерлингов. Мы ожидаем, что большая часть этих расходов — это в том или ином смысле вклад в \"развитие способностей ИИ\", ведь цель DeepMind — создание мощного ИИ общего назначения. (Впрочем, следует заметить, что DeepMind также вкладывается в работу по безопасности ИИ, что может уменьшать экзистенциальный риск.)",
    "problem-profiles--artificial-intelligence:0094": "Если расходы DeepMind — это примерно 10% от всего, что тратится на развитие способностей ИИ, мы получаем оценку примерно 10 миллиардов фунтов стерлингов. (Учитывая, что большинство компаний, занимающихся ИИ, находятся в США, и много усилий по созданию продвинутого ИИ тратятся в Китае, мы считаем, что 10% — это, наверное, неплохая оценка.)",
    "problem-profiles--artificial-intelligence:0096": "В качестве верхней оценки можно взять общий доход в секторе ИИ в 2021 году, который [примерно равнялся 340 миллиардам долларов](https://web.archive.org/web/20221016011608/https://www.idc.com/getdoc.jsp?containerId=prUS48127321).",
    "problem-profiles--artificial-intelligence:0098": "Таким образом, мы считаем, что на развитие способностей ИИ тратится от 1 до 340 миллиардов долларов в год. Даже если предположить, что тратится всего лишь 1 миллиард, это всё равно будет больше чем в 100 раз, чем расходы на снижение рисков от ИИ.`] Хотя мы видим, что эксперты по ИИ всё больше беспокоятся по этому поводу, по нашим оценкам над уменьшением вероятности связанной с ИИ экзистенциальной катастрофы работают лишь 400 человек (90% доверительный интервал — от 200 до 1000).[^:`См. оригинальную сноску, которая начинается в https://t.80000hours.ru/translate/80k/problem-profiles--artificial-intelligence/ru/?checksum=498ab4abcb09a42c и заканчивается в https://t.80000hours.ru/translate/80k/problem-profiles--artificial-intelligence/ru/?checksum=57843e78e5944b87",
    "problem-profiles--artificial-intelligence:0123": "Здесь заканчивается большая сноска, которая начинается в https://t.80000hours.ru/translate/80k/problem-profiles--artificial-intelligence/ru/?checksum=2b67d87bb0d7161b`] Из них, судя по всему, примерно три четверти работают над техническими вопросами безопасности ИИ, а остальные делятся между разработкой стратегий (и других вопросов регулирования) и популяризацией.[^:`Заметим, что до 19 декабря 2022 года на этой странице число работающих над уменьшением экзистенциальных рисков оценивалось в 300 ЭПЗ, из которых две трети работали над техническими вопросами безопасности ИИ, а остальные делились между разработкой стратегий (и других вопросов регулирования) и популяризацией.",
    "problem-profiles--artificial-intelligence:0125": "Это изменение вызвано улучшенной (как мы надеемся!) оценкой, а не значительным увеличением количества исследователей.`]",
    "problem-profiles--artificial-intelligence:0127": "</div></div>",
    "problem-profiles--artificial-intelligence:0129": "<div class=\"margin-top\">",
    "problem-profiles--artificial-intelligence:0131": "#### Разрешимость",
    "problem-profiles--artificial-intelligence:0133": "<div>",
    "problem-profiles--artificial-intelligence:0135": "Похоже, добиться прогресса в предотвращении связанной с ИИ катастрофы довольно сложно, однако есть много направлений для дальнейших исследований и работа в этой области ведётся не так давно. Поэтому мы считаем, что эта проблема умеренно разрешима, хотя здесь мы очень не уверенны: повторимся, оценки разрешимости проблемы \"сделать безопасный ИИ\" различаются многократно.",
    "problem-profiles--artificial-intelligence:0137": "</div></div></div></div>",
    "problem-profiles--artificial-intelligence:0139": "<div><div class=\"row small\"><div class=\"col-sm-4 margin-top-smaller\"><h5>Проработанность профиля</h5><p>Глубокая <i class=\"fas fa-question-circle text-primary icon-tooltip career-tooltip\" data-placement=\"right\" data-toggle=\"tooltip\" title=\"Мы опросили как минимум десять людей с релевантным опытом в этой области, прочитали все лучшие существующие исследования, которые смогли найти, провели тщательное исследование практически всех основных причин нашей неуверенности, после чего описали всё, что мы нашли.\"> </i></p></div></div></div></div>",
    "problem-profiles--artificial-intelligence:0141": "<div class=\"no-print well margin-top\">",
    "problem-profiles--artificial-intelligence:0143": "<small>Этот профиль — как и многие другие — мы написали, чтобы помочь людям разобраться, какие самые важные проблемы они могут решить в процессе своей карьеры. Ты можешь <a href=\"/career-guide/most-pressing-problems/\" data-index=\"11\">прочитать подробнее</a> о том, как мы сравниваем различные проблемы, узнать, как мы пытаемся <a href=\"/articles/problem-framework/\" data-index=\"12\">оценивать их численно</a>, и увидеть, <a href=\"/problem-profiles/\" data-index=\"13\">насколько важна эта проблема по сравнению с другими</a>, которые мы уже рассмотрели.</small>",
    "problem-profiles--artificial-intelligence:0145": "</div></div>",
    "problem-profiles--artificial-intelligence:0147": "<div class=\"problem-profile-content padding-bottom-large\">\n!toc-container",
    "problem-profiles--artificial-intelligence:0150": "<div class=\"well bg-gray-lighter margin-bottom margin-top padding-top-small padding-bottom-small\">",
    "problem-profiles--artificial-intelligence:0152": "**Примечание автора:** В сущности в этом профиле мы пытаемся предсказать будущее одной из технологий, а это заведомо сложная задача. Кроме того, по вопросам рисков от ИИ существует гораздо меньше строгих исследований, чем по другим проблемам, о которых писали \"80 000 часов\" (например, чем про [пандемии](/preventing-catastrophic-pandemics/) или [изменение климата](/problem-profiles/climate-change/)).[^:`Сложно понять, как следует реагировать на упомянутое отсутствие исследований. Возможно, нам следует беспокоиться меньше, потому что это свидетельство в пользу того, что исследователи выбирают не заниматься этим риском (и следовательно этот риск не велик — в предположении, что исследователи предпочитают заниматься более существенными рисками), или нам следует беспокоиться больше, потому что, похоже, этот риск [гораздо более недоооценён](#neglectedness).",
    "problem-profiles--artificial-intelligence:0154": "Бен Гарфинкель — исследователь из [Центра регулирования ИИ](https://www.governance.ai/) — указывает, что беспокойство в сообществе по поводу различных экзистенциальных рисков в какой-то степени коррелирует с тем, насколько сложно проанализировать риск. Он развивает свою мысль так:",
    "problem-profiles--artificial-intelligence:0156": "> Из этого вовсе не следует, что сообщество ведёт себя иррационально, когда беспокоится о незаалайненном ИИ значительно больше, чем о других потенциальных рисках. Такое беспокойство прекрасно соотносится с примерно следующим подходом: \"Если бы я более чётко понимал, какие риски таит в себе незаалайненный ИИ, вероятно, я бы осознал, что это не слишком большая проблема. Однако на самом деле я это не понимаю достаточно чётко. Поэтому, в отличие от ситуации с изменением климата, я не могу исключить малую вероятность того, что дополнительная информация обеспокоит меня ещё больше. Таким образом я должен беспокоиться о незаалайненном ИИ больше, чем о других рисках. Мне следует сосредоточить мои усилия на этой проблеме, даже если стороннему наблюдателю впоследствии может показаться, что это было зря\".",
    "problem-profiles--artificial-intelligence:0158": "Полностью пост Гарфинкеля можно прочитать [здесь](https://web.archive.org/web/20221016004022/https://forum.effectivealtruism.org/posts/M68oj7fwXoPFJisap/we-should-expect-to-worry-more-about-speculative-risks).`] Как уже упоминалось, я пытаюсь описать предмет растущей области исследований. При написании этой статьи я в первую очередь основывался на [предварительном докладе](https://doi.org/10.48550/arXiv.2206.13353) Джозефа Карлсмита из [Open Philanthropy](https://www.openphilanthropy.org/), который оказался самым детальным обзором по этой теме из всех, что я смог найти. Также эту статью проверили [больше 30 человек — с разной специализацией и разными мнениями по данной теме](#acknowledgements). (Почти всех из них беспокоит потенциальное воздействие продвинутого ИИ на мир.)",
    "problem-profiles--artificial-intelligence:0160": "Если вы хотите дать какую-либо обратную связь по поводу этой статьи — например, если вы заметили какие-то технические детали, в которых мы ошиблись, или вы считаете, что какие-то формулировки можно было бы улучшить, или даже если вам просто понравилась или не понравилась эта статья — мы будем рады, если вы сообщите нам, что вы думаете, с помощью [этой формы](https://forms.gle/RRZaFTfdDkSQ6fJG8).",
    "problem-profiles--artificial-intelligence:0162": "</div>",
    "problem-profiles--artificial-intelligence:0164": "Почему мы считаем, что снижение рисков от ИИ — одна из важнейших задач нашего времени? Вкратце, наши основания таковы:",
    "problem-profiles--artificial-intelligence:0166": "1. Даже не вдаваясь в настоящую аргументацию, мы видим, что у нас есть некоторые причины для беспокойства: [многие эксперты по ИИ считают, что есть малая, однако существенная, вероятность появления ИИ, который приведёт к плохим последствиям вплоть до исчезновения человечества](#experts-are-concerned).\n2. [Мы развиваем ИИ чрезвычайно быстро](#making-advances-extremely-quickly), и это означает, что системы ИИ смогут влиять на общество и очень скоро.\n3. Есть сильные доводы в пользу того, что [\"ищущий власти\" ИИ может представлять экзистенциальную угрозу для человечества](#power-seeking-ai)[^:` В [опросе 2020 года](https://web.archive.org/web/20221016004901/https://www.alignmentforum.org/posts/WiXePTj7KeEycbiwK/survey-on-ai-existential-risk-scenarios) исследователей, работающих над снижением экзистенциальных рисков от ИИ, просили указать, каких именно рисков они опасаются больше всего. Авторы опроса спрашивали о пяти источниках экзистенциального риска:",
    "problem-profiles--artificial-intelligence:0170": "- Риски от сверхразумного ИИ (похожие на сценарий, который мы описали [здесь](/articles/what-could-an-ai-caused-existential-catastrophe-actually-look-like/#superintelligence))\n- Риски, связанные с накоплением влияния\n- Риски от систем ИИ, преследующих легко измеримые цели (похожие на сценарий, который мы описали [here](/articles/what-could-an-ai-caused-existential-catastrophe-actually-look-like/#getting-what-you-measure))\n- Применение ИИ в [военных](#artificial-intelligence-and-war) целях\n- [Другое](#other-risks) умышленное использование ИИ во вред, не связанное с войной",
    "problem-profiles--artificial-intelligence:0176": "Опрошенные исследователи беспокоились по поводу всех этих рисков примерно в равной степени. Первые три из них освещаются в этой статье в секции, посвящённой [ИИ, ищущему власти](#power-seeking-ai), два оставшихся — в секции про [другие риски](#other-risks). Если такая группировка имеет смысл (мы полагаем, что да), это означает, что в то время, когда проводился опрос, исследователей в три раза больше волновал обобщённый риск ищущего власти ИИ, чем риски военного применения или риски иного умышленного вредоносного использования по отдельности.`], которые мы рассмотрим ниже.\n4. [Если мы даже решим проблему поиска власти, есть и другие риски](#other-risks).\n5. Мы считаем, что [со всеми этими рисками можно работать](#we-can-tackle-these-risks).\n6. [Эта работа чрезвычайно недооценена](#neglectedness).",
    "problem-profiles--artificial-intelligence:0181": "Мы по очереди разберём каждый из этих пунктов, затем рассмотрим [лучшие из контраргументов](#best-arguments-against-this-problem-being-pressing), расскажем, [что конкретно вы можете сделать, чтобы помочь](#what-can-you-do-concretely-to-help), и в конце перечислим [некоторые из лучших ресурсов, где вы можете узнать больше об этой области](#top-resources-to-learn-more).",
    "problem-profiles--artificial-intelligence:0183": "## 1\\. Многие эксперты по ИИ считают, что есть существенная вероятность появления ИИ, который приведёт к плохим последствиям вплоть до исчезновения человечества",
    "problem-profiles--artificial-intelligence:0185": "Если ты считаешь, что некая новая технология приведёт к какому-то прорыву (и возможно даже приведёт к исчезновению человечества), но все, кто реально работают над этой технологией, думают, что твои опасения беспочвенны), то, вероятно, ты что-то упускаешь.",
    "problem-profiles--artificial-intelligence:0187": "Поэтому перед тем, как переходить к аргументам по поводу рисков от ИИ, давай посмотрим, что думают эксперты.",
    "problem-profiles--artificial-intelligence:0189": "Мы изучили три опроса исследователей ИИ, работы которых публиковались в тезисах конференций NeurIPS и ICML (две самые престижные конференции, посвящённые машинному обучению). Опросы проводились в 2016, 2019 и 2022 годах. [^:`Речь идёт о следующих опросах:",
    "problem-profiles--artificial-intelligence:0191": "- [Стейн-Перлман и другие (2022)](https://web.archive.org/web/20221016004611/https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) (доступны только предварительные результаты)\n- [Чжан и другие (2022)](https://doi.org/10.48550/arXiv.2206.04132)\n- [Грейс и другие (2018)](https://doi.org/10.1613/jair.1.11222)",
    "problem-profiles--artificial-intelligence:0195": "Во всех трёх работах опрашивались исследователи, работы которых публиковались в материалах конференций NeurIPS и ICML.",
    "problem-profiles--artificial-intelligence:0197": "Стейн-Перлман и соавторы опросили 4271 исследователя, которые публиковались на конференции 2021 года (все исследователи были случайно разбиты на две группы: первая получила опросник Стейн-Перлмана и соавторов, вторая — опросник других людей), и получили 738 ответов (доля ответов — 17%).",
    "problem-profiles--artificial-intelligence:0199": "Чжан и соавторы связались с 2652 авторами, работы которых публиковались в материалах конференций 2018 года, и получили 524 ответа (доля ответов — 20%). Впрочем, из-за технической ошибки можно было использовать лишь 296 ответов.",
    "problem-profiles--artificial-intelligence:0201": "Грейс и соавторы связались с 1634 авторами, работы которых публиковались в материалах конференций 2015 года, и получили 352 ответа (доля ответов — 21%)`]",
    "problem-profiles--artificial-intelligence:0203": "Важно помнить, что в подобных опросах может быть значительная ошибка выборки. Например, не исключено, что исследователи, участвующие в престижнейших конференциях по ИИ, более оптимистичны по отношению к ИИ, поскольку они привыкли считать, что исследования ИИ — это полезное дело. Или наоборот, возможно, исследователи, у которых есть опасения по поводу ИИ, более склонны отвечать на опросы о таких опасениях.[^:`Катя Грейс, проводившая опросы 2016 и 2022 года, [в своём блоге писала](https://web.archive.org/web/20221016004704/https://aiimpacts.org/some-survey-results/), что переформулировка вопроса значительно меняет получаемые ответы:",
    "problem-profiles--artificial-intelligence:0205": "> Люди стабильно занижают прогнозы, если их спрашивать о вероятности события через N лет, по сравнению с вопросом, в каком году вероятность наблюдать событие будет M. Мы наблюдали этот эффект и на стандартном вопросе о высокоуровневом машинном интеллекте, и на вопросах о большинстве задач и профессий, и на множестве других штук, которые мы проверяли на \"[механических турках](https://ru.wikipedia.org/wiki/Amazon_Mechanical_Turk)\" раньше. Например, если спрашивать, когда появится высокоуровневый машинный интеллект, медианный ответ будет 40 лет, однако если спросить, какова вероятность высокоуровневого машинного интеллекта через 40 лет, то медианный ответ будет 30%.",
    "problem-profiles--artificial-intelligence:0207": "Из [нашего интервью с Катей](/podcast/episodes/katja-grace-forecasting-technology/) вы можете узнать больше о возможных проблемах опроса 2016 года.`]",
    "problem-profiles--artificial-intelligence:0209": "Тем не менее, вот что мы обнаружили:",
    "problem-profiles--artificial-intelligence:0211": "Во всех трёх опросах по мнению медианного исследователя вероятность того, что последствия появления ИИ окажутся \"чрезвычайно хорошими\", довольно высока: 20% в опросе 2016 года, 20% в опросе 2019 года, и 10% — в опросе 2020 года.[^:`Под \"по мнению медианного исследователя вероятность _x_%\" мы подразумеваем \"около половины исследователей считают, что вероятность больше или равна _x_%\".`]",
    "problem-profiles--artificial-intelligence:0213": "И в самом деле ИИ уже приносят значительную пользу: например, при [уходе за больными](https://web.archive.org/web/20221013010307/https://www.deepmind.com/blog/announcing-deepmind-health-research-partnership-with-moorfields-eye-hospital) или в [научных исследованиях](https://ought.org/elicit).",
    "problem-profiles--artificial-intelligence:0215": "Однако также во всех трёх опросах медианный исследователь считал, что существует небольшая, но тем не менее существенная вероятность того, что последствия появления ИИ окажутся \"чрезвычайно плохими (например, исчезновение человечества)\": в 2016 году такая вероятность оценивалась в 5%, в 2019 — в 2%, в 2022 — в 5%.[^:`48% респондентов опроса 2022, проведённого Стейн-Перлманом и соавторами, считали, что вероятность \"чрезвычайно плохого (например, исчезновение человечества)\" исхода — 10% или выше. Некоторые респонденты беспокоились гораздо меньше: 25% респондентов оценили вероятность чрезвычайно плохого исхода как 0%`] [^:`В опросах Стейна-Перлмана и соавторов и Грейс и соавторов исследователей спрашивали о \"высокоуровневом машинном интеллекте\". Он определялся как:",
    "problem-profiles--artificial-intelligence:0217": "> Когда машина без посторонней помощи сможет выполнить любую задачу лучше, чем человек, и затраты при этом будут меньше, чем затраты на труд человека. Игнорируйте такие задачи, в которых \"быть человеком\" — это ключевое преимущество, например, задачу \"стать членом жюри присяжных\". _Важна принципиальная возможность выполнить работу, а не принятие в качестве работника._",
    "problem-profiles--artificial-intelligence:0219": "В опросе, проводимом Чжаном и соавторами, исследователей спрашивали о \"машинном интеллекте человеческого уровня\", который определялся как:",
    "problem-profiles--artificial-intelligence:0221": "> Машинный интеллект считается достигнувшим человеческого уровня, если машины коллективно способны выполнять практически любые задачи (>90% от всех задач), которые имеют смысл с экономической точки зрения \\*лучше чем медианный оплачиваемый сотрудник выполняет эту работу в 2019 году. Игнорируйте задачи, которые по юридическим или культурным соображениям могут выполнять только люди, такие как участие в жюри присяжных. Под задачами мы подразумеваем все, что упомянуты в базе данных Occupational Information Network (O\\*NET). (O\\*NET) — это часто используемая база данных, описывающая какие задачи нужно решать на той или иной работе.",
    "problem-profiles--artificial-intelligence:0223": "Исследователей спрашивали:",
    "problem-profiles--artificial-intelligence:0225": "> Для целей этого вопроса предположим, что машинный интеллект человеческого уровня в какой-то момент появился. Насколько положительными или отрицательными, с вашей точки зрения, будут последствия для человечества от его появления в долгосрочной перспективе?\n>\n> Пожалуйста ответьте, указав вероятности для указанных ниже вариантов последствий (сумма вероятностей должна давать 100%):\n>\n> - Чрезвычайно хорошие (например, стремительное процветание человечества) (2)\n> - В целом хорошие (1)\n> - Более или менее нейтральные (0)\n> - В целом плохие (-1)\n> - Чрезвычайно плохие (например, исчезновение человечества) (-2)",
    "problem-profiles--artificial-intelligence:0235": "Для каждого опроса посчитали итоговую функцию плотности распределения вероятности появления машинного интеллекта человеческого уровня на основе средних или медианных оценок в опросе. Эти функции дали следующие вероятности появления машинного интеллекта человеческого уровня:",
    "problem-profiles--artificial-intelligence:0237": "- 50% к 2059 (Стейн-Перлман и соавторы, оценка по среднему)\n- 75% by 2080 (Чжан и соавторы, оценка по медиане)\n- 65% by 2080 (Чжан и соавторы, оценка по среднему)\n- 75% by 2116 (Грейс и соавторы, оценка по среднему)",
    "problem-profiles--artificial-intelligence:0242": "Это означает, что ответ, который мы цитируем, близок, но тем не менее отличается от ответа на вопрос: \"Без обязательного предположения о том, что машинный интеллект человеческого уровня будет существовать в ближайшие сто лет, насколько положительными или отрицательными, с вашей точки зрения, будут результаты его появления для человечества в ближайшие сто лет?\" Более подробно прогнозы экспертов по поводу появления ИИ мы рассмотрим в разделе о том, [когда стоит ожидать ИИ, способного изменить мир](#when-can-we-expect-to-develop-transformative-AI).`]",
    "problem-profiles--artificial-intelligence:0244": "В опросе 2022 года участникам задали отдельный вопрос про вероятность экзистенциальной катастрофы, вызванной будущим прогрессом в области ИИ. И снова _больше половины исследователей считали, что эта вероятность больше 5%_.[^:`Если быть точным, Стейн-Перлман и соавторы (2022) спрашивали участников:",
    "problem-profiles--artificial-intelligence:0246": "> Какую вероятность вы присваиваете тому, что будущее развитие ИИ приведёт к исчезновению человечества или аналогичной необратимой и значительной потери возможности для человеческого вида управлять своей судьбой?",
    "problem-profiles--artificial-intelligence:0248": "Эта формулировка эквивалентна определению [экзистенциальной катастрофы](/articles/existential-risks/), которое обычно используем мы. Также она похожа на определение экзистенциальной катастрофы, которое дал Орд в [\"Пропасти\" (2020)](/the-precipice/):",
    "problem-profiles--artificial-intelligence:0250": "> \"Экзистенциальная катастрофа\" — это уничтожение потенциала человечества в долгосрочной перспективе.",
    "problem-profiles--artificial-intelligence:0252": "Орд относит к экзистенциальным рискам как риски исчезновения, так и риски \"невозможности дальнейшего развития\" (здесь Орд приводит пример появления [стабильного тоталитарного режима](/problem-profiles/risks-of-stable-totalitarianism/)). Мы считаем, что необратимая и значительная потеря возможности для человеческого вида управлять своей судьбой — это вариант \"невозможности дальнейшего развития\" в рамках определения Орда.",
    "problem-profiles--artificial-intelligence:0254": "Стейн-Перлман и соавторы также спросили участников именно о [тех видах рисков, о которых мы больше всего беспокоимся](#power-seeking-ai):",
    "problem-profiles--artificial-intelligence:0256": "> Какую вероятность вы присваиваете тому, что люди не смогут контролировать будущее развитие систем ИИ, которое приведёт к исчезновению человечества или аналогичной необратимой и значительной потери возможности для человеческого вида управлять своей судьбой?",
    "problem-profiles--artificial-intelligence:0258": "Медианный ответ на этот вопрос был 10%.",
    "problem-profiles--artificial-intelligence:0260": "Стейн-Перлман замечает:",
    "problem-profiles--artificial-intelligence:0262": "> Этот вопрос более конкретен, то есть, здесь речь идёт о менее вероятном событии, чем в предыдущем вопросе, однако медианная вероятность оказалась выше. Возможно, это результат шума — этот вопрос получили разные случайные подмножества респондентов, поэтому их ответы не обязаны сочетаться друг с другом логичным образом. Или это результат [эвристики репрезентативности](https://en.wikipedia.org/wiki/Representativeness_heuristic).`]",
    "problem-profiles--artificial-intelligence:0264": "Итак, эксперты расходятся в оценках того, насколько ИИ представляет собой экзистенциальный риск — вид угрозы, которая, [как мы утверждаем](/articles/existential-risks/), чрезвычайно важна с этической точки зрения.",
    "problem-profiles--artificial-intelligence:0266": "Это соотносится с нашими представлениями о текущем прогрессе в области исследования ИИ. В двух из ведущих лабораторий по разработке ИИ — DeepMind и OpenAI — также есть команды, задача которых понять, как решить вопросы безопасности, которые, как мы считаем по причинам изложенным ниже, ведут к [экзистенциальной угрозе](/articles/existential-risks/) для человечества.[^:`[Команда по безопасности](https://web.archive.org/web/20221016004051/https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1) в DeepMind и [команда алайнмента](https://openai.com/alignment/) в OpenAI исследуют технические вопросы безопасности ИИ, некоторые из которых могут уменьшить риски, обсуждаемые в этой статье. Мы беседовали с исследователями из обеих этих команд, и эти исследователи говорили нам, что они считают, что искусственный интеллект таит в себе самый значительный экзистенциальный риск для человечества в этом столетии, и их исследования направлены на снижение этого риска. На эту же тему:",
    "problem-profiles--artificial-intelligence:0268": "- В 2011 году Шейн Легг — сооснователь и руководитель научных исследований DeepMind — [заявил, что](https://web.archive.org/web/20221016004215/https://www.lesswrong.com/posts/No5JpRCHzBrWA4jmS/q-and-a-with-shane-legg-on-risks-from-ai) для него ИИ \"находится на первом месте среди \\[экзистенциальных\\] рисков этого столетия, незначительно опережая искусственно созданные биологические патогены\".\n- Сэм Альтман — сооснователь и генеральный директор OpenAI — неоднократно высказывал опасения по этому поводу, хотя, судя по всему, он очень оптимистичен по поводу суммарных последствий появления ИИ. Например, в своём [интервью 2021 года Эзре Кляйну](https://web.archive.org/web/20221016004421/https://www.nytimes.com/2021/06/11/podcasts/transcript-ezra-klein-interviews-sam-altman.html) в ответ на вопрос о стимулах, связанных с созданием ИИ, он ответил, что существующие механизмы решают много проблем, однако \"остаётся одна, которая меня беспокоит больше всего — во всей области, не только в нашей компании. Речь о том, что мы приближаемся к сверхмощным системам, вроде тех, что создают экзистенциальные риски для человечества по мнению некоторых людей\".\n- Для подкаста \"80 000 часов\" мы брали интервью у некоторых ведущих исследователей из этих организаций, например у [Дарио Амодея, бывшего вице-президента OpenAI, отвечавшего за исследования](/podcast/episodes/the-world-needs-ai-researchers-heres-how-to-become-one/) (сейчас он сооснователь и генеральный директор Anthropic, ещё одной лаборатории, создающей ИИ), [Яна Лейке, бывшего исследователя из DeepMind](/podcast/episodes/jan-leike-ml-alignment/)(сейчас он глава команды Алайнмента в OpenAI), [Джека Кларка, Аманды Аскелл и Майлза Брандиджа из команды OpenAI по регулированию](/podcast/episodes/openai-askell-brundage-clark-latest-in-ai-policy-and-strategy/) (Кларк сейчас сооснователь Anthropic, Аскелл — работает над техническими вопросами в Anthropic, а Брандидж возглавляет исследование вопросов регулирования в OpenAI). Все они выражали беспокойство по поводу последствий для будущего человечества со стороны ИИ.`]",
    "problem-profiles--artificial-intelligence:0272": "Есть и другие научные исследовательские группы (например, в [МТИ](https://people.csail.mit.edu/dhm/), [Оксфорде](https://www.fhi.ox.ac.uk/research/research-areas/#aisafety_tab), [Кэмбридже](https://www.davidscottkrueger.com/), [университете Карнеги — Меллона](https://www.cs.cmu.edu/~focal/) и [Калифорнийском университете в Беркли](https://humancompatible.ai/)), которые занимаются теми же техническими проблемами безопасности ИИ.[^:`В [списке профессоров, которые сказали, что они работают над вопросом безопасности ИИ, потому что считают, что эта работа уменьшает экзистенциальный риск]((https://futureoflife.org/team/ai-existential-safety-community/) есть представители всех упомянутых исследовательских групп. Список ведётся [Институтом будущего жизни](https://futureoflife.org). В списке есть и учёные из других университетов.`]",
    "problem-profiles--artificial-intelligence:0274": "Сложно точно понять, какой отсюда следует сделать вывод, однако, с нашей точки зрения, это показывает, что мнение о том, что есть существенный риск плохого исхода вплоть до экзистенциальной катастрофы, не является маргинальным среди специалистов по данному вопросу.",
    "problem-profiles--artificial-intelligence:0276": "Тем не менее, остаётся вопрос: почему мы на стороне тех, кто обеспокоен больше? Если вкратце, то потому что существуют убеждающие нас аргументы о том, что ИИ действительно может представлять собой экзистенциальную угрозу. Эти аргументы мы по порядку изложим далее.",
    "problem-profiles--artificial-intelligence:0278": "Важно понимать, что из того, что многие эксперты признают существование некоторой проблемы, не следует, что всё в порядке, у экспертов всё под контролем. В целом мы считаем, что эта проблема чрезвычайно недооценена, над ней напрямую работает всего лишь примерно 300 человек в мире (более подробно читайте [далее](#neglectedness)).",
    "problem-profiles--artificial-intelligence:0280": "И в то же время на развитие ИИ ежегодно тратятся миллиарды долларов.[^:`См оригинальную сноску 4, перевод которой начинается в https://t.80000hours.ru/translate/80k/problem-profiles--artificial-intelligence/ru/?checksum=d33739d9296f97bf и заканчивается в https://t.80000hours.ru/translate/80k/problem-profiles--artificial-intelligence/ru/?checksum=2b67d87bb0d7161b",
    "problem-profiles--artificial-intelligence:0294": "##~id=\"making-advances-extremely-quickly\"~ 2\\. Мы развиваем ИИ чрезвычайно быстро",
    "problem-profiles--artificial-intelligence:0296": "![Два кота, одетые как программисты, созданные разными программами ИИ](https://80000hours.org/wp-content/uploads/2022/06/dalle_cat-1.png)[`\"Кот, одетый как программист\", созданный Craiyon (в прошлом DALL-E mini) (слева) и DALL-E 2 от OpenAI (справа). DALL-E mini выпущена в январе 2021 года и использует модель в 27 раз меньше, чем DALL-E 1 от OpenAI. DALL-E 2 выпущена в апреле 2022 года. `]",
    "problem-profiles--artificial-intelligence:0298": "Прежде чем мы попытаемся разобраться, как может выглядеть будущее ИИ, полезно посмотреть: а что ИИ уже умеет?",
    "problem-profiles--artificial-intelligence:0300": "В число современных способов создания ИИ входит [\"машинное обучение\"](https://ru.wikipedia.org/wiki/%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5) (ML), которое основано на том, что модели автоматически улучшаются за счёт входных данных. Наиболее популярная сейчас форма машинного обучения известна как [глубокое обучение](https://ru.wikipedia.org/wiki/%D0%93%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5).",
    "problem-profiles--artificial-intelligence:0302": "<aside class=\"well well-person pull-right clearfix   padding-top-small padding-bottom-small\">\n<h4 class=\"no-margin-top\">Что такое глубокое обучение</h4>",
    "problem-profiles--artificial-intelligence:0305": "<a id=\"what-is-deep-learning\" class=\"link-anchor\"></a>",
    "problem-profiles--artificial-intelligence:0307": "Модели машинного обучения берут какие-то данные на вход и выдают какие-то данные на выход, причём выход зависит от параметров модели, которые получаются в результате автоматического обучения, а не задаются программистами.",
    "problem-profiles--artificial-intelligence:0309": "Подавляющая часть недавних прорывов в машинном обучении использует \"нейронные сети\". Нейронная сеть преобразует входные данные в выходные данные, прогоняя их через несколько скрытых \"слоёв\" простых вычислений, где каждый слой состоит из \"нейронов\". Каждый нейрон получает данные с предыдущего слоя, производит какие-то вычисления на основании своих параметров (в сущности, каких-то чисел, зависящих от этого нейрона) и передаёт результат на следующий слой.",
    "problem-profiles--artificial-intelligence:0311": "!img~ class=\"alignright\" ~[Нейронная сеть с одним скрытым слоем](https://80000hours.org/wp-content/uploads/2022/06/768px-Neural_network_example.svg_.png)",
    "problem-profiles--artificial-intelligence:0313": "Инженеры, разрабатывающие сеть, выбирают меру успеха для этой сети (известную как [\"функция потерь\" или \"целевая функция\"](https://en.wikipedia.org/wiki/Loss_function)). Насколько сеть успешна (в соответствии с выбранной мерой), определяется точным значением параметров для каждого нейрона сети.",
    "problem-profiles--artificial-intelligence:0315": "Дальше сеть \"тренируется\" на большом количестве данных. Каждый раз, когда сеть проверяется на данных с помощью функции потерь, параметры каждого нейрона немного меняются с помощью некоторого оптимизационного алгоритма (в подавляющем большинстве случаев используется [стохастический градиентный спуск](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA). (Обычно) оптимизационный алгоритм приводит к тому, что после изменения параметров нейронная сеть работает чуть лучше. В итоге инженеры получают нейронную сеть, которая работает довольно неплохо относительно выбранной меры.",
    "problem-profiles--artificial-intelligence:0317": "\"Глубокое обучение\" означает, что в используемой нейронной сети много слоёв.",
    "problem-profiles--artificial-intelligence:0319": "Для дальнейшего знакомства с темой мы рекомендуем:",
    "problem-profiles--artificial-intelligence:0321": "- [Цикл на Ютубе про нейронные сети от канала 3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi). Это прекрасное введение в формате видео.\n- Ричард Нго, \"[Краткое введение в машинное обучение](https://web.archive.org/web/20221013010903/https://www.alignmentforum.org/posts/qE73pqxAZmeACsAdF/a-short-introduction-to-machine-learning)\" — короткий пост в блоге с обзором этой темы.\n- Вишал Майни и Самер Сабри, [Машинное обучение для людей](https://web.archive.org/web/20221013010851/https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12) — длинное, но доступное введение в машинное обучение.",
    "problem-profiles--artificial-intelligence:0325": "</aside>",
    "problem-profiles--artificial-intelligence:0327": "Сегодня ML-системы могут выполнять [лишь очень малую долю](https://web.archive.org/web/20221013010946/https://www.alexirpan.com/2018/02/14/rl-hard.html) задач, которые способны решать люди. И (за редким исключением) у них у всех очень узкая специализация (например, играть в одну конкретную игру или создавать конкретный тип картинок).",
    "problem-profiles--artificial-intelligence:0329": "Тем не менее, после [массового распространения глубокого обучения в середине 2010-х](https://en.wikipedia.org/wiki/Deep_learning#Deep_learning_revolution) ML-системы научились очень-очень многому. Вот краткий перечень лишь некоторых достижений, которые мы увидели начиная с 2019 года:",
    "problem-profiles--artificial-intelligence:0331": "- [AlphaStar](https://web.archive.org/web/20221013011100/https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii), обыгравшая лучших профессиональных игроков в StarCraft II (январь 2019)\n- [MuZero](https://web.archive.org/web/20221013011105/https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules), единая система, которая научилась выигрывать в шахматы, [сёги](https://ru.wikipedia.org/wiki/%D0%A1%D1%91%D0%B3%D0%B8), and [го](https://ru.wikipedia.org/wiki/%D0%93%D0%BE) — хотя ей даже не объясняли правила (ноябрь 2019)\n- [GPT-3](https://en.wikipedia.org/wiki/GPT-3), модель естественного языка, создающая высококачественный текст (май 2020)\n- [GPT-f](https://web.archive.org/web/20221013011121/https://openai.com/blog/formal-math/), решающая некоторые задачи математических олимпиад (сентябрь 2020)\n- [AlphaFold 2](https://en.wikipedia.org/wiki/AlphaFold), огромный шаг вперёд в решении очень давней [задачи фолдинга белка](https://web.archive.org/web/20221011212305/https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology) (июль 2021)\n- [Codex](https://en.wikipedia.org/wiki/OpenAI_Codex), создающий программный код на основе команд на естественном языке (август 2021)\n- [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html), языковая модель, продемонстрировавшая впечатляющие способности рассуждать о причинах и следствиях, а также объяснять шутки (апрель 2022)\n- [DALL-E 2](https://openai.com/dall-e-2/) (April 2022) и [Imagen](https://imagen.research.google/) (май 2022), две модели, способные создавать высококачественное изображение на основе словесного описания\n- [SayCan](https://say-can.github.io/), управляющая роботом на основе команд на естественном языке (апрель 2022)\n- [Gato](https://web.archive.org/web/20221013011518/https://www.deepmind.com/publications/a-generalist-agent), единая ML-модель, способная выполнять множество разных задач(например, играть в Атари, давать названия картинкам, общаться в чатах и складывать блоки с помощью реальной роботизированной руки) — решение о том, что именно нужно дать на выход, принимается на основе контекста (май 2022)\n- [Minerva](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html) способна решать сложные математические задачи. Она неплохо справляется на уровне колледжа и ещё лучше на уровне математических соревнований для старшеклассников. (Minerva оказалась гораздо более успешной, [чем предсказывали прогнозисты в 2021 году](https://web.archive.org/web/20221013011528/https://bounded-regret.ghost.io/ai-forecasting-one-year-in/).)",
    "problem-profiles--artificial-intelligence:0343": "Если вы чем-то похожи на нас, вас тоже удивила сложность и разнообразность задач, которые решают эти системы.",
    "problem-profiles--artificial-intelligence:0345": "И если технологии и дальше будут развиваться с той же скоростью, видимо, это очень сильно повлияет на общество. Уж точно можно ожидать, что автоматизация соответствующих задач сделает их выполнение дешевле. В итоге мы увидим значительный экономический рост (возможно даже сравнимый с ростом [во время промышленной революции](https://web.archive.org/web/20221013011653/https://www.britannica.com/topic/productivity/Historical-trends)).",
    "problem-profiles--artificial-intelligence:0347": "Если у нас получится частично или полностью [автоматизировать получение новых научных результатов](https://web.archive.org/web/20221013011707/https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/), возможно, общество и технологии изменятся ещё более радикально.[^:`Экономисты называют технологии, которые влияют на всю экономику в целом \"[технологиями общего назначения](https://docs.google.com/document/d/1I13_0o3kUe1AVQNfevOF9sHpc4mCQkuFDxOXFj_4g-I/)\". В сущности мы здесь утверждаем, что ИИ, вероятно, тоже станет технологией общего назначения (как, например, паровой двигатель или электричество).",
    "problem-profiles--artificial-intelligence:0349": "Сказать, что именно станет технологией общего назначения, иногда нелегко. Например, потребовалось [200 лет](https://doi.org/10.1086/ahr/84.1.159), чтобы паровой двигатель начали использовать не только для откачивания воды из шахт.",
    "problem-profiles--artificial-intelligence:0351": "Тем не менее экономисты всё больше считают, что ИИ — это довольно неплохой кандидат на технологию общего назначения, поскольку у его появления будет настолько много последствий.",
    "problem-profiles--artificial-intelligence:0353": "Довольно вероятно, что [множество работ будут автоматизированы](https://web.archive.org/web/20221016005338/https://www.technologyreview.com/2018/01/25/146020/every-study-we-could-find-on-what-automation-will-do-to-jobs-in-one-chart/). Способность ИИ ускорить развитие новых технологий [может значительно повлиять на нашу экономику](https://web.archive.org/web/20221013011707/https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/), но при этом таит в себе риск потенциального появления [опасной новой технологии](#dangerous-new-technology).",
    "problem-profiles--artificial-intelligence:0355": "Влияние ИИ на экономику может увеличить неравенство. Владельцы индустрий, основанных на использовании ИИ, могут оказаться намного богаче, чем остальное общество — подробности, например, можно прочитать в статье Коринека и Стиглица \"[Искусственный интеллект и его следствия на распределение доходов и безработицу](https://dx.doi.org/10.3386/w24174)\"(2017):",
    "problem-profiles--artificial-intelligence:0357": "> Неравенство — это одна из основных проблем, которую повлечёт массовое распространение искусственного интеллекта (ИИ) и других видов технологического прогресса, которые могут заменить человека на рабочем месте. В этой статье предлагается систематизация связанных с этим экономических вопросов. Во-первых, мы обсудим общие условия, при которых новые технологии — такие, как ИИ — могут улучшить эффективность по Парето. Во-вторых, мы в общих чертах опишем две главные причины, влияющие на неравенство — прибыль, появляющаяся у применяющих новые технологии, и перераспределение богатства, вызванное изменением цены ресурсов. В-третьих, мы предложим несколько простых экономических моделей, описывающих, какая политика может противодействовать этим эффектам — даже в случае \"сингулярности\", когда значительная часть труда станет трудом машин. При правдоподобных условиях потери тех, кто потенциально может пострадать, можно компенсировать введением неискажающих налогов. В-четвёртых, мы рассмотрим два основных фактора, посредством которых технологический прогресс может привести к технологической безработице — эффект \"эффективных зарплат\" и быстрые изменения рынка труда. В конце мы порассуждаем о том, как технологии, порождающие сверх-человеческий интеллект, могут влиять на неравенство и как спасти человечество от мальтузианской ловушки, в которую оно может попасть.",
    "problem-profiles--artificial-intelligence:0359": "Системы ИИ уже дискриминируют группы, находящиеся в неблагоприятном социальном положении. Например, исследование [Суини (2013)](https://dx.doi.org/10.1145/2460276.2460278) обнаружило, что две поисковые системы непропорционально часто показывают рекламу с предположением о наличии истории задержаний, когда люди ищут по именам, ассоциирующимися с темнокожими людьми. А [Али и соавторы (2019)](https://dx.doi.org/10.1145/3359301) пишут про рекламу Фейсбука следующее:",
    "problem-profiles--artificial-intelligence:0361": "> Рассматривалась гипотеза, что этот процесс может исказить показ рекламы незапланированным для рекламодателя образом: некоторые пользователи увидят определённую рекламу с меньшей вероятностью из-за своих демографических характеристик. В этой статье мы показываем, что такой искажённый показ рекламы случается на Фейсбуке и его причинами могут быть как рыночные эффекты и финансовая оптимизация, так и собственные предсказания платформы о \"релевантности\" рекламы для различных групп пользователей. Мы обнаружили, что и бюджет рекламодателя, и содержимое рекламного объявления значительно влияют на искажение в показе рекламы в Фейсбуке. Что особо важно, мы наблюдали значительный перекос в показе рекламы в зависимости от гендера и расы для \"настоящих\" объявлений с предложениями работы и жилья, несмотря на то, что целевая аудитория задавалась нейтральными параметрами.",
    "problem-profiles--artificial-intelligence:0363": "Мы уже умеем производить простое [автономное оружие](https://en.wikipedia.org/wiki/Lethal_autonomous_weapon). По мере усложнения этого оружия, оно [полностью изменит процесс ведения войны](https://web.archive.org/web/20221016005551/https://www.vox.com/2019/6/21/18691459/killer-robots-lethal-autonomous-weapons-ai-war). Ниже по тексту мы утверждаем, что [ИИ может даже повлиять на сценарии использования ядерного оружия](#artificial-intelligence-and-war).",
    "problem-profiles--artificial-intelligence:0365": "И, наконец, вопросы политики. Многие уже озабочены тем, что [автоматические алгоритмы социальных сетей увеличивают политическую поляризацию](https://web.archive.org/web/20221016005607/https://www.vox.com/recode/21534345/polarization-election-social-media-filter-bubble). А некоторые эксперты предупреждают, что в ближайшие годы [на политику могут значительно повлиять](https://doi.org/10.17863/CAM.22520) улучшающиеся способности создавать реалистичные видео и фотографии, а также автоматизированные кампании, направленные на изменение общественного мнения.",
    "problem-profiles--artificial-intelligence:0367": "В число знаменитых экономистов, которые считают, что ИИ, скорее всего, станет технологией общего назначения, входят Мануэль Трахтенберг и Эрик Бринолффсон.",
    "problem-profiles--artificial-intelligence:0369": "В статье \"[Искусственный интеллект как следующая технология общего назначения: Политико-экономическая перспектива](https://dx.doi.org/10.3386/w24245)\"(2019) Трахтенберг пишет:",
    "problem-profiles--artificial-intelligence:0371": "> ИИ может стать мощной технологической силой. В связи с этим я обсуждаю способы уменьшить практически неизбежные разрушения, вызванные этим, и увеличить огромный потенциал ИИ творить добро. В настоящее время это особенно важно в свете политико-экономических соображений, которые практически отсутствовали, когда новые технологии общего назначения появлялись в прошлом.",
    "problem-profiles--artificial-intelligence:0373": "В статье \"[Искусственный интеллект и современный парадокс продуктивности: столкновение ожиданий и статистики](https://ideas.repec.org/h/nbr/nberch/14007.html)\"(2018) Бринольфссон пишет:",
    "problem-profiles--artificial-intelligence:0375": "> Применение ИИ в конкретных отраслях может оказаться важным, однако мы утверждаем, что ещё более важный экономический эффект от ИИ, машинного обучения и связанных с ними технологий может проистекать из того, что у них есть все характеристики технологий общего назначения.`]",
    "problem-profiles--artificial-intelligence:0377": "И, возможно, это будет лишь только начало. Возможно, у нас появятся компьютеры, которые [рано или поздно автоматизируют абсолютно всё, что делают люди](https://web.archive.org/web/20221013011716/https://www.cold-takes.com/how-digital-people-could-change-the-world/). Кажется, это вполне вероятно — по меньшей мере, теоретически, — просто потому, что достаточно сложный компьютер при достаточном количестве энергии сможет симулировать человеческий мозг. Это уже само по себе способ автоматизировать всё, что могут делать люди (пусть и не самый эффективный).",
    "problem-profiles--artificial-intelligence:0379": "И как мы увидим в следующем разделе, есть свидетельства, что автоматизация может распространяться благодаря масштабированию уже имеющихся технологий.",
    "problem-profiles--artificial-intelligence:0381": "### Способности ML-систем в последнее время стремительно растут",
    "problem-profiles--artificial-intelligence:0383": "Чтобы построить ИИ с помощью машинного обучения, нужны три составляющие:",
    "problem-profiles--artificial-intelligence:0385": "1. Хорошие алгоритмы (то есть, более эффективные алгоритмы лучше)\n2. Данные, чтобы тренировать алгоритмы\n3. Достаточно вычислительных ресурсов, чтобы обучать модель",
    "problem-profiles--artificial-intelligence:0389": "Мы поговорили с [Данни Эрнандесом](/podcast/episodes/danny-hernandez-forecasting-ai-progress/), который (в то время) был исследователем в команде прогнозистов [OpenAI](https://openai.com/). Эрнандес и его команда изучали, как две из этих составляющих (вычислительные ресурсы и эффективность алгоритмов) менялись со временем.",
    "problem-profiles--artificial-intelligence:0391": "Они обнаружили, что после 2012 года [количество вычислительных ресурсов, используемых для обучения](https://web.archive.org/web/20221012110830/https://openai.com/blog/ai-and-compute/) самых больших моделей ИИ росло экспоненциально — удваивалось каждые 3,4 месяца.",
    "problem-profiles--artificial-intelligence:0393": "И таким образом с 2012 года количество вычислительных ресурсов, используемых для обучения самых больших моделей выросло больше чем в миллиард раз.",
    "problem-profiles--artificial-intelligence:0395": "Эрнандес и его команда также проверили, сколько нужно вычислительных ресурсов, чтобы обучить нейронную сеть, у которой будет [такая же эффективность, какая была у AlexNet](https://web.archive.org/web/20221013011919/https://openai.com/blog/ai-and-efficiency/)(одного из ранних алгоритмов распознавания картинок).",
    "problem-profiles--artificial-intelligence:0397": "<aside class=\"well well-person pull-right clearfix   padding-top-small padding-bottom-small\">\n<h4 class=\"no-margin-top\">Давайте посмотрим, на что способна GPT-3</h4>",
    "problem-profiles--artificial-intelligence:0400": "<a id=\"GPT-3\" class=\"link-anchor\"></a>",
    "problem-profiles--artificial-intelligence:0402": "GPT-3, выпущенная OpenAI в июне 2020 года, [по мнению многочисленных СМИ](https://en.wikipedia.org/wiki/GPT-3#Reviews) стала значительным шагом вперёд в возможностях систем глубокого обучения. Во время релиза она [была самой большой когда-либо созданной нейронной сетью](https://towardsdatascience.com/gpt-3-a-complete-overview-190232eb25fd) — 175 миллиардов параметров.",
    "problem-profiles--artificial-intelligence:0404": "В сущности GPT-3 пытается продолжить фрагмент текста.",
    "problem-profiles--artificial-intelligence:0406": "Например, мы попросили GPT-3 создать стихотворение в стиле Шекспира об искусственном интеллекте, дав ей на вход строку “a Shakespearean poem about artificial intelligence”:[^:`GPT-3 каждый раз на эту строку выдаёт новое стихотворение. Мы создали пять и выбрали лучшее.`]",
    "problem-profiles--artificial-intelligence:0424": "Но GPT-3 может гораздо больше. Ниже перечислены некоторые из наиболее впечатляющих вещей, связанных с ней:[^:`Следует заметить, что когда вы видите как люди в интернете делятся результатами, полученными от систем вроде GPT-3, зачастую это специально выбранные лучшие результаты. Однако это не делает их менее впечатляющими — ведь GPT-3 создаёт их достаточно часто, чтобы люди могли их получать за разумное время. И производительность больших языковых моделей, таких как GPT-3, после 2020 года только улучшилась — в частности нас очень впечатлили результаты [LaMDA](https://blog.google/technology/ai/lamda/) — одной из больших языковых моделей от Google Brain, выпущенной в мае 2022 года.`]",
    "problem-profiles--artificial-intelligence:0426": "- GPT-3 может [правильно отвечать на некоторые медицинские вопросы](https://twitter.com/QasimMunye/status/1278750809094750211).\n- Приложение, использующее GPT-3, может [написать код для создания простой разметки web-сайта по текстовому описанию](https://twitter.com/sharifshameem/status/1282676454690451457).\n- [Основанная на GPT-3 функция для табличного редактора](https://twitter.com/pavtalk/status/1285410751092416513) способна предсказывать, что нужно записать в ту или иную ячейку.\n- GPT-3 умеет [играть в шахматы](https://analyticsindiamag.com/how-this-ai-expert-taught-gpt-3-to-play-chess/) (хотя и не слишком хорошо).\n- GPT-3 стала соавтором в написании [романа в жанре фэнтези на 200 страниц](https://docs.google.com/document/d/1asIdoEhZD2oLasLloo3DV96_W0bfr-RM0vDYqlxlMI4/edit#).\n- Девять философов написали эссе о [следствиях появления GPT-3](https://web.archive.org/web/20221013012208/https://dailynous.com/2020/07/30/philosophers-gpt-3/), а GPT-3 написала им [ответ](https://drive.google.com/file/d/1B-OymgKE1dRkBcJ7fVhTs9hNqx1IuUyW/view).",
    "problem-profiles--artificial-intelligence:0433": "</aside>",
    "problem-profiles--artificial-intelligence:0435": "Они обнаружили, что количество вычислительных ресурсов требуемых для получения той же производительности падает экспоненциально — уменьшается вдвое каждые 16 месяцев.",
    "problem-profiles--artificial-intelligence:0437": "Таким образом, с 2012 года количество вычислительных ресурсов, требуемых для получения той же производительности упало больше, чем в 100 раз. В сочетании с тем, что количество доступных вычислительных ресурсов увеличилось, это значительный рост.[^:`[Более свежая работа](https://web.archive.org/web/20221016005120/https://www.alignmentforum.org/posts/XKtybmbjhC6mXDm5z/compute-trends-across-three-eras-of-machine-learning), судя по всему, поддерживает идею экспоненциального роста вычислительных ресурсов, однако утверждает, что этот рост несколько медленнее указанного в анализе OpenAI. Также есть экспериментальная работа, исследующая вопрос, насколько производительность [масштабируется относительно таких ключевых факторов как вычислительные ресурсы и размер модели](https://web.archive.org/web/20220829202030/https://www.gwern.net/docs/www/arxiv.org/20d126b9c3baf640f8d1d5dff3e253faac2e8242.pdf) (а не просто как производительность меняется от месяца к месяцу) и она поддерживает предсказания об экспоненциальном росте.`]",
    "problem-profiles--artificial-intelligence:0439": "Сложно сказать, продолжатся ли эти тенденции, однако они говорят о том, что за последние десять лет границы возможного при помощи машинного обучения существенно расширились.",
    "problem-profiles--artificial-intelligence:0441": "Более того, судя по всему, увеличение размера моделей (и количества вычислительных ресурсов, используемых для их обучения) приводит к [даже более сложному поведению](https://web.archive.org/web/20221013012207/https://www.alignmentforum.org/posts/XusDPpXr6FYJqWkxh/an-156-the-scaling-hypothesis-a-plan-for-building-agi). Например, модели вроде GPT-3 начинают выполнять задачи, на которые их не обучали.",
    "problem-profiles--artificial-intelligence:0443": "Эти наблюдения ведут к [гипотезе о масштабировании](https://web.archive.org/web/20221013012219/https://www.gwern.net/Scaling-hypothesis), которая гласит, что мы можем просто строить всё большие и большие нейронные сети и в итоге будем получать всё более и более мощный искусственный интеллект и таким образом дойдём до интеллекта человеческого уровня и дальше.",
    "problem-profiles--artificial-intelligence:0445": "Если это правда, то мы можем предсказать, как будут со временем улучшаться создаваемые ИИ, — просто на основании того, как быстро увеличиваются вычислительные мощности, доступные для обучения моделей.",
    "problem-profiles--artificial-intelligence:0447": "Однако как мы увидим ниже, скорое появление чрезвычайно мощного ИИ предсказывает не только гипотеза о масштабировании. Другие способы оценки прогресса в области ИИ говорят о том же самом.",
    "problem-profiles--artificial-intelligence:0449": "### Когда нам стоит ожидать появление трансформационного ИИ?",
    "problem-profiles--artificial-intelligence:0478": "<div class=\"container--page-width\"><div class=\"row\"><div class=\"tablepress-scroll-wrapper\" id=\"tablepress-189-no-2-scroll-wrapper\">",
    "problem-profiles--artificial-intelligence:0487": "</div></div></div>",
    "problem-profiles--artificial-intelligence:0611": "</div>",
    "problem-profiles--artificial-intelligence:0613": "<div class=\"panel-body-collapse collapse\" id=\"-20\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:0621": "</div></div></div>",
    "problem-profiles--artificial-intelligence:0626": "</div>",
    "problem-profiles--artificial-intelligence:0628": "<div class=\"panel-body-collapse collapse\" id=\"-21\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:0636": "</div></div></div>",
    "problem-profiles--artificial-intelligence:0641": "</div>",
    "problem-profiles--artificial-intelligence:0643": "<div class=\"panel-body-collapse collapse\" id=\"-22\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:0653": "</div></div></div></div></div>",
    "problem-profiles--artificial-intelligence:0724": "<div class=\"well bg-gray-lighter margin-bottom margin-top padding-top-small padding-bottom-small\">",
    "problem-profiles--artificial-intelligence:0735": "</div>",
    "problem-profiles--artificial-intelligence:0776": "<div class=\"panel clearfix \">",
    "problem-profiles--artificial-intelligence:0786": "</div>",
    "problem-profiles--artificial-intelligence:0941": "</div>",
    "problem-profiles--artificial-intelligence:1003": "</div>",
    "problem-profiles--artificial-intelligence:1005": "<div class=\"panel-body-collapse collapse\" id=\"-23\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1019": "</div></div></div>",
    "problem-profiles--artificial-intelligence:1024": "</div>",
    "problem-profiles--artificial-intelligence:1026": "<div class=\"panel-body-collapse collapse\" id=\"-24\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1038": "</div></div></div>",
    "problem-profiles--artificial-intelligence:1043": "</div>",
    "problem-profiles--artificial-intelligence:1045": "<div class=\"panel-body-collapse collapse\" id=\"-25\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1060": "</div></div></div>",
    "problem-profiles--artificial-intelligence:1065": "</div>",
    "problem-profiles--artificial-intelligence:1067": "<div class=\"panel-body-collapse collapse\" id=\"-26\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1111": "</div></div></div></div>",
    "problem-profiles--artificial-intelligence:1120": "</div>",
    "problem-profiles--artificial-intelligence:1122": "<div class=\"panel-body-collapse collapse\" id=\"-28\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1138": "</div></div></div>",
    "problem-profiles--artificial-intelligence:1143": "</div>",
    "problem-profiles--artificial-intelligence:1145": "<div class=\"panel-body-collapse collapse\" id=\"-29\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1168": "</div>",
    "problem-profiles--artificial-intelligence:1170": "<div class=\"panel-body-collapse collapse\" id=\"-30\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1187": "</div></div></div>",
    "problem-profiles--artificial-intelligence:1192": "</div>",
    "problem-profiles--artificial-intelligence:1194": "<div class=\"panel-body-collapse collapse\" id=\"-31\"><div class=\"panel-body\">",
    "problem-profiles--artificial-intelligence:1211": "</div>",
    "problem-profiles--artificial-intelligence:1213": "<div class=\"panel-body-collapse collapse\" id=\"-32\"><div class=\"panel-body\">"
}
